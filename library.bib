
@techreport{2006,
  title = {Human-{{Systems Integration Requirements}}},
  year = {2006},
  journal = {Integration The Vlsi Journal},
  pages = {1--310},
  institution = {{NASA}},
  file = {/home/cameron/Zotero/storage/ANPTIXWL/20120014522.pdf}
}

@techreport{2017,
  title = {{{HEOMD-001 REVISION A HUMAN EXPLORATION AND OPERATIONS EXPLORATION OBJECTIVES}}},
  year = {2017},
  pages = {22},
  institution = {{NASA Human Exploration and Operations Mission Directorate}},
  abstract = {This document establishes the integrated set of Exploration Objectives for all exploration activities in the Human Exploration and Operations Mission Directorate (HEOMD). The purpose of the Exploration Objectives is to translate and bridge the gap between agency-level human exploration goals as articulated in NASA's Strategic Plan and other programmatic documentation into clear and discrete objectives for implementation by HEOMD organizations and HEOMD-led missions. The Exploration Objectives can also be used to help inform, identify, and/or prioritize agency technology and science investments. The Exploration Objectives are intended to be dynamic, and will be updated as necessary to reflect changes in agency strategy, improving scientific and technical knowledge, accumulated operational experience, availability of resources, and/or programmatic lessons learned. As HEOMD and other executing organizations evolve, and as relevant HEOMD constituent organizational plans and mission products such as Mission Objectives (MOs) are created or updated in the normal course of business, those plans and products shall align to the Exploration Objectives. The HEOMD Exploration Objectives will inform, and be driven by, future updates to the NASA Strategic Plan. While not explicitly stated in the exploration objectives listed in this document, the health and safety of crew is implicit in all of the HEOMD objectives. The objectives for exploration phases beyond the Earth-Moon system (see Section 4) are not defined in this document. However, this is not intended to preclude investment by HEOMD or stakeholder organizations in planned missions and early-stage technology activities with potential relevance for crewed missions beyond the Earth-Moon system. This strategy allows NASA the opportunity to remain agile and to continue to refine its plans over time while maintaining the focus on exploration of the martian surface as the horizon goal. NASA's planning will continue to be informed by advances in technology and scientific knowledge, as well as by opportunities presented by growing government/commercial capabilities and international participation. The exploration objectives contained herein are intended to inform program requirements and revisions as necessary.},
  file = {/home/cameron/Zotero/storage/W9PN4A4B/m-api-0717249e-88e8-2b4b-5608-b875ef5120b9.pdf}
}

@article{2019,
  title = {Lunar {{Exploration Science Objectives}}},
  year = {2019},
  isbn = {2019002965},
  file = {/home/cameron/Zotero/storage/RC6XBMXS/20190029655.pdf}
}

@techreport{2019a,
  title = {{{SLS-SPEC-159 REVISION G CROSS-PROGRAM DESIGN SPECIFICATION FOR NATURAL ENVIRONMENTS}} ({{DSNE}})},
  year = {2019},
  file = {/home/cameron/Zotero/storage/MVV88ZHT/DSNE_2019-12-11.pdf}
}

@techreport{2554,
  title = {Why {{We Should Study}} the {{Themis Asteroid Family}} in the 2023\textendash 2032 {{Decade M}}.},
  author = {Landis, M. E. and {Castillo-Rogez}, J. C. and Hayne, P. O. and Hsieh, H. H. and Hughson, K. H. G. and Miller, K. E. and Kubitschek, D. and Prettyman, T. H. and Rivkin, A. S. and Schmidt, B. E. and Scully, J. E. C. and Yamashita, N. and Villarreal, M. N.},
  year = {2020},
  pages = {1--7},
  abstract = {The last decade of icy asteroid science has made it clear that understanding the origin and interior evolution of these bodies is fundamental for understanding the history of organics and ices in the solar system, as well as cross-cutting themes like Ocean Worlds. Observations and modeling have suggested that 24 Themis is the core of an icy asteroid, and the Themis family likely represents the fragments of an icy protoplanet broken apart by Nature's rock hammer. In this white paper, we identify key questions in planetary science, earth science, and astrobiology, that would be addressed by further study of the interior evolution of icy asteroids such as 24 Themis. We present possible measurements and potential future research focused on the Themis family in the next decade. While much can still be done telescopically and in the laboratory, a spacecraft mission would be necessary to make key measurements that address origin and ice-rock fractionation science objectives for 24 Themis and family members.},
  file = {/home/cameron/Zotero/storage/D3QDZD7U/pdf}
}

@article{Abercromby2013,
  title = {Desert {{RATS}} 2011: {{Human}} and Robotic Exploration of near-{{Earth}} Asteroids},
  author = {Abercromby, Andrew F.J. and Chappell, Steven P. and Gernhardt, Michael L.},
  year = {2013},
  journal = {Acta Astronautica},
  volume = {91},
  pages = {34--48},
  publisher = {{Elsevier}},
  issn = {00945765},
  doi = {10.1016/j.actaastro.2013.05.002},
  abstract = {The Desert Research and Technology Studies (D-RATS) 2011 field test involved the planning and execution of a series of exploration scenarios under operational conditions similar to those expected during a human exploration mission to a near-Earth asteroid (NEA). The focus was on understanding the operations tempo during simulated NEA exploration and the implications of communications latency and limited data bandwidth. Anchoring technologies and sampling techniques were not evaluated due to the immaturity of those technologies and the inability to meaningfully test them at D-RATS. Reduced gravity analogs and simulations are being used to fully evaluate Space Exploration Vehicle (SEV) and extravehicular (EVA) operations and interactions in near-weightlessness at a NEA as part of NASA's integrated analogs program. Hypotheses were tested by planning and performing a series of 1-day simulated exploration excursions comparing test conditions all of which involved a single Deep Space Habitat (DSH) and either 0, 1, or 2 SEVs; 3 or 4 crewmembers; 1 of 2 different communications bandwidths; and a 50-second each-way communications latency between the field site and Houston. Excursions were executed at the Black Point Lava Flow test site with a remote Mission Control Center and Science Support Room at Johnson Space Center (JSC) being operated with 50-second each-way communication latency to the field. Crews were composed of astronauts and professional field geologists. Teams of Mission Operations and Science experts also supported the mission simulations each day. Data were collected separately from the Crew, Mission Operations, and Science teams to assess the test conditions from multiple perspectives. For the operations tested, data indicates practically significant benefits may be realized by including at least one SEV and by including 4 versus 3 crewmembers in the NEA exploration architecture as measured by increased scientific data quality, EVA exploration time, capability assessment ratings, and consensus acceptability ratings provided by Crew, Mission Operations, and Science teams. A combination of text and voice was used to effectively communicate over the communications latency, and increased communication bandwidth yielded a small but practically significant improvement in overall acceptability as rated by the Science team, although the impact of bandwidth on scientific strategic planning and public outreach was not assessed. No effect of increased bandwidth was observed with respect to Crew or Mission Operations team ratings of overall acceptability. \textcopyright{} 2013 IAA.},
  keywords = {Asteroids,Human exploration,Operations,Robotics,Space exploration vehicle Analog},
  file = {/home/cameron/Zotero/storage/U3YE34QA/desert-rats-2011.pdf}
}

@article{abercromby2013,
  title = {Desert {{RATS}} 2011: {{Human}} and Robotic Exploration of near-{{Earth}} Asteroids},
  author = {Abercromby, Andrew F.J. and Chappell, Steven P. and Gernhardt, Michael L.},
  year = {2013},
  journal = {Acta Astronautica},
  volume = {91},
  pages = {34--48},
  issn = {00945765},
  doi = {10.1016/j.actaastro.2013.05.002},
  abstract = {The Desert Research and Technology Studies (D-RATS) 2011 field test involved the planning and execution of a series of exploration scenarios under operational conditions similar to those expected during a human exploration mission to a near-Earth asteroid (NEA). The focus was on understanding the operations tempo during simulated NEA exploration and the implications of communications latency and limited data bandwidth. Anchoring technologies and sampling techniques were not evaluated due to the immaturity of those technologies and the inability to meaningfully test them at D-RATS. Reduced gravity analogs and simulations are being used to fully evaluate Space Exploration Vehicle (SEV) and extravehicular (EVA) operations and interactions in near-weightlessness at a NEA as part of NASA's integrated analogs program. Hypotheses were tested by planning and performing a series of 1-day simulated exploration excursions comparing test conditions all of which involved a single Deep Space Habitat (DSH) and either 0, 1, or 2 SEVs; 3 or 4 crewmembers; 1 of 2 different communications bandwidths; and a 50-second each-way communications latency between the field site and Houston. Excursions were executed at the Black Point Lava Flow test site with a remote Mission Control Center and Science Support Room at Johnson Space Center (JSC) being operated with 50-second each-way communication latency to the field. Crews were composed of astronauts and professional field geologists. Teams of Mission Operations and Science experts also supported the mission simulations each day. Data were collected separately from the Crew, Mission Operations, and Science teams to assess the test conditions from multiple perspectives. For the operations tested, data indicates practically significant benefits may be realized by including at least one SEV and by including 4 versus 3 crewmembers in the NEA exploration architecture as measured by increased scientific data quality, EVA exploration time, capability assessment ratings, and consensus acceptability ratings provided by Crew, Mission Operations, and Science teams. A combination of text and voice was used to effectively communicate over the communications latency, and increased communication bandwidth yielded a small but practically significant improvement in overall acceptability as rated by the Science team, although the impact of bandwidth on scientific strategic planning and public outreach was not assessed. No effect of increased bandwidth was observed with respect to Crew or Mission Operations team ratings of overall acceptability. \textcopyright{} 2013 IAA.},
  keywords = {Asteroids,Human exploration,Operations,Robotics,Space exploration vehicle Analog},
  file = {/home/cameron/Zotero/storage/M9MSAXFH/desert-rats-2011.pdf}
}

@article{Abercromby2016a,
  title = {Integrated {{Extravehicular Activity Human Research Plan}}: 2016},
  author = {Abercromby, Andrew F.J. and Cupples, J Scott and Rajulu, Sudhakar and Buffington, Jesse A and Norcross, Jason R and Chappell, Steven P. and Science, Wyle and Group, Engineering},
  year = {2016},
  journal = {46th International Conference on Environmental Systems},
  number = {July},
  abstract = {Multiple organizations within NASA as well as industry and academia fund and participate in research related to extravehicular activity (EVA). In October 2015, representatives of the EVA Office, the Crew and Thermal Systems Division (CTSD), and the Human Research Program (HRP) at NASA Johnson Space Center agreed on a formal framework to improve multi-year coordination and collaboration in EVA research. At the core of the framework is an Integrated EVA Human Research Plan and a process by which it will be annually reviewed and updated. The over-arching objective of the collaborative framework is to conduct multi-disciplinary cost-effective research that will enable humans to perform EVAs safely, effectively, comfortably, and efficiently, as needed to enable and enhance human space exploration missions. Research activities must be defined, prioritized, planned and executed to comprehensively address the right questions, avoid duplication, leverage other complementary activities where possible, and ultimately provide actionable evidence-based results in time to inform subsequent tests, developments and/or research activities. Representation of all appropriate stakeholders in the definition, prioritization, planning and execution of research activities is essential to accomplishing the over-arching objective. A formal review of the Integrated EVA Human Research Plan will be conducted annually. Coordination with stakeholders outside of the EVA Office, CTSD, and HRP is already in effect on a study-by-study basis; closer coordination on multi-year planning with other EVA stakeholders including academia is being actively pursued. Details of the preliminary Integrated EVA Human Research Plan are presented including description of ongoing and planned research activities in the areas of: physiological and performance capabilities; suit design parameters; EVA human health and performance modeling; EVA tasks and concepts of operations; EVA informatics; human-suit sensors; suit sizing and fit; and EVA injury risk and mitigation},
  file = {/home/cameron/Zotero/storage/3VE3DMM9/Abercromby et al. - 2016 - Integrated Extravehicular Activity Human Research Plan 2016(2).pdf;/home/cameron/Zotero/storage/DRCJMBLU/Abercromby et al. - 2016 - Integrated Extravehicular Activity Human Research Plan 2016.pdf}
}

@techreport{Abercromby2016d,
  title = {Integrated {{Extravehicular Activity Human Research Plan}}},
  author = {Abercromby, Andrew F. and Cupples, J Scott and Rajulu, Sudhakar and Buffington, Jesse A and Norcross, Jason R and Chappell, Steven P.},
  year = {2016},
  pages = {19},
  institution = {{National Aeronautics and Space Administration}},
  abstract = {Multiple organizations within NASA as well as industry and academia fund and participate in research related to extravehicular activity (EVA). In October 2015, representatives of the EVA Office, the Crew and Thermal Systems Division (CTSD), and the Human Research Program (HRP) at NASA Johnson Space Center agreed on a formal framework to improve multi-year coordination and collaboration in EVA research. At the core of the framework is an Integrated EVA Human Research Plan and a process by which it will be annually reviewed and updated. The over-arching objective of the collaborative framework is to conduct multi-disciplinary cost-effective research that will enable humans to perform EVAs safely, effectively, comfortably, and efficiently, as needed to enable and enhance human space exploration missions. Research activities must be defined, prioritized, planned and executed to comprehensively address the right questions, avoid duplication, leverage other complementary activities where possible, and ultimately provide actionable evidence-based results in time to inform subsequent tests, developments and/or research activities. Representation of all appropriate stakeholders in the definition, prioritization, planning and execution of research activities is essential to accomplishing the over-arching objective. A formal review of the Integrated EVA Human Research Plan will be conducted annually. Coordination with stakeholders outside of the EVA Office, CTSD, and HRP is already in effect on a study-by-study basis; closer coordination on multi-year planning with other EVA stakeholders including academia is being actively pursued. Details of the preliminary Integrated EVA Human Research Plan are presented including description of ongoing and planned research activities in the areas of: physiological and performance capabilities; suit design parameters; EVA human health and performance modeling; EVA tasks and concepts of operations; EVA informatics; human-suit sensors; suit sizing and fit; and EVA injury risk and mitigation.},
  file = {/home/cameron/Zotero/storage/KTGEGL8Y/ICES_2016_370.pdf}
}

@article{Activity2019,
  title = {Opportunities and {{Challenges}} of {{Promoting Scientific Dialog}} throughout {{Execution}}},
  author = {Nawotniak, Shannon E Kobs and Miller, Matthew J. and Stevens, Adam H. and Marquez, Jessica J. and Payler, Samuel J. and Brady, Allyson L. and Hughes, Scott S. and Haberle, Christopher W and Sehlke, Alexander and Beaton, Kara H. and Chappell, Steven P. and Elphic, Richard C and Lim, Darlene S.S.},
  year = {2019},
  volume = {19},
  number = {3},
  pages = {426--439},
  doi = {10.1089/ast.2018.1901},
  file = {/home/cameron/Zotero/storage/Z8DTDVHH/ast.2018.1901.pdf}
}

@article{Ade2014,
  title = {Relationship between Simulated Extravehicular Activity Tasks and Measurements of Physical Performance},
  author = {Ade, C. J. and Broxterman, R. M. and Craig, J. C. and Schlup, S. J. and Wilcox, S. L. and Barstow, T. J.},
  year = {2014},
  journal = {Respiratory Physiology and Neurobiology},
  volume = {203},
  pages = {19--27},
  publisher = {{Elsevier B.V.}},
  issn = {18781519},
  doi = {10.1016/j.resp.2014.08.007},
  abstract = {The purpose was to evaluate the relationships between tests of fitness and two activities that simulate components of Lunar- and Martian-based extravehicular activities (EVA). Seventy-one subjects completed two field tests: a physical abilities test and a 10. km Walkback test. The relationships between test times and the following parameters were determined: running V\textperiodcentered O2max, gas exchange threshold (GET), speed at V\textperiodcentered O2max (s-V\textperiodcentered O2max), highest sustainable rate of aerobic metabolism [critical speed (CS)], and the finite distance that could be covered above CS (D'): arm cranking V\textperiodcentered O2peak, GET, critical power (CP), and the finite work that can be performed above CP (W'). CS, running V\textperiodcentered O2max, s-V\textperiodcentered O2max, and arm cranking V\textperiodcentered O2peak had the highest correlations with the physical abilities field test (r = 0.66-0.82, P{$<$}. 0.001). For the 10. km Walkback, CS, s-V\textperiodcentered O2max, and running V\textperiodcentered O2max were significant predictors (r = 0.64-0.85, P{$<$}. 0.001). CS and to a lesser extent V\textperiodcentered O2max are most strongly associated with tasks that simulate aspects of EVA performance, highlighting CS as a method for evaluating astronaut physical capacity.},
  pmid = {25169116},
  keywords = {Critical speed,Endurance performance,Extravehicular activity,VË™O2max},
  file = {/home/cameron/Zotero/storage/AZ2NHGEJ/Ade et al. - 2014 - Relationship between simulated extravehicular activity tasks and measurements of physical performance.pdf}
}

@article{Aeronautics2008,
  title = {Increment {{Definition}} and {{Requirements Document}} for {{Increments}} 19 and 20 {{International Space Station Program Baseline}}},
  author = {Aeronautics, National},
  year = {2008},
  number = {July},
  file = {/home/cameron/Zotero/storage/XND3GUUE/Aeronautics - 2008 - Increment Definition and Requirements Document for Increments 19 and 20 International Space Station Program Baselin.pdf}
}

@article{Aeronautics2015,
  title = {Evidence {{Report}} : {{RISK OF INJURY AND COMPROMISED Human Research Program Human Health}} and {{Countermeasures Element}}},
  author = {Aeronautics, National},
  year = {2015},
  file = {/home/cameron/Zotero/storage/UIR86VVA/20170002574.pdf}
}

@article{Ahmed2010,
  title = {An Empirical Comparison of Machine Learning Models for Time Series Forecasting},
  author = {Ahmed, Nesreen K. and Atiya, Amir F. and El Gayar, Neamat and {El-Shishiny}, Hisham},
  year = {2010},
  journal = {Econometric Reviews},
  volume = {29},
  number = {5},
  pages = {594--621},
  issn = {07474938},
  doi = {10.1080/07474938.2010.481556},
  abstract = {In this work we present a large scale comparison study for themajormachine learning models for time series forecasting. Specifically, we apply the models on the monthly M3 time series competition data (around a thousand time series). There have been very few, if any, large scale comparison studies for machine learning models for the regression or the time series forecasting problems, so we hope this study would fill this gap. The models considered are multilayer perceptron, Bayesian neural networks, radial basis functions, generalized regression neural networks (also called kernel regression), K-nearest neighbor regression, CART regression trees, support vector regression, and Gaussian processes. The study reveals significant differences between the different methods. The best two methods turned out to be the multilayer perceptron and the Gaussian process regression. In addition to model comparisons, we have tested different preprocessing methods and have shown that they have different impacts on the performance.},
  isbn = {0747-4938},
  keywords = {Comparison study,Gaussian process regression,Machine learning models,Neural network forecasting,Support vector regression},
  file = {/home/cameron/Zotero/storage/LKKQJFUQ/empirical-comparison-of-regression.pdf}
}

@inproceedings{akmal2019,
  title = {Quantifying Degrees of Controllability in Temporal Networks with Uncertainty},
  booktitle = {Proceedings {{International Conference}} on {{Automated Planning}} and {{Scheduling}}, {{ICAPS}}},
  author = {Akmal, Shyan and Ammons, Savana and Li, Hemeng and Boerkoel, James C.},
  year = {2019},
  issn = {23340843},
  abstract = {Controllability for Simple Temporal Networks with Uncertainty (STNUs) has thus far been limited to three levels: strong, dynamic, and weak. Because of this, there is currently no systematic way for an agent to assess just how far from being controllable an uncontrollable STNU is. We use a new geometric interpretation of STNUs to introduce the degrees of strong and dynamic controllability - continuous metrics that measure how far a network is from being controllable. We utilize these metrics to approximate the probabilities that an STNU can be dispatched successfully offline and online respectively. We introduce new methods for predicting the degrees of strong and dynamic controllability for uncontrollable networks. In addition, we show empirically that both metrics are good predictors of the actual dispatch success rate.},
  isbn = {978-1-57735-807-7}
}

@article{Akmal2020,
  title = {Quantifying Controllability in Temporal Networks with Uncertainty},
  author = {Akmal, Shyan and Ammons, Savana and Li, Hemeng and Gao, Michael and Popowski, Lindsay and Boerkoel, James C.},
  year = {2020},
  journal = {Artificial Intelligence},
  volume = {289},
  pages = {103384},
  publisher = {{Elsevier B.V.}},
  issn = {00043702},
  doi = {10.1016/j.artint.2020.103384},
  abstract = {Controllability for Simple Temporal Networks with Uncertainty (STNUs) has thus far been limited to three levels: strong, dynamic, and weak. Because of this, there is currently no systematic way for an agent to assess just how far from being controllable an uncontrollable STNU is. We provide new insights inspired by a geometric interpretation of STNUs to introduce the degrees of strong and dynamic controllability \textemdash{} continuous metrics that measure how far a network is from being controllable. We utilize these metrics to approximate the probabilities that an STNU can be dispatched successfully offline and online respectively. We introduce new methods for predicting the degrees of strong and dynamic controllability for uncontrollable networks. We further generalize these metrics by defining likelihood of controllability, a controllability measure that applies to Probabilistic Simple Temporal Networks (PSTNs). Finally, we empirically demonstrate that these metrics are good predictors of actual dispatch success rate for STNUs and PSTNs.},
  keywords = {Controllability,Probabilistic simple temporal networks,Scheduling,Simple temporal networks with uncertainty,Temporal planning},
  file = {/home/cameron/Zotero/storage/CYYAVNCF/Akmal et al. - 2020 - Quantifying controllability in temporal networks with uncertainty.pdf}
}

@article{Allen1983,
  title = {Maintaining Knowledge about Temporal Intervals. Communications of {{ACM}}, {{Vol}}. 26, {{No}}. 11, 832\textendash 843},
  author = {Allen, James},
  year = {1983},
  volume = {26},
  number = {11},
  pages = {832--843},
  file = {/home/cameron/Zotero/storage/3V25R8WS/Allen-CACM1983.pdf}
}

@article{Allen1984,
  title = {Towards a General Theory of Action and Time},
  author = {Allen, James F.},
  year = {1984},
  journal = {Artificial Intelligence},
  volume = {23},
  number = {2},
  pages = {123--154},
  issn = {00043702},
  doi = {10.1016/0004-3702(84)90008-0},
  abstract = {A formalism for reasoning about actions is proposed that is based on a temporal logic. It allows a much wider range of actions to be described than with previous approaches such as the situation calculus. This formalism is then used to characterize the different types of events, processes, actions, and properties that can be described in simple English sentences. In addressing this problem, we consider actions that involve non-activity as well as actions that can only be defined in terms of the beliefs and intentions of the actors. Finally, a framework for planning in a dynamic world with external events and multiple agents is suggested. \textcopyright{} 1984.},
  file = {/home/cameron/Zotero/storage/HYSLFYI5/m-api-602bab14-99f6-25cf-5734-4c4266e24c69.pdf}
}

@article{Allen1999,
  title = {Mixed-Initiative Interaction},
  author = {Allen, James F.},
  year = {1999},
  journal = {IEEE Intelligent Systems and Their Applications},
  volume = {14},
  number = {5},
  pages = {14--16},
  issn = {10947167},
  doi = {10.1109/5254.796083},
  abstract = {Research in mixed-initiative interaction is still in its infancy, and the research problems we will face are significant. The potential impact of such systems, however, cannot be overestimated. If we are ever to build computer systems that can seamlessly interact with humans as they perform complex tasks, these systems will need to support effective mixed-initiative interaction.},
  file = {/home/cameron/Zotero/storage/BU76VZXR/Allen - 1999 - Mixed-initiative interaction.pdf}
}

@article{Altman1992,
  title = {An Introduction to Kernel and Nearest-Neighbor Nonparametric Regression},
  author = {Altman, N. S.},
  year = {1992},
  journal = {American Statistician},
  volume = {46},
  number = {3},
  pages = {175--185},
  issn = {15372731},
  doi = {10.1080/00031305.1992.10475879},
  abstract = {Nonparametric regression is a set of techniques for estimating a regression curve without making strong assumptions about the shape of the true regression function. These techniques are therefore useful for building and checking parametric models, as well as for data description. Kernel and nearest-neighbor regression estimators are local versions of univariate location estimators, and so they can readily be introduced to beginning students and consulting clients who are familiar with such summaries as the sample mean and median. \textcopyright{} 1992 American Statistical Association.},
  keywords = {Confidence intervals,Local linear regression,Model building,Model checking,Smoothing},
  file = {/home/cameron/Zotero/storage/PEA5QD58/BU-1065-MA.pdf\;jsessionid=6D209E36CC2DA19914943B0D1AF613B0.pdf}
}

@inproceedings{Amir2019,
  title = {Minimizing {{Travel}} in the {{Uniform Dispersal Problem}} for {{Robotic Sensors}}},
  booktitle = {{{AAMAS}}},
  author = {Amir, Michael and Bruckstein, Alfred M.},
  year = {2019},
  eprint = {1903.03259},
  eprinttype = {arxiv},
  pages = {113--121},
  address = {{Montr\'eal, Canada}},
  abstract = {The limited energy capacity of individual robotic agents in a swarm often limits the possible cooperative tasks they can perform. In this work, we investigate the problem of covering an unknown connected grid environment (e.g. a maze or connected corridors) with a robotic swarm so as to minimize the maximal number of steps that each member of the swarm makes and their activity time before their work is finished, thereby minimizing the energy requirements. The robots are autonomous, anonymous and identical, with local sensors and finite memory, and possess no communication capabilities. They are assumed to disperse over time from a fixed location, and to move synchronously. The robots are tasked with occupying every cell of the environment, while avoiding collisions. In the literature such topics are known as \textbackslash textit\{uniform dispersal problems\}. The goal of minimizing the number of steps traveled by the robots has previously been studied in this context. Our contribution is a local robotic strategy for simply connected grid environments that, by exploiting their topology, achieves optimal makespan (the amount of time it takes to cover the environment) and minimizes the maximal number of steps taken by the individual robots before their deactivation. The robots succeed in discovering optimal paths to their eventual destinations, and finish the covering process in \$2V-1\$ time steps, where \$V\$ is the number of cells in the environment.},
  archiveprefix = {arXiv},
  keywords = {Area coverage,Grid environment,minimizing movement,Minimizing movement,mobile robot,Mobile robot,uni-,Uniform dispersal,unknown environment,Unknown environment},
  file = {/home/cameron/Zotero/storage/J35T6PUC/p113.pdf}
}

@techreport{analogs2018,
  title = {{{NASA}}'s {{Analog Missions Paving}} the {{Way}} for {{Space Exploration}}},
  author = {{NASA}},
  year = {2018},
  volume = {51},
  number = {1},
  eprint = {1011.1669v3},
  eprinttype = {arxiv},
  pages = {51},
  address = {{Hampton, VA}},
  institution = {{NASA}},
  issn = {1098-6596},
  abstract = {Today NASA pursues technical innovations and scientific discoveries to advance human exploration of space. To prepare for these complex missions, a vast amount of planning, testing, and technology development must be accomplished. Yet, forecasting how that planning will translate into everyday operations in space is difficult while still on Earth. To help prepare for the real-life challenges of space exploration, NASA relies on Earth-based missions that are similar, or analogous, to space. These are called analog missions\textemdash field activities set in remote locations with extreme characteristics that resemble the challenges of a space mission. NASA conducts these missions in extreme environments around the globe to test technologies and systems and to help guide the future direction of human exploration of the solar system. This report profiles NASA's active analog missions, with highlights and successes from the last few years},
  archiveprefix = {arXiv},
  isbn = {9788578110796},
  pmid = {25246403},
  file = {/home/cameron/Zotero/storage/MKWBFDXZ/NASA - 2018 - NASA's Analog Missions Paving the Way for Space Exploration.pdf}
}

@article{Anderson2015,
  title = {Pressure Sensing for In-Suit Measurement of Space Suited Biomechanics},
  author = {Anderson, Allison P. and Newman, Dava J.},
  year = {2015},
  journal = {Acta Astronautica},
  volume = {115},
  pages = {218--225},
  publisher = {{Elsevier}},
  issn = {00945765},
  doi = {10.1016/j.actaastro.2015.05.024},
  abstract = {Extravehicular Activity (EVA) is a critical component of human spaceflight, but working in gas-pressurized space suits causes fatigue, excessive energy expenditure, and injury. Relatively little is known about how the astronaut moves and interacts with the space suit, and what factors lead to injury. A wearable pressure sensing system to quantitatively measure areas on the body's surface that the space suit impacts during dynamic EVA movement is developed. The system is used to characterize human-suit interaction in the NASA Mark III space suit. Three experienced subjects perform a series of upper body movements: 3 isolated joint movements and 2 functional tasks. Movements are repeated 12 times each and in-suit pressure responses are evaluated both by quantifying peak pressure and full profile responses. Results Sequential sensor activation allows subjects to be indexed inside the space suit during complicated motions to better understand suited biomechanics. Subjectively, subjects generally feel they are consistent for all movements. However, using a nonparametric H-test, 54\% of movements are found to be biomechanically inconsistent (p{$<$}0.05). This experiment provides the first "window" inside the space suit to evaluate contact pressures and sequential indexing of the person inside the suit for realistic EVA movement. It cannot be extrapolated how changes in contact pressure would affect a subject's propensity for injury as injuries accumulate over long time scales. However, changes in pressure may be due to alterations in biomechanical strategies or fatigue, both of which could be precursors for injury and discomfort.},
  keywords = {Abbreviations EVA Extravehicular Activity,EMU Extravehicular Mobility Unit,MG Movement group},
  file = {/home/cameron/Zotero/storage/X473XLV4/Anderson, Newman - 2015 - Pressure sensing for in-suit measurement of space suited biomechanics.pdf}
}

@article{Anderson2015a,
  title = {Life {{Support Baseline Values}} and {{Assumptions Document}}},
  author = {Anderson, Molly S. and Ewert, Michael K. and Keener, John F. and Wagner, Sandra A.},
  year = {2015},
  journal = {Nasa/Tp-2015-218570},
  number = {March},
  pages = {1--220},
  abstract = {The Advanced Life Support (ALS) Baseline Values and Assumptions Document (BVAD) provides analysts and modelers as well as other ALS researchers with a common set of initial values and assumptions called a baseline. This baseline, in turn, provides a common point of origin from which all Systems Integration, Modeling, and Analysis (SIMA) Element studies will depart.},
  file = {/home/cameron/Zotero/storage/BY7CGLDB/20180001338.pdf}
}

@book{annualieeecomputerconference,
  title = {{{IEEE Aerospace Conference}}, 2015 7-14 {{March}} 2015, {{Big Sky}}, {{MT}}},
  author = {{Annual IEEE Computer Conference} and {IEEE Aerospace Conference 2014.03.01-08 Big Sky}, Mont.},
  abstract = {Conference location: Big Sky, MT},
  isbn = {978-1-4799-5380-6},
  file = {/home/cameron/Zotero/storage/BJSLDUSA/information-flow-model-of-eva-ops.pdf}
}

@techreport{Apollo1972,
  title = {Apollo 17 {{Final Lunar Surface Procedures Vol}} 1: {{Nominal Plans}}},
  author = {Zedekar, R. G. and Schultz, D. C. and Bilodeau, J. W. and Slayton, D. K.},
  year = {1972},
  pages = {354},
  address = {{Houston, TX}},
  institution = {{National Aeronautics and Space Administration}},
  abstract = {his document has been prepared by the Crew Procedures Division, Flight Crew Operations Directorate, Manned Space- craft Center, Houston, Texas and by General Electric, Apollo and Ground Systems, Houston Programs. The information con- tained herein represents Lunar Surface Procedures for Apollo 17 Mission J-3, the seventh manned lunar landing mission. The final document consists of two parts: Vol. 1 - Nominal Plans; Vol. 2 - Contingency Plans.},
  file = {/home/cameron/Zotero/storage/BRYFV5YA/Zedekar et al. - 1972 - Apollo 17 Final Lunar Surface Procedures Vol 1 Nominal Plans.pdf}
}

@article{Arbor2013,
  title = {44th {{Lunar}} and {{Planetary Science Conference}} ( 2013 ) 1830 . Pdf 44th {{Lunar}} and {{Planetary Science Conference}} ( 2013 )},
  author = {Arbor, Ann},
  year = {2013},
  volume = {164},
  pages = {1--2},
  doi = {10.1029/2008JE003195.},
  isbn = {3110075113},
  file = {/home/cameron/Zotero/storage/NMKVUVAB/rats-2012.pdf}
}

@article{Armando2000,
  title = {{{SAT-based}} Procedures for Temporal Reasoning},
  author = {Armando, Alessandro and Castellini, Claudio and Giunchiglia, Enrico},
  year = {2000},
  journal = {Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science)},
  volume = {1809},
  number = {2},
  pages = {97--108},
  issn = {03029743},
  doi = {10.1007/10720246_8},
  abstract = {In this paper we study the consistency problem for a set of disjunctive temporal constraints [Stergiou and Koubarakis, 1998]. We propose two SAT-based procedures, and show that|on sets of binary randomly generated disjunctive constraints|they perform up to 2 orders of magnitude less consistency checks than the best procedure presented in [Stergiou and Koubarakis, 1998]. On these tests, our experimental ana-lysis conffirms Stergiou and Koubarakis's result about the existence of an easy-hard-easy pattern whose peak corresponds to a value in between 6 and 7 of the ratio of clauses to variables.},
  isbn = {3540678662},
  file = {/home/cameron/Zotero/storage/NBNNBU82/Armando, Castellini, Giunchiglia - 2000 - SAT-based procedures for temporal reasoning.pdf}
}

@techreport{Artemis202009,
  title = {{{NASA}}'s {{Lunar Exploration Program Overview}}},
  author = {{NASA}},
  year = {2020},
  number = {September},
  pages = {74},
  address = {{Washington DC. USA}},
  institution = {{National Aeronautics and Space Administration}},
  abstract = {America has entered a new era of exploration. NASA's Artemis program will lead humanity forward to the Moon and prepare us for the next giant leap, the exploration of Mars. It has been almost 50 years since astronauts last walked on the lunar surface during the Apollo program, and since then the robotic exploration of deep space has seen decades of technological advancement and scientific discoveries. For the last 20 years, humans have continuously lived and worked aboard the International Space Station 250 miles above Earth, preparing for the day we move farther into the solar system. Sending human explorers 250,000 miles to the Moon, then 140 million miles to Mars, requires a bold vision, effective program management, funding for modern systems development and mission operations, and support from all corners of our great nation as well as our partners across the globe. NASA has been fine-tuning the plan to achieve that bold vision since the president called on the agency in December 2017 to lead a human return to the Moon and beyond with commercial and international partners. Two years later, he challenged us yet again, this time to send the first woman and next man to the Moon within five years. NASA is implementing the Artemis program to achieve those goals, and this document lays out the agency's Moon to Mars exploration approach explaining how we will do it. The Moon plan is twofold: it's focused on achieving the goal of an initial human landing by 2024 with acceptable technical risks, while simultaneously working toward sustainable lunar exploration in the mid- to late 2020s. 2024 is not an arbitrary date. It is the most ambitious date possible, and our success at the Moon, and later, at Mars, will be grounded in our national goals and robust capabilities. The United States leads in space exploration now; however, as more countries and companies take aim at the Moon, America needs the earliest possible landing to maintain and build on that leadership, as well as to prepare for a historic first human mission to Mars. Landing astronauts on the Moon within four years will better focus this global initiative on the engineering, technology development, and process improvements necessary to safely and successfully carry out sustained human exploration of the Moon. It also paves the way for U.S. commercial companies and international partners to further contribute to the exploration and development of the Moon. We need several years in orbit and on the surface of the Moon to build operational confidence for conducting long-term work and supporting life away from Earth before we can embark on the first multi-year human mission to Mars. The sooner we get to the Moon, the sooner we get American astronauts to Mars.We need to act fast to make this vision a reality, and a crewed lunar landing by 2024 is the key to a successful Moon to Mars exploration approach. Our next lunar landing paves the way for a new and sustainable lunar economy\textemdash one where U.S. companies and international partners will benefit from and build on what we learn.},
  keywords = {NP-2020-05-2853-HQ},
  file = {/home/cameron/Zotero/storage/DFV2A3II/m-api-2ea4fd37-c1ae-048b-447b-dad4c5889835.pdf}
}

@article{Arvidson2017,
  title = {Relating Geologic Units and Mobility System Kinematics Contributing to {{Curiosity}} Wheel Damage at {{Gale Crater}}, {{Mars}}},
  author = {Arvidson, R.E. and DeGrosse, P. and Grotzinger, J.P. and Heverly, M.C. and Shechet, J. and Moreland, S.J. and Newby, M.A. and Stein, N. and Steffy, A.C. and Zhou, F. and Zastrow, A.M. and Vasavada, A.R. and Fraeman, A.A. and Stilly, E.K.},
  year = {2017},
  month = oct,
  journal = {Journal of Terramechanics},
  volume = {73},
  pages = {73--93},
  issn = {00224898},
  doi = {10.1016/j.jterra.2017.03.001},
  abstract = {Curiosity landed on plains to the north of Mount Sharp in August 2012. By June 2016 the rover had traversed 12.9 km to the southwest, encountering extensive strata that were deposited in a fluvial-deltaic-lacustrine system. Initial drives across sharp sandstone outcrops initiated an unacceptably high rate of punctures and cracks in the thin aluminum wheel skin structures. Initial damage was found to be related to the drive control mode of the six wheel drive actuators and the kinematics of the rocker-bogie suspension. Wheels leading a suspension pivot were forced onto sharp, immobile surfaces by the other wheels as they maintained their commanded angular velocities. Wheel damage mechanisms such as geometry-induced stress concentration cracking and low-cycle fatigue were then exacerbated. A geomorphic map was generated to assist in planning traverses that would minimize further wheel damage. A steady increase in punctures and cracks between landing and June 2016 was due in part because of drives across the sharp sandstone outcrops that could not be avoided. Wheel lifetime estimates show that with careful path planning the wheels will be operational for an additional ten kilometers or more, allowing the rover to reach key strata exposed on the slopes of Mount Sharp.},
  langid = {english},
  file = {/home/cameron/Zotero/storage/AXAW679Q/Arvidson et al. - 2017 - Relating geologic units and mobility system kinema.pdf}
}

@book{Atrey2010,
  title = {Multimodal Fusion for Multimedia Analysis: {{A}} Survey},
  author = {Atrey, Pradeep K. and Hossain, M. Anwar and El Saddik, Abdulmotaleb and Kankanhalli, Mohan S.},
  year = {2010},
  journal = {Multimedia Systems},
  volume = {16},
  issn = {09424962},
  doi = {10.1007/s00530-010-0182-0},
  abstract = {This survey aims at providing multimedia researchers with a state-of-the-art overview of fusion strategies, which are used for combining multiple modalities in order to accomplish various multimedia analysis tasks. The existing literature on multimodal fusion research is presented through several classifications based on the fusion methodology and the level of fusion (feature, decision, and hybrid). The fusion methods are described from the perspective of the basic concept, advantages, weaknesses, and their usage in various analysis tasks as reported in the literature. Moreover, several distinctive issues that influence a multimodal fusion process such as, the use of correlation and independence, confidence level, contextual information, synchronization between different modalities, and the optimal modality selection are also highlighted. Finally, we present the open issues for further research in the area of multimodal fusion. \textcopyright{} 2010 Springer-Verlag.},
  isbn = {0-05-300100-1},
  keywords = {Multimedia analysis,Multimodal information fusion},
  file = {/home/cameron/Zotero/storage/564E2PGN/Multimodal_fusion_for_multimedia_analysis_A_survey.pdf}
}

@article{Atzmon2019,
  title = {Probabilistic Robust Multi-Agent Path Finding},
  author = {Atzmon, Dor and Felner, Ariel and Stern, Roni},
  year = {2019},
  journal = {Proceedings of the 12th International Symposium on Combinatorial Search, SoCS 2019},
  number = {Icaps},
  pages = {162--163},
  isbn = {9781577358084},
  keywords = {ICAPS Main},
  file = {/home/cameron/Zotero/storage/PGIGJ5KH/Atzmon, Felner, Stern - 2019 - Probabilistic robust multi-agent path finding.pdf}
}

@article{Auer2002,
  title = {Finite-Time {{Analysis}} of the {{Multiarmed Bandit Problem}}},
  author = {Auer, Peter and {Cesa-Bianchi}, Nicolo and Fischer, Paul},
  year = {2002},
  journal = {Machine Learning},
  number = {47},
  pages = {235--256},
  abstract = {Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support.},
  keywords = {adaptive allocation rules,bandit problems,finite horizon regret},
  file = {/home/cameron/Zotero/storage/VAYKL6JN/Auer, Cesa-Bianchi, Fischer - 2002 - Finite-time Analysis of the Multiarmed Bandit Problem.pdf}
}

@article{Ayton2016,
  title = {Iterative {{Risk Allocation}}},
  author = {Ayton, Ben and Levine, Steve},
  year = {2016},
  pages = {1--11},
  file = {/home/cameron/Zotero/storage/TIALX38Y/ira-tutorial.pdf}
}

@article{Ayton2019,
  title = {Measurement {{Maximizing Adaptive Sampling}} with {{Risk Bounding Functions}}},
  author = {Ayton, Benjamin and Williams, Brian C. and Camilli, Richard},
  year = {2019},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  pages = {7511--7519},
  issn = {2159-5399},
  doi = {10.1609/aaai.v33i01.33017511},
  abstract = {In autonomous exploration a mobile agent must adapt to new measurements to seek high reward, but disturbances cause a probability of collision that must be traded off against expected reward. This paper considers an autonomous agent tasked with maximizing measurements from a Gaussian Process while subject to unbounded disturbances. We seek an adaptive policy in which the maximum allowed probability of failure is constrained as a function of the expected reward. The policy is found using an extension to Monte Carlo Tree Search (MCTS) which bounds probability of failure. We apply MCTS to a sequence of approximating problems, which allows constraint satisfying actions to be found in an anytime manner. Our innovation lies in defining the approximating problems and replanning strategy such that the probability of failure constraint is guaranteed to be satisfied over the true policy. The approach does not need to plan for all measurements explicitly or constrain planning based only on the measurements that were observed. To the best of our knowledge, our approach is the first to enforce probability of failure constraints in adaptive sampling. Through experiments on real bathymetric data and simulated measurements, we show our algorithm allows an agent to take dangerous actions only when the reward justifies the risk. We then verify through Monte Carlo simulations that failure bounds are satisfied.},
  keywords = {State Space Diameter; Causal Graph; Plan Length Bo},
  file = {/home/cameron/Zotero/storage/752PML58/4742-Article Text-7781-1-10-20190707.pdf}
}

@article{bach2019,
  title = {{{REQUIREMENTS}} , {{RESOURCE PLANNING AND MANAGEMENT FOR DECREWING}} / {{RECREWING SCENARIOS OF THE INTERNATIONAL SPACE}}},
  author = {Bach, David A and Brand, Susan N and Hasbrook, Peter V},
  year = {2019},
  number = {1},
  file = {/home/cameron/Zotero/storage/YCXC4UUK/Bach, Brand, Hasbrook - 2019 - REQUIREMENTS , RESOURCE PLANNING AND MANAGEMENT FOR DECREWING RECREWING SCENARIOS OF THE INTERNATIONAL S.pdf}
}

@article{Bak2015,
  title = {{{HYST}}: {{A}} Source Transformation and Translation Tool for Hybrid Automaton Models},
  author = {Bak, Stanley and Bogomolov, Sergiy and Johnson, Taylor T.},
  year = {2015},
  journal = {Proceedings of the 18th International Conference on Hybrid Systems: Computation and Control, HSCC 2015},
  number = {February 2016},
  pages = {128--133},
  doi = {10.1145/2728606.2728630},
  abstract = {A number of powerful and scalable hybrid systems model checkers have recently emerged. Although all of them honor roughly the same hybrid systems semantics, they have drastically different model description languages. This situation (a) makes it difficult to quickly evaluate a specific hybrid automaton model using the different tools, (b) obstructs comparisons of reachability approaches, and (c) impedes the widespread application of research results that perform model modification and could benefit many of the tools. In this paper, we present Hyst, a Hybrid Source Transformer. Hyst is a source-to-source translation tool, currently taking input in the SpaceEx model format, and translating to the formats of HyCreate, Flow{${_\ast}$}, or dReach. Internally, the tool supports generic model-to-model transformation passes that serve to both ease the translation and potentially improve reachability results for the supported tools. Although these model transformation passes could be implemented within each tool, the Hyst approach provides a single place for model modification, generating modified input sources for the unmodified target tools. Our evaluation demonstrates Hyst is capable of automatically translating benchmarks in several classes (including affine and nonlinear hybrid automata) to the input formats of several tools. Additionally, we illustrate a general model transformation pass based on pseudo-invariants implemented in Hyst that illustrates the reachability improvement.},
  isbn = {9781450334334},
  keywords = {Formal methods,Hybrid systems,Reachability},
  file = {/home/cameron/Zotero/storage/ZMRP486S/Bak, Bogomolov, Johnson - 2015 - HYST A source transformation and translation tool for hybrid automaton models.pdf}
}

@article{Bak2017,
  title = {Rigorous Simulation-Based Analysis of Linear Hybrid Systems},
  author = {Bak, Stanley and Duggirala, Parasara Sridhar},
  year = {2017},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {10205 LNCS},
  pages = {555--572},
  issn = {16113349},
  doi = {10.1007/978-3-662-54577-5_32},
  abstract = {Design analysis of Cyber-Physical Systems (CPS) with complex continuous and discrete behaviors, in-practice, relies heavily on numerical simulations. While useful for evaluation and debugging, such analysis is often incomplete owing to the nondeterminism in the discrete transitions and the uncountability of the continuous space. In this paper, we present a precise notion of simulations for CPS called simulation-equivalent reachability, which includes all the states that can be reached by any simulation. While this notion is weaker than traditional reachability, we present a technique that performs simulation-equivalent reachability in an efficient, scalable, and theoretically sound and complete manner. For achieving this, we describe two improvements, namely invariant constraint propagation for handling invariants and on-demand successor deaggregation for handling discrete transitions. We use our tool implementation of the approach, HyLAA (Hybrid Linear Automata Analyzer), to evaluate the improvements, and demonstrate computing the simulation-equivalent reachable set for a replicated helicopter systems with over 1000 dimensions in about 10 min.},
  isbn = {9783662545768},
  file = {/home/cameron/Zotero/storage/CT8TXIBE/Bak, Duggirala - 2017 - Rigorous simulation-based analysis of linear hybrid systems.pdf}
}

@article{baker1996,
  title = {Continuous Approximation with Embedded {{Runge-Kutta}} Methods},
  author = {Baker, T. S. and Dormand, J. R. and Gilmore, J. P. and Prince, P. J.},
  year = {1996},
  journal = {Applied Numerical Mathematics},
  issn = {01689274},
  doi = {10.1016/s0168-9274(96)00025-6},
  abstract = {The criteria to be satisfied by "dense" formulae associated with Runge-Kutta embedded pairs are considered. From a new criterion based on global error considerations, new continuous formulae are derived and tested for some well known efficient pairs.},
  keywords = {Continuous extensions,Dense output,Global error,Ordinary differential equations,Runge-Kutta pairs}
}

@techreport{barnard1963,
  title = {New {{Methods}} of {{Quality Control}}},
  author = {Barnard, G A},
  year = {1963},
  journal = {Source: Journal of the Royal Statistical Society. Series A (General)},
  volume = {126},
  number = {2},
  pages = {255--258},
  file = {/home/cameron/Zotero/storage/JGGL7ACA/new-methods-of-quality-control.pdf}
}

@techreport{bartlett,
  title = {Computer {{Vision}} and {{Pattern Recognition}} 2005 {{Recognizing Facial Expression}}: {{Machine Learning}} and {{Application}} to {{Spontaneous Behavior}}},
  author = {Bartlett, Marian Stewart and Littlewort, Gwen and Frank, Mark and Lainscsek, Claudia and Fasel, Ian and Movellan, Javier},
  abstract = {We present a systematic comparison of machine learning methods applied to the problem of fully automatic recognition of facial expressions. We report results on a series of experiments comparing recognition engines, including Ad-aBoost, support vector machines, linear discriminant analysis. We also explored feature selection techniques, including the use of AdaBoost for feature selection prior to classification by SVM or LDA. Best results were obtained by selecting a subset of Gabor filters using AdaBoost followed by classification with Support Vector Machines. The system operates in real-time, and obtained 93\% correct generalization to novel subjects for a 7-way forced choice on the Cohn-Kanade expression dataset. The outputs of the clas-sifiers change smoothly as a function of time and thus can be used to measure facial expression dynamics. We applied the system to to fully automated recognition of facial actions (FACS). The present system classifies 17 action units, whether they occur singly or in combination with other actions , with a mean accuracy of 94.8\%. We present preliminary results for applying this system to spontaneous facial expressions.},
  file = {/home/cameron/Zotero/storage/67NRDMXB/Bartlett_CVPR05.pdf}
}

@article{Bartlett2005,
  title = {Recognizing Facial Expression: {{Machine}} Learning and Application to Spontaneous Behavior},
  author = {Bartlett, Marian Stewart and Littlewort, Gwen and Frank, Mark and Lainscsek, Claudia and Fasel, Ian and Movellan, Javier},
  year = {2005},
  journal = {Proceedings - 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, CVPR 2005},
  volume = {II},
  pages = {568--573},
  issn = {10636919},
  doi = {10.1109/CVPR.2005.297},
  abstract = {We present a systematic comparison of machine learning methods applied to the problem of fully automatic recognition of facial expressions. We report results on a series of experiments comparing recognition engines, including AdaBoost, support vector machines, linear discriminant analysis. We also explored feature selection techniques, including the use of AdaBoost for feature selection prior to classification by SVM or LDA. Best results were obtained by selecting a subset of Gabor filters using AdaBoost followed by classification with support vector machines. The system operates in real-time, and obtained 93\% correct generalization to novel subjects for a 7-way forced choice on the Cohn-Kanade expression dataset. The outputs of the classifiers change smoothly as a function of time and thus can be used to measure facial expression dynamics. We applied the system to to fully automated recognition of facial actions (FACS). The present system classifies 17 action units, whether they occur singly or in combination with other actions, with a mean accuracy of 94.8\%. We present preliminary results for applying this system to spontaneous facial expressions.},
  isbn = {0769523722},
  file = {/home/cameron/Zotero/storage/PVLUXNMX/Bartlett_CVPR05.pdf}
}

@article{Baxter2008,
  title = {[ {{No Title}} ]},
  author = {Baxter, R. and Hastings, N. and Law, a. and Glass, E. J..},
  year = {2008},
  journal = {Animal Genetics},
  volume = {39},
  number = {5},
  pages = {561--563},
  keywords = {1999,also,antigen,bola,bola-drb3,bovine,called the bovine leucocyte,complex,genotyping,includes many immune-related genes,lewin et al,mhc,of cattle,the major histocompatibility complex},
  file = {/home/cameron/Zotero/storage/UL5NCET6/Mirmalek 2017 Interactions.pdf}
}

@article{Bayardo1997,
  title = {Using {{CSP}} Look-Back Techniques to Solve Exceptionally Hard {{SAT}} Instances},
  author = {Bayardo, Roberto J. and Schrag, Robert},
  year = {1997},
  journal = {Proc. of the 14th National Conference on Artificial Intelligence},
  pages = {203--208},
  issn = {16113349},
  doi = {10.1007/3-540-61551-2_65},
  abstract = {While CNF prepositional satisfiability (SAT) is a sub-class of the more general constraint satisfaction problem (CSP), conventional wisdom has it that some well-known CSP look-back techniques - including backjumping and learning \textemdash{} are of little use for SAT. We enhance the Tableau SAT algorithm of Crawford and Auton with look-back techniques and evaluate its performance on problems specifically designed to challenge it. The Random 3-SAT problem space has commonly been used to benchmark SAT algorithms because consistently difficult instances can be found near a region known as the phase transition. We modify Random 3-SAT in two ways which make instances even harder. First, we evaluate problems with structural regularities and find that CSP look-back techniques offer little advantage. Second, we evaluate problems in which a hard unsatisfiable instance of medium size is embedded in a larger instance, and we find the look-back enhancements to be indispensable. Without them, most instances are "exceptionally hard" -orders of magnitude harder than typical Random 3-SAT instances with the same surface characteristics.},
  isbn = {3540615512},
  file = {/home/cameron/Zotero/storage/NHVDVMD2/Bayardo, Schrag - 1997 - Using CSP look-back techniques to solve exceptionally hard SAT instances.pdf}
}

@article{Beardwood1959,
  title = {The Shortest Path through Many Points},
  author = {Beardwood, Jillian and Halton, J. H. and Hammersley, J. M.},
  year = {1959},
  journal = {Mathematical Proceedings of the Cambridge Philosophical Society},
  volume = {55},
  number = {4},
  pages = {299--327},
  issn = {14698064},
  doi = {10.1017/S0305004100034095},
  abstract = {We prove that the length of the shortest closed path through n points in a bounded plane region of area v is `almost always' asymptotically proportional to {$\surd$}(nv) for large n; and we extend this result to bounded Lebesgue sets in k-dimensional Euclidean space. The constants of proportionality depend only upon the dimensionality of the space, and are independent of the shape of the region. We give numerical bounds for these constants for various values of k; and we estimate the constant in the particular case k = 2. The results are relevant to the travelling-salesman problem, Steiner's street network problem, and the Loberman\textendash Weinberger wiring problem. They have possible generalizations in the direction of Plateau's problem and Douglas' problem. \textcopyright{} 1959, Cambridge Philosophical Society. All rights reserved.},
  file = {/home/cameron/Zotero/storage/8WGYN9WY/Beardwood, Halton, Hammersley - 1959 - The shortest path through many points.pdf}
}

@article{Beaton2019,
  title = {Using {{Science-Driven Analog Research}} to {{Investigate Extravehicular Activity Science Operations Concepts}} and {{Capabilities}} for {{Human Planetary Exploration}}},
  author = {Beaton, Kara H. and Chappell, Steven P. and Abercromby, Andrew F.J. and Miller, Matthew J. and Kobs Nawotniak, Shannon E. and Brady, Allyson L. and Stevens, Adam H. and Payler, Samuel J. and Hughes, Scott S. and Lim, Darlene S.S.},
  year = {2019},
  month = mar,
  journal = {Astrobiology},
  volume = {19},
  number = {3},
  pages = {300--320},
  publisher = {{Mary Ann Liebert Inc.}},
  issn = {15311074},
  doi = {10.1089/ast.2018.1861},
  abstract = {Biologic Analog Science Associated with Lava Terrains (BASALT) is a science-driven exploration program seeking to determine the best tools, techniques, training requirements, and execution strategies for conducting Mars-relevant field science under spaceflight mission conditions. BASALT encompasses Science, Science Operations, and Technology objectives. This article outlines the BASALT Science Operations background, strategic research questions, study design, and a portion of the results from the second field test. BASALT field tests are used to iteratively develop, integrate, test, evaluate, and refine new concepts of operations (ConOps) and capabilities that enable efficient and productive science. This article highlights the ConOps investigated during BASALT in light of future planetary extravehicular activity (EVA), which will focus on scientific exploration and discovery, and serves as an introduction to integrating exploration flexibility with operational rigor, the value of tactical and strategic science planning and execution, and capabilities that enable and enhance future science EVA operations.},
  keywords = {Concepts of operations.,Extravehicular activity,Extreme environments,Human spaceflight analog,Science operations},
  file = {/home/cameron/Zotero/storage/UGLKRAEI/Beaton et al. - 2019 - Using Science-Driven Analog Research to Investigate Extravehicular Activity Science Operations Concepts and Capab.pdf}
}

@article{Beer2014,
  title = {Toward a {{Framework}} for {{Levels}} of {{Robot Autonomy}} in {{Human-Robot Interaction}}},
  author = {Beer, Jenay M and Fisk, Arthur D and Rogers, Wendy A},
  year = {2014},
  journal = {Journal of Human-Robot Interaction},
  volume = {3},
  number = {2},
  pages = {74},
  issn = {2163-0364},
  doi = {10.5898/jhri.3.2.beer},
  abstract = {14.00 Autonomy is a critical construct related to human-robot interaction (HRI) and varies widely across robot platforms. Levels of robot autonomy (LORA), ranging from teleoperation to fully autonomous systems, influence the way in which humans and robots interact with one another. Thus, there is a need to understand HRI by identifying variables that influence\textemdash and are influenced by\textemdash robot autonomy. Our overarching goal is to develop a framework for LORA in HRI. To reach this goal, our framework draws links between HRI and human-automation interaction, a field with a long history of studying and understanding human-related variables. The construct of autonomy is reviewed and redefined within the context of HRI. Additionally, this framework proposes a process for determining a robot's autonomy level by categorizing autonomy along a 10-point taxonomy. The framework is intended to be treated as a guideline for determining autonomy, categorizing the LORA along a qualitative taxonomy and considering HRI variables (e.g., acceptance, situation awareness, reliability) that may be influenced by the LORA Normal 0 false false false EN-US JA X-NONE /* Style Definitions */ table.MsoNormalTable \{mso-style-name:"Table Normal"; mso-tstyle-rowband-size:0; mso-tstyle-colband-size:0; mso-style-noshow:yes; mso-style-priority:99; mso-style-parent:""; mso-padding-alt:0in 5.4pt 0in 5.4pt; mso-para-margin-top:0in; mso-para-margin-right:0in; mso-para-margin-bottom:10.0pt; mso-para-margin-left:0in; line-height:115\%; mso-pagination:widow-orphan; font-size:11.0pt; font-family:"Calibri","sans-serif"; mso-ascii-font-family:Calibri; mso-ascii-theme-font:minor-latin; mso-hansi-font-family:Calibri; mso-hansi-theme-font:minor-latin;\}},
  keywords = {automation,autonomy,framework,human-robot interaction,levels of robot autonomy},
  file = {/home/cameron/Zotero/storage/RHTSGAKA/JHRI.3.2.Beer.pdf}
}

@inproceedings{bera2016,
  title = {{{GLMP-}} Realtime Pedestrian Path Prediction Using Global and Local Movement Patterns},
  booktitle = {Proceedings - {{IEEE International Conference}} on {{Robotics}} and {{Automation}}},
  author = {Bera, Aniket and Kim, Sujeong and Randhavane, Tanmay and Pratapa, Srihari and Manocha, Dinesh},
  year = {2016},
  volume = {2016-June},
  pages = {5528--5535},
  issn = {10504729},
  doi = {10.1109/ICRA.2016.7487768},
  abstract = {We present a novel real-time algorithm to predict the path of pedestrians in cluttered environments. Our approach makes no assumption about pedestrian motion or crowd density, and is useful for short-term as well as long-term prediction. We interactively learn the characteristics of pedestrian motion and movement patterns from 2D trajectories using Bayesian inference. These include local movement patterns corresponding to the current and preferred velocities and global characteristics such as entry points and movement features. Our approach involves no precomputation and we demonstrate the real-time performance of our prediction algorithm on sparse and noisy trajectory data extracted from dense indoor and outdoor crowd videos. The combination of local and global movement patterns can improve the accuracy of long-term prediction by 12-18\% over prior methods in high-density videos.},
  isbn = {978-1-4673-8026-3},
  file = {/home/cameron/Zotero/storage/PXXU53TS/Aniket_ICRA_2016.pdf}
}

@article{Bernard1998,
  title = {Design of the {{Remote Agent}} Experiment for Spacecraft Autonomy},
  author = {Bernard, D. E. and Dorais, G. A. and Fry, C. and Gamble, E. B. and Kanefsky, B. and Kurien, J. and Millar, W. and Muscettola, N. and Nayak, P. P. and Pell, B. and Rajan, K. and Rouquette, N. and Smith, B. and Williams, Brian C.},
  year = {1998},
  journal = {IEEE Aerospace Conference Proceedings},
  volume = {2},
  number = {August 2015},
  pages = {259--281},
  issn = {1095323X},
  doi = {10.1109/AERO.1998.687914},
  abstract = {This paper describes the Remote Agent flight experiment for\textbackslash nspacecraft commanding and control. In the Remote Agent approach, the\textbackslash noperational rules and constraints are encoded in the flight software.\textbackslash nThe software may be considered to be an autonomous \&amp;ldquo;remote\textbackslash nagent\&amp;rdquo; of the spacecraft operators in the sense that the operators\textbackslash nrely on the agent to achieve particular goals. The experiment will be\textbackslash nexecuted during the flight of NASA's Deep Space One technology\textbackslash nvalidation mission. During the experiment, the spacecraft will not be\textbackslash ngiven the usual detailed sequence of commands to execute. Instead, the\textbackslash nspacecraft will be given a list of goals to achieve during the\textbackslash nexperiment. In flight, the Remote Agent flight software will generate a\textbackslash nplan to accomplish the goals and then execute the plan in a robust\textbackslash nmanner while keeping track of how well the plan is being accomplished.\textbackslash nDuring plan execution, the Remote Agent stays on the lookout for any\textbackslash nhardware faults that might require recovery actions or replanning. In\textbackslash naddition to describing the design of the remote agent, this paper\textbackslash ndiscusses technology-insertion challenges and the approach used in the\textbackslash nRemote Agent approach to address these challenges. The experiment\textbackslash nintegrates several spacecraft autonomy technologies developed at NASA\textbackslash nAmes and the Jet Propulsion Laboratory: on-board planning, a robust\textbackslash nmulti threaded executive, and model-based failure diagnosis and recovery\textbackslash n},
  isbn = {0780343115},
  file = {/home/cameron/Zotero/storage/DV5KQY8Y/ieee-aero.pdf}
}

@article{Bezanson2017,
  title = {Julia: {{A}} Fresh Approach to Numerical Computing},
  author = {Bezanson, Jeff and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  year = {2017},
  journal = {SIAM Review},
  volume = {59},
  number = {1},
  eprint = {1411.1607},
  eprinttype = {arxiv},
  pages = {65--98},
  issn = {00361445},
  doi = {10.1137/141000671},
  abstract = {Bridging cultures that have often been distant, Julia combines expertise from the diverse helds of computer science and computational science to create a new approach to numerical computing. Julia is designed to be easy and fast and questions notions generally held to be "laws of nature" by practitioners of numerical computing: 1. High-level dynamic programs have to be slow. 2. One must prototype in one language and then rewrite in another language for speed or deployment. 3. There are parts of a system appropriate for the programmer, and other parts that are best left untouched as they have been built by the experts. We introduce the Julia programming language and its design-a dance between specialization and abstraction. Specialization allows for custom treatment. Multiple dispatch, a technique from computer science, picks the right algorithm for the right circumstance. Abstraction, which is what good computation is really about, recognizes what remains the same after differences are stripped away. Abstractions in mathematics are captured as code through another technique from computer science, generic programming. Julia shows that one can achieve machine performance without sacrificing human convenience.},
  archiveprefix = {arXiv},
  keywords = {Julia,Numerical,Parallel,Scientific computing},
  file = {/home/cameron/Zotero/storage/VI43NIA8/julia-fresh-approach-BEKS.pdf}
}

@article{Bezanson2018,
  title = {Julia: Dynamism and Performance Reconciled by Design},
  author = {Bezanson, Jeff and Chen, Jiahao and Chung, Benjamin and Karpinski, Stefan and Shah, Viral B. and Vitek, Jan and Zoubritzky, Lionel},
  year = {2018},
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {2},
  number = {OOPSLA},
  pages = {1--23},
  issn = {2475-1421},
  doi = {10.1145/3276490},
  abstract = {Julia is a programming language for the scientific community that combines features of productivity languages, such as Python or MATLAB, with characteristics of performance-oriented languages, such as C++ or Fortran. Julia's productivity features include: dynamic typing, automatic memory management, rich type annotations, and multiple dispatch. At the same time, Julia allows programmers to control memory layout and leverages a specializing just-in-time compiler to eliminate much of the overhead of those features. This paper details the design choices made by the creators of Julia and reflects on the implications of those choices for performance and usability.},
  file = {/home/cameron/Zotero/storage/4AJYEFHI/m-api-782d0d4e-85a4-1d3b-45e2-9b0f66903a7d.pdf}
}

@article{Bhargava2018,
  title = {Variable-Delay Controllability},
  author = {Bhargava, Nikhil and Muise, Christian and Williams, Brian C.},
  year = {2018},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  volume = {2018-July},
  pages = {4660--4666},
  issn = {10450823},
  doi = {10.24963/ijcai.2018/648},
  abstract = {In temporal planning, agents must schedule a set of events satisfying a set of predetermined constraints. These scheduling problems become more difficult when the duration of certain actions are outside the agent's control. Delay controllability is the generalized notion of whether a schedule can be constructed in the face of uncertainty if the agent eventually learns when events occur. Our work introduces the substantially more complex setting of determining variable-delay controllability, where an agent learns about events after some unknown but bounded amount of time has passed. We provide an efficient O(n3) variable-delay controllability checker and show how to create an execution strategy for variable-delay controllability problems. To our knowledge, these essential capabilities are absent from existing controllability checking algorithms. We conclude by providing empirical evaluations of the quality of variable-delay controllability results as compared to approximations that use fixed delays to model the same problems.},
  isbn = {9780999241127},
  keywords = {Planning and Scheduling: Planning and Scheduling,Planning and Scheduling: Planning under Uncertaint},
  file = {/home/cameron/Zotero/storage/97IPCWL3/0648.pdf}
}

@techreport{Bhargava2018a,
  title = {Delay {{Controllability}} : {{Multi-Agent Coordination}} under {{Communication Delay}}},
  author = {Bhargava, Nikhil and Muise, Christian and Vaquero, Tiago and Williams, Brian},
  year = {2018},
  pages = {36},
  address = {{Cambridge, MA}},
  institution = {{Massachusetts Institute of Technology}},
  abstract = {Simple Temporal Networks with Uncertainty provide a useful framework for modeling temporal constraints and, importantly, for modeling actions with uncertain durations. To determine whether we can construct a schedule for a given network, we typically consider one of two types of controllability: dynamic or strong. These controllability checks have strict conditions on how uncertainty is resolved; uncertain outcomes are either recognized immediately or not at all. In this paper, we introduce delay controllability, a novel generalization of both strong and dynamic controllability that additionally exposes a large range of controllability classes in between. To do so, we use a delay function to parameterize our controllability checking. This delay function represents the difference between when an event happens and the time that it is observed. We also provide a single unified algorithm for checking delay controllability that runs in O(n\^3) time, matching the best known runtime for dynamic controllability, which we use to motivate the decision to generalize dynamic and strong controllability. We conclude by providing an empirical evaluation of delay controllability, demonstrating its superior accuracy and practical efficiency as compared to other existing approximations.},
  keywords = {â˜…,Bhargava18},
  file = {/home/cameron/Zotero/storage/DKG74Z3N/35681123147c4d7cb4cca3e5e7f40faeecfa.pdf}
}

@article{Bhargava2018b,
  title = {Managing Communication Costs under Temporal Uncertainty},
  author = {Bhargava, Nikhil and Muise, Christian and Vaquero, Tiago and Williams, Brian},
  year = {2018},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  volume = {2018-July},
  pages = {84--90},
  issn = {10450823},
  doi = {10.24963/ijcai.2018/12},
  abstract = {In multi-agent temporal planning, individual agents cannot know a priori when other agents will execute their actions and so treat those actions as uncertain. Only when others communicate the results of their actions is that uncertainty resolved. If a full communication protocol is specified ahead of time, then delay controllability can be used to assess the feasibility of the temporal plan. However, agents often have flexibility in choosing when to communicate the results of their action. In this paper, we address the question of how to choose communication protocols that guarantee the feasibility of the original temporal plan subject to some cost associated with that communication. To do so, we introduce a means of extracting delay controllability conflicts and show how we can use these conflicts to more efficiently guide our search. We then present three conflict-directed search algorithms and explore the theoretical and empirical trade-offs between the different approaches.},
  isbn = {9780999241127},
  keywords = {Agent-based and Multi-agent Systems: Multi-agent P,Planning and Scheduling: Planning and Scheduling,Planning and Scheduling: Search in Planning and Sc},
  file = {/home/cameron/Zotero/storage/ZVS6GK7T/Bhargava et al. - 2018 - Managing communication costs under temporal uncertainty.pdf}
}

@article{Bhargava2019,
  title = {Faster Dynamic Controllability Checking in Temporal Networks with Integer Bounds},
  author = {Bhargava, Nikhil and Williams, Brian C.},
  year = {2019},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  volume = {2019-Augus},
  pages = {5509--5515},
  issn = {10450823},
  doi = {10.24963/ijcai.2019/765},
  abstract = {Simple Temporal Networks with Uncertainty (STNUs) provide a useful formalism with which to reason about events and the temporal constraints that apply to them. STNUs are in particular notable because they facilitate reasoning over stochastic, or uncontrollable, actions and their corresponding durations. To evaluate the feasibility of a set of constraints associated with an STNU, one checks the network's dynamic controllability, which determines whether an adaptive schedule can be constructed on-the-fly. Our work improves the runtime of checking the dynamic controllability of STNUs with integer bounds to O (min(mn, mn log N) + km + k2n + kn log n). Our approach pre-processes the STNU using an existing O(n3) dynamic controllability checking algorithm and provides tighter bounds on its runtime. This makes our work easily adaptable to other algorithms that rely on checking variants of dynamic controllability.},
  isbn = {9780999241141},
  file = {/home/cameron/Zotero/storage/BTWNWYMV/ijcai19.pdf}
}

@article{Bhargava2019a,
  title = {Complexity Bounds for the Controllability of Temporal Networks with Conditions, Disjunctions, and Uncertainty},
  author = {Bhargava, Nikhil and Williams, Brian C.},
  year = {2019},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  volume = {2019-Augus},
  eprint = {1901.02307v1},
  eprinttype = {arxiv},
  pages = {6353--6357},
  issn = {10450823},
  doi = {10.24963/ijcai.2019/886},
  abstract = {In temporal planning, many different temporal network formalisms are used to model real world situations. Each of these formalisms has different features which affect how easy it is to determine whether the underlying network of temporal constraints is consistent. While many of the simpler models have been well-studied from a computational complexity perspective, the algorithms developed for advanced models which combine features have very loose complexity bounds. In this work, we provide tight completeness bounds for strong, weak, and dynamic controllability checking of temporal networks that have conditions, disjunctions, and temporal uncertainty. Our work exposes some of the subtle differences between these different structures and, remarkably, establishes a guarantee that all of these problems are computable in PSPACE.},
  archiveprefix = {arXiv},
  isbn = {9780999241141},
  keywords = {temporal planning,temporal uncertainty},
  file = {/home/cameron/Zotero/storage/DQWNBQAV/Bhargava, Williams - 2019 - Complexity bounds for the controllability of temporal networks with conditions, disjunctions, and uncertaint.pdf}
}

@article{Bhargava2019b,
  title = {Multiagent Disjunctive Temporal Networks},
  author = {Bhargava, Nikhil and Williams, Brian},
  year = {2019},
  journal = {Proceedings of the International Joint Conference on Autonomous Agents and Multiagent Systems, AAMAS},
  volume = {1},
  pages = {458--466},
  issn = {15582914},
  abstract = {Temporal network formalisms allow us to encode a set of constraints relating distinct events in time, and by deploying algorithms over these networks, we can determine whether schedules for these networks exist that satisfy all constraints. By augmenting simple temporal networks, we can consider the effects that disjunctive constraints, temporal uncertainty, and coordinating agents have on modeling fidelity and the algorithmic efficiency of schedule construction. In this paper, we introduce Partially Observable Disjunctive Temporal Networks with Uncertainty (PODTNUs) and Multiagent Disjunctive Temporal Networks with Uncertainty (MaDTNUs), generalizing previously studied multi-agent variants of temporal networks. We provide the first theoretical completeness results for the controllability of multiagent temporal network structures and discuss the importance of these results for modelers.},
  isbn = {9781510892002},
  keywords = {Coordination and control models for multiagent sys,Single and multiagent planning and scheduling},
  file = {/home/cameron/Zotero/storage/I3ZDJSFY/Bhargava, Williams - 2019 - Multiagent disjunctive temporal networks.pdf}
}

@phdthesis{Bhargava2020,
  title = {Multi-{{Agent Coordination}} under {{Limited Communication}}},
  author = {Bhargava, Nikhil},
  year = {2020},
  number = {2000},
  abstract = {In this thesis, we present a theory for constructing real-time executives that can rea- son about communication between agents. In multi-agent coordination problems, different agents have different beliefs about the state of the world that can eventually be reconciled if the agents are able to share sufficient information with one another. When communication is limited, this task becomes more difficult. To achieve robust- ness, coordination decisions need to be made, executed, and adapted in real-time by a real-time executive. Most existing real-time executives rely on perfect knowledge of the state of the world, making it difficult to use them in scenarios where agents either cannot or prefer not to communicate and share information. This thesis offers three contributions that together provide the basis for constructing a real-time executive capable of handling multi-agent coordination under limited communication. First, we introduce delay controllability as a way to augment the input plan rep- resentation to including a communication model. Delay controllability lets us reason about multi-agent activities under limited communication in the form of communica- tion delays and provides a guarantee that problem evaluation is tractable. Second, we provide a way to indicate by when each agent must communicate the results of their actions. Many agents have flexibility in choosing exactly when to communicate. We provide an algorithm for choosing a low-cost set of moments to communicate and present a strategy for adjusting those strategies when commu- nication networks are unreliable causing disruptions in the original communication plan. Third, we offer a way to model noisy communication. Noisy communication offers approximate temporal information that is useful during execution but is generally difficult to incorporate. We introduce variable-delay controllability as a way to model this kind of communication and provide the first sound and complete algorithm for incorporating noisy information that runs in polynomial time},
  school = {Massachusetts Institute of Technology},
  file = {/home/cameron/Zotero/storage/NHTVGTCC/m-api-4140a74b-4d25-ac3d-06bc-49543c16c5a1.pdf}
}

@techreport{Bindel2011,
  title = {Parallel {{All-Pairs Shortest Paths}}},
  author = {Bindel, David},
  year = {2011},
  institution = {{Cornell University}},
  file = {/home/cameron/Zotero/storage/MQ6XTK4R/Unknown - 2011 - Parallel All-Pairs Shortest Paths.pdf}
}

@article{Bingham2001,
  title = {Random Projection in Dimensionality Reduction: {{Applications}} to Image and Text Data},
  author = {Bingham, Ella and Mannila, Heikki},
  year = {2001},
  journal = {Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages = {245--250},
  issn = {1362-704X},
  doi = {10.1145/502512.502546},
  abstract = {Random projections have recently emerged as a powerful method for dimensionality reduction. Theoretical results indicate that the method preserves distances quite nicely; however, empirical results are sparse. We present experimental results on using random projection as a dimensionality reduction tool in a number of cases, where the high dimensionality of the data would otherwise lead to burdensome computations. Our application areas are the processing of both noisy and noiseless images, and information retrieval in text documents. We show that projecting the data onto a random lower-dimensional subspace yields results comparable to conventional dimensionality reduction methods such as principal component analysis: the similarity of data vectors is preserved well under random projection. However, using random projections is computationally significantly less expensive than using, e.g., principal component analysis. We also show experimentally that using a sparse random matrix gives additional computational savings in random projection.},
  isbn = {158113391X},
  file = {/home/cameron/Zotero/storage/3729GUHQ/random-projection.pdf}
}

@article{Binney2010,
  title = {Informative Path Planning for an Autonomous Underwater Vehicle},
  author = {Binney, Jonathan and Krause, Andreas and Sukhatme, Gaurav S.},
  year = {2010},
  journal = {Proceedings - IEEE International Conference on Robotics and Automation},
  pages = {4791--4796},
  issn = {10504729},
  doi = {10.1109/ROBOT.2010.5509714},
  abstract = {We present a path planning method for autonomous underwater vehicles in order to maximize mutual information. We adapt a method previously used for surface vehicles, and extend it to deal with the unique characteristics of underwater vehicles. We show how to generate near-optimal paths while ensuring that the vehicle stays out of high-traffic areas during predesignated time intervals. In our objective function we explicitly account for the fact that underwater vehicles typically take measurements while moving, and that they do not have the ability to communicate until they resurface. We present field results from ocean trials on planning paths for a specific AUV, an underwater glider. \textcopyright 2010 IEEE.},
  isbn = {9781424450381},
  file = {/home/cameron/Zotero/storage/RD4SRJV9/642.pdf}
}

@article{Bit-Monnot2016,
  title = {Which Contingent Events to Observe for the Dynamic Controllability of a Plan},
  author = {{Bit-Monnot}, Arthur and Ghallab, Malik and Ingrand, Felix},
  year = {2016},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  volume = {IJCAI-16},
  number = {July},
  pages = {3038--3044},
  issn = {10450823},
  abstract = {Planning and acting in a dynamic environment require distinguishing controllable and contingent events and checking the dynamic controllability of plans. Known procedures for testing the dynamic controllability assume that all contingent events are observable. Often this assumption does not hold. We consider here the general case of networks with invisible as well as observable contingent points. We propose a first procedure for testing their dynamic controllability. Further, we define an algorithm for choosing among the observable contingent points which to observe with additional sensing actions, such as to make a plan dynamically controllable. We show how these procedures can be incrementally integrated into a constraint-based temporal planner.},
  file = {/home/cameron/Zotero/storage/UBEI2QGS/POSTNU-hal1.pdf}
}

@article{Blackmore2006,
  title = {A Probabilistic Approach to Optimal Robust Path Planning with Obstacles},
  author = {Blackmore, Lars and Li, Hui and Williams, Brian},
  year = {2006},
  journal = {Proceedings of the American Control Conference},
  volume = {2006},
  number = {December},
  pages = {2831--2837},
  issn = {07431619},
  doi = {10.1109/acc.2006.1656653},
  abstract = {Autonomous vehicles need to plan trajectories to a specified goal that avoid obstacles. Previous approaches that used a constrained optimization approach to solve for unite sequences of optimal control inputs have been highly effective. For robust execution, it is essential to take into account the inherent uncertainty in the problem, which arises due to uncertain localization, modeling errors, and disturbances. Prior work has handled the case of deterministically bounded uncertainty. We present here an alternative approach that uses a probabilistic representation of uncertainty, and plans the future probabilistic distribution of the vehicle state so that the probability of collision with obstacles is below a specified threshold. This approach has two main advantages; first, uncertainty is often modeled more naturally using a probabilistic representation (for example in the case of uncertain localization); second, by specifying the probability of successful execution, the desired level of conservatism in the plan can be specified in a meaningful manner. The key idea behind the approach is that the probabilistic obstacle avoidance problem can be expressed as a Disjunctive Linear Program using linear chance constraints. The resulting Disjunctive Linear Program has the same complexity as that corresponding to the deterministic path planning problem with no representation of uncertainty. Hence the resulting problem can be solved using existing, efficient techniques, such that planning with uncertainty requires minimal additional computation. Finally, we present an empirical validation of the new method with a number of aircraft obstacle avoidance scenarios. \textcopyright{} 2006 IEEE.},
  isbn = {1424402107},
  file = {/home/cameron/Zotero/storage/DY4QDKAU/Blackmore, Li, Williams - 2006 - A probabilistic approach to optimal robust path planning with obstacles.pdf}
}

@article{Blasch2019,
  title = {Autonomy in Use for Space Situation Awareness},
  author = {Blasch, Erik and Shen, Dan and Jia, Bin and Wang, Zhonghai and Chen, Genshe and Chen, Yu and Pham, Khanh},
  year = {2019},
  volume = {1101707},
  number = {July},
  pages = {7},
  issn = {1996756X},
  doi = {10.1117/12.2519212},
  isbn = {9781510627017},
  keywords = {aiu,autonomy in use,machine learning,satcom,space situational awareness},
  file = {/home/cameron/Zotero/storage/93SVJQYC/1101707.pdf}
}

@inproceedings{bleacher2020,
  title = {{{ARTEMIS SCIENCE PLAN}}},
  booktitle = {Lunar and {{Planetary Science XLVIII}}},
  author = {Bleacher, Jake and Bussey, Ben},
  year = {2020},
  address = {{Houston, TX}},
  file = {/home/cameron/Zotero/storage/A63ZDVRQ/Artemis Science Strategy_LSSW-v2.pdf}
}

@phdthesis{Block2007,
  type = {Doctoral},
  title = {Distributed {{Method Selection}} and {{Dispatching}} of {{Contingent}}, {{Temporally Flexible Plans}}},
  author = {Block, Stephen},
  year = {2007},
  month = feb,
  address = {{Cambridge, MA}},
  abstract = {Many applications of autonomous agents require groups to work in tight coordination. To be dependable, these groups must plan, carry out and adapt their activities in a way that is robust to failure and to uncertainty. Previous work developed contingent, temporally flexible plans. These plans provide robustness to uncertain activity durations, through flexible timing constraints, and robustness to plan failure, through alternate approaches to achieving a task. Robust execution of contingent, temporally flexible plans consists of two phases. First, in the plan extraction phase, the executive chooses between the functionally redundant methods in the plan to select an execution sequence that satisfies the temporal bounds in the plan. Second, in the plan execution phase, the executive dispatches the plan, using the temporal flexibility to schedule activities dynamically.},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  file = {/home/cameron/Zotero/storage/PSY8M8VP/Block - Distributed Method Selection and Dispatching of Co.pdf}
}

@article{Blum1997,
  title = {Fast Planning through Planning Graph Analysis},
  author = {Blum, Avrim L. and Furst, Merrick L.},
  year = {1997},
  journal = {Artificial Intelligence},
  volume = {90},
  number = {1-2},
  pages = {281--300},
  issn = {00043702},
  doi = {10.1016/s0004-3702(96)00047-1},
  abstract = {We introduce a new approach to planning in STRIPS-like domains based on constructing and analyzing a compact structure we call a planning graph. We describe a new planner, Graphplan, that uses this paradigm. Graphplan always returns a shortest possible partial-order plan, or states that no valid plan exists. We provide empirical evidence in favor of this approach, showing that Graphplan outperforms the total-order planner, Prodigy, and the partial-order planner, UCPOP, on a variety of interesting natural and artificial planning problems. We also give empirical evidence that the plans produced by Graphplan are quite sensible. Since searches made by this approach are fundamentally different from the searches of other common planning methods, they provide a new perspective on the planning problem. \textcopyright{} 1997 Elsevier Science B.V.},
  keywords = {General purpose planning,Graph algorithms,Planning graph analysis,STRIPS planning},
  file = {/home/cameron/Zotero/storage/J2Z96NUF/graphplan.pdf}
}

@article{Boerkoel2013,
  title = {Distributed {{Reasoning}} for {{Multiagent Simple Temporal Problems Publisher Association}} for the {{Advancement}} of {{Artificial Intelligence Distributed Reasoning}} for {{Multiagent Simple Temporal Problems}}},
  author = {Boerkoel, James C. and Durfee, Edmund H},
  year = {2013},
  journal = {Journal of Artificial Intelligence Research},
  volume = {47},
  pages = {95--156},
  abstract = {This research focuses on building foundational algorithms for scheduling agents that assist people in managing their activities in environments where tempo and complex activity interdependencies outstrip people's cognitive capacity. We address the critical challenge of reasoning over individuals' interacting schedules to efficiently answer queries about how to meet scheduling goals while respecting individual privacy and autonomy to the extent possible. We formally define the Multiagent Simple Temporal Problem for naturally capturing and reasoning over the distributed but interconnected scheduling problems of multiple individuals. Our hypothesis is that combining bottom-up and top-down approaches will lead to effective solution techniques. In our bottom-up phase, an agent externalizes constraints that compactly summarize how its local subproblem affects other agents' subproblems, whereas in our top-down phase an agent proactively constructs and internalizes new local constraints that decouple its subproblem from others'. We confirm this hypothesis by devising distributed algorithms that calculate summaries of the joint solution space for multiagent scheduling problems, without centralizing or otherwise redistributing the problems. The distributed algorithms permit concurrent execution to achieve significant speedup over the current art and also increase the level of privacy and independence in individual agent reasoning. These algorithms are most advantageous for problems where interactions between the agents are sparse compared to the complexity of agents' individual problems.},
  isbn = {1721.1/80710},
  file = {/home/cameron/Zotero/storage/ULLEWAYK/Boerkoel, Durfee - 2013 - Distributed Reasoning for Multiagent Simple Temporal Problems Publisher Association for the Advancement of Art.pdf}
}

@article{Bogomolov2014,
  title = {Planning as Model Checking in Hybrid Domains},
  author = {Bogomolov, Sergiy and Magazzeni, Daniele and Podelski, Andreas and Wehrle, Martin},
  year = {2014},
  journal = {Proceedings of the National Conference on Artificial Intelligence},
  volume = {3},
  pages = {2228--2234},
  abstract = {Planning in hybrid domains is an important and challenging task, and various planning algorithms have been proposed in the last years. From an abstract point of view, hybrid planning domains are based on hybrid automata, which have been studied intensively in the model checking community. In particular, powerful model checking algorithms and tools have emerged for this formalism. However, despite the quest for more scalable planning approaches, model checking algorithms have not been applied to planning in hybrid domains so far. In this paper, we make a first step in bridging the gap between these two worlds. We provide a formal translation scheme from PDDL+ to the standard formalism of hybrid automata, as a solid basis for using hybrid system modelchecking tools for dealing with hybrid planning domains. As a case study, we use the SpaceEx model checker, showing how we can address PDDL+ domains that are out of the scope of state-of-the-art planners.},
  isbn = {9781577356790},
  keywords = {Planning and Scheduling},
  file = {/home/cameron/Zotero/storage/DDAKSSBM/Bogomolov et al. - 2014 - Planning as model checking in hybrid domains.pdf}
}

@article{Bouchard2017,
  title = {Mixed-Initiative System for Tactical Planning Allowing Real-Time Constraint Insertions},
  author = {Bouchard, Jean and Gaudreault, Jonathan and Quimper, Claude Guy and Marier, Philippe and Brotherton, Edith and Simard, Nathaniel},
  year = {2017},
  journal = {IFAC-PapersOnLine},
  volume = {50},
  number = {1},
  pages = {15233--15240},
  publisher = {{Elsevier B.V.}},
  issn = {24058963},
  doi = {10.1016/j.ifacol.2017.08.2376},
  abstract = {In complex industrial contexts, planning is a challenging problem. The companies can use two different approaches: planning can be carried out ``manually'' by a human planner or they can rely on a mathematical model to provide an optimal solution. In this paper, we introduce an Excel add-in allowing the decision-maker to generate their own interactive dashboard. The user can modify the value of some variables and all the other variables are updated in real-time. Moreover, we provide an algorithm to overcome to major drawback of the original approach.},
  keywords = {Algorithms,Decision support systems,Generalized linear systems,Mathematical programming,User interfaces},
  file = {/home/cameron/Zotero/storage/BZPK2BDE/Bouchard et al. - 2017 - Mixed-initiative system for tactical planning allowing real-time constraint insertions.pdf}
}

@article{BoydelaTour1992,
  title = {An Optimality Result for Clause Form Translation},
  author = {{Boy de la Tour}, Thierry},
  year = {1992},
  journal = {Journal of Symbolic Computation},
  volume = {14},
  number = {4},
  pages = {283--301},
  issn = {07477171},
  doi = {10.1016/0747-7171(92)90009-S},
  abstract = {The exponential complexity in size of the standard clause form translation is often considered as a serious drawback of the resolution method. Fortunately, a polynomial translation is possible by first introducing definitions, one for each subformula of the conjecture. This exhaustiveness can however be proved inefficient when the length of proofs is considered. In order to improve this interesting technique, we first generalize it to renamings, which consist in introducing definitions only for a subset of subformulas, resulting in a wide set of possible clause forms from a single conjecture. We show how a simple and efficient algorithm yields a renaming which, on equivalence-free conjectures, minimizes the number of clauses among these clause forms. This translation has been tested on the famous challenge problem by P. Andrews, yielding a spectacular reduction in search space and time, and therefore is one of the more simple and general technique to efficiently produce a resolution proof for this problem. \textcopyright{} 1992.},
  file = {/home/cameron/Zotero/storage/KYLTGKRI/Boy de la Tour - 1992 - An optimality result for clause form translation.pdf}
}

@article{Brady2019b,
  title = {Strategic {{Planning Insights}} for {{Future Science-Driven Extravehicular Activity}} on {{Mars}}},
  author = {Brady, Allyson L. and Kobs Nawotniak, Shannon E. and Hughes, Scott S. and Payler, Samuel J. and Stevens, Adam H. and Cockell, Charles S. and Elphic, Richard C. and Sehlke, Alexander and Haberle, Christopher W. and Slater, Gregory F. and Lim, Darlene S.S.},
  year = {2019},
  month = mar,
  journal = {Astrobiology},
  volume = {19},
  number = {3},
  pages = {347--368},
  issn = {1531-1074},
  doi = {10.1089/ast.2018.1850},
  file = {/home/cameron/Zotero/storage/79E66ZJ3/Brady et al. - 2019 - Strategic Planning Insights for Future Science-Driven Extravehicular Activity on Mars.pdf}
}

@article{Brenner1995,
  title = {A Call for Paleolimnology Studies in the Tropics},
  author = {Brenner, Mark},
  year = {1995},
  journal = {Journal of Paleolimnology},
  volume = {13},
  number = {1},
  pages = {89--92},
  issn = {09212728},
  doi = {10.1007/BF00678114},
  file = {/home/cameron/Zotero/storage/GLBKC9XG/Brenner - 1995 - A call for paleolimnology studies in the tropics.pdf}
}

@article{BrentVenable2010a,
  title = {Weak and Dynamic Controllability of Temporal Problems with Disjunctions and Uncertainty},
  author = {Brent Venable, K. and Volpato, Michele and Peintner, Bart and {Yorke-Smith}, Neil},
  year = {2010},
  journal = {COPLAS 2010 - Proceedings of the Workshop on Constraint Satisfaction Techniques for Planning and Scheduling Problems},
  pages = {50--59},
  abstract = {The Temporal Constraint Satisfaction Problem with Uncertainty (TCSPU) and its disjunctive generalization, the Disjunctive Temporal Problem with Uncertainty (DTPU), are quantitative models for temporal reasoning that account simultaneously for disjunctive constraints and for events not under the control of the executing agent. Such a problem is Weakly Controllable if in each possible scenario the agent can find a decision for that scenario that satisfies all the constraints; further, a problem is Dynamically Controllable if the agent can build a consistent decision online, as the scenario is incrementally revealed. We first consider Weak Controllability. We present two sound and complete algorithms for checking Weak Controllability of DTPUs. The first algorithm needs to examine only a limited number of scenarios, but operates only on a restricted class of problems. The second algorithm is fully general, but is more expensive in terms of space. We then consider Dynamic Controllability. We present a complete algorithm for testing this property for TCSPUs. Complexity results are presented for all three algorithms. \textcopyright{} 2010, Association for the Advancement of Artificial Intelligence.},
  file = {/home/cameron/Zotero/storage/G4AYAAV2/Brent Venable et al. - 2010 - Weak and dynamic controllability of temporal problems with disjunctions and uncertainty.pdf}
}

@article{Bresina1996,
  title = {Heuristic-{{Biased Stochastic Sampling}}},
  author = {Bresina, John L},
  year = {1996},
  pages = {271--278},
  abstract = {This paper presents a search technique for scheduling problems, called Heuristic-Biased Stochastic Sampling (HBSS). The underlying assumption behind the HBSS approach is that strictly adhering to a search heuris-tic often does not yield the best solution and, there-fore, exploration off the heuristic path can prove fruit-ful. Within the HBSS approach, the balance between heuristic adherence and exploration can be controlled according to the confidence one has in the heuristic. By varying this balance, encoded as a bias function, the HBSS approach encompasses a family of search al-gorithms of which greedy search and completely ran-dom search are extreme members. We present empir-ical results from an application of HBSS to the real-world problem of observation scheduling. These re-sults show that with the proper bias function, it can be easy to outperform greedy search.},
  file = {/home/cameron/Zotero/storage/FXH56LRM/AAAI96-041.pdf}
}

@article{Bresina2017,
  title = {Traverse {{Planning}} with {{Temporal-Spatial Constraints}}},
  author = {Bresina, John L. and Morris, Paul H. and Deans, Matthew C. and Cohen, Tamar E. and Lees, David S.},
  year = {2017},
  journal = {International Conference on Automated Planning and Scheduling},
  abstract = {We present an approach to planning rover traverses in a domain that includes temporal-spatial constraints. We are using the NASA Resource Prospector mission as a reference mission in our research. The primary objective of this mission is to assess the feasibility of in-situ resource utilization (ISRU) on the lunar surface. One of the mission operations constraints is that the rover is generally required to avoid being in shadow, because it is solar-powered. This requirement depends on where the rover is located and when it is at that location. Such a temporal-spatial constraint makes traverse planning more challenging for both humans and machines. We present a mixed-initiative traverse planner which helps address this challenge. This traverse planner is part of the Exploration Ground Data Systems (xGDS), which we have enhanced with new visualization features, new analysis tools, and new automation for path planning, in order to be applicable to the Resource Prospector mission. The key concept that is the basis of the analysis tools and that supports the automated path planning is reachability in this dynamic environment due to the temporal-spatial constraints.},
  file = {/home/cameron/Zotero/storage/JIUAZ3A5/20170010241.pdf}
}

@article{Brin2012a,
  title = {The Anatomy of a Large-Scale Hypertextual Web Search Engine},
  author = {Brin, Sergey and Page, Lawrence},
  year = {2012},
  journal = {Computer Networks},
  volume = {56},
  number = {18},
  pages = {3825--3833},
  issn = {13891286},
  doi = {10.1016/j.comnet.2012.10.007},
  abstract = {In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/ To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from 3 years ago. This paper provides an in-depth description of our large-scale web search engine - the first such detailed public description we know of to date. Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections, where anyone can publish anything they want. \textcopyright{} 2012 Elsevier B.V. All rights reserved.},
  keywords = {Google,Information retrieval,Pagerank,Search engines,World Wide Web},
  file = {/home/cameron/Zotero/storage/9KKRPJXW/Brin, Page - 2012 - The anatomy of a large-scale hypertextual web search engine.pdf}
}

@article{Brownlee2018,
  title = {Deep {{Learning}} for {{Time Series Forecasting Predict}} the {{Future}} with {{MLPs}}, {{CNNs}} and {{LSTMs}} in {{Python Acknowledgements Copyright Deep Learning}} for {{Time Series Forecasting}}},
  author = {Brownlee, Jason},
  year = {2018},
  doi = {10.1007/978-1-4614-6309-2},
  isbn = {978-1-4614-6308-5},
  file = {/home/cameron/Zotero/storage/MCQ94MBW/deep_learning_time_series_forecasting_sample.pdf}
}

@techreport{brownlee2018,
  title = {Deep {{Learning}} for {{Time Series Forecasting Predict}} the {{Future}} with {{MLPs}}, {{CNNs}} and {{LSTMs}} in {{Python}}},
  author = {Brownlee, Jason},
  year = {2018},
  file = {/home/cameron/Zotero/storage/2DP46YFW/deep_learning_time_series_forecasting_sample.pdf}
}

@inproceedings{Bruttomesso2008,
  title = {The {{MathSAT}} 4 {{SMT Solver}}},
  booktitle = {Computer {{Aided Verification}}, 20th {{International Conference}}},
  author = {Bruttomesso, Roberto and Cimatti, Alessandro and Griggio, Alberto and Sebastiani, Roberto},
  year = {2008},
  number = {July},
  pages = {1--5},
  address = {{Princeton, NJ}},
  issn = {03029743},
  doi = {10.1007/978-3-540-70545-1},
  abstract = {We present MathSAT4, a state-of-the-art SMT solver. MathSAT4 handles several useful theories: (combinations of) equality and uninterpreted functions, difference logic, linear arithmetic, and the theory of bit-vectors. It was explicitly designed for being used in formal verification, and thus provides functionalities which extend the applicability of SMT in this setting. In particular: model generation (for counterexample reconstruction), model enumeration (for predicate abstraction), an incremental interface (for BMC), and computation of unsatisfiable cores and Craig interpolants (for abstraction refinement).},
  isbn = {3-540-70543-0},
  file = {/home/cameron/Zotero/storage/ZKIVIXIS/Bruttomesso et al. - 2008 - The MathSAT 4 SMT Solver.pdf}
}

@article{Bryce2015,
  title = {{{SMT-Based}} Nonlinear {{PDDL}}+ Planning},
  author = {Bryce, Daniel and Gao, Sicun and Musliner, Davi and Goldman, Robert},
  year = {2015},
  journal = {AAAI Workshop - Technical Report},
  volume = {WS-15-12},
  pages = {17--23},
  abstract = {PDDL+ planning involves reasoning about mixed discrete- continuous change over time. Nearly all PDDL+ planners assume that continuous change is linear. We present a new technique that accommodates nonlinear change by encoding problems as nonlinear hybrid systems. Using this encoding, we apply a Satisfiability Modulo Theories (SMT) solver to find plans. We show that it is important to use novel planning- specific hcuristics for variable and value selection for SMT solving, which is inspired by recent advances in planning as SAT. We show the promising performance of the resulting solver on challenging nonlinear problems.},
  isbn = {9781577357230},
  keywords = {Planning and Scheduling Track},
  file = {/home/cameron/Zotero/storage/N26X77NN/Bryce et al. - 2015 - SMT-Based nonlinear PDDL planning.pdf}
}

@article{Campbell2012,
  title = {Advanced {{EMU Portable Life Support System}} ({{PLSS}}) and {{Shuttle}}/{{ISS EMU Schematics}}, a {{Comparison}}},
  author = {Campbell, Colin},
  year = {2012},
  journal = {42nd International Conference on Environmental Systems},
  pages = {1--18},
  doi = {10.2514/6.2012-3411},
  abstract = {In order to be able to adapt to differing vehicle interfaces such as suitport and airlock, adjust to varying vehicle pressure schedules, tolerate lower quality working fluids, and adapt to differing suit architectures as dictated by a range of mission architectures, the next generation space suit requires more adaptability and robustness over that of the current Shuttle/ISS Extra-vehicular Mobility Unit (EMU). While some features have been added to facilitate interfaces to differing vehicle and suit architectures, the key performance gains have been made via incorporation of new technologies such as the variable pressure regulators, Rapid Cycle Amine swing-bed, and Suit Water Membrane Evaporator. This paper performs a comparison between the Shuttle/ISS EMU PLSS schematic and the Advanced EMU PLSS schematic complete with a discussion for each difference.},
  isbn = {978-1-60086-934-1},
  file = {/home/cameron/Zotero/storage/GAL73EEC/plss-schematics.pdf}
}

@inproceedings{Campbell2015,
  title = {Shuttle/{{ISS EMU Failure History}} and the {{Impact}} on {{Advanced EMU PLSS Design}}},
  booktitle = {45th {{International Conference}} on {{Environmental Systems}}},
  author = {Campbell, Colin},
  year = {2015},
  pages = {42},
  address = {{Bellevue, Washington}},
  abstract = {As the Shuttle/ISS EMU Program exceeds 35 years in duration and is still supporting the needs of the International Space Station (ISS), a critical benefit of such a long running program with thorough documentation of system and component failures is the ability to study and learn from those failures when considering the design of the next generation space suit. Study of the subject failure history leads to changes in the Advanced EMU Portable Life Support System (PLSS) schematic, selected component technologies, as well as the planned manner of ground testing. This paper reviews the Shuttle/ISS EMU failure history and discusses the implications to the AEMU PLSS.},
  file = {/home/cameron/Zotero/storage/W2MCK38X/plss-emu-fiar.pdf}
}

@techreport{campbell2015,
  title = {Shuttle/{{ISS EMU Failure History}} and the {{Impact}} on {{Advanced EMU Portable Life Support System}} ({{PLSS}}) {{Design}}},
  author = {Campbell, Colin},
  year = {2015},
  volume = {1},
  file = {/home/cameron/Zotero/storage/RPCDVU79/plss-emu-fiar.pdf}
}

@article{Campbell2017,
  title = {Schematics and {{Behavioral Description}} for the {{Advanced EMU}} ({{AEMU}}) {{Portable Life Support Subsystem}} ({{PLSS}}) {{Engineering Directorate Crew}} and {{Thermal Systems Division}}},
  author = {Campbell, Colin},
  year = {2017},
  file = {/home/cameron/Zotero/storage/KVYD4ICU/20170009505.pdf}
}

@article{canellas2018,
  title = {Viewpoint {{Unsafe At Any Level}}},
  author = {Canellas, Marc C. and Haga, Rachel},
  year = {2018},
  pages = {0--3},
  doi = {10.1145/3342102},
  file = {/home/cameron/Zotero/storage/7HH3N6RB/CanellasHaga_2020_UnsafeAtAnyLevel.pdf}
}

@article{Cao1997,
  title = {Cooperative {{Mobile Robotics}}: {{Antecedents}} and {{Directions}}},
  author = {Cao, Y. Uny and Fukunaga, Alex S. and Kahng, Andrew B.},
  year = {1997},
  journal = {Autonomous Robots},
  volume = {4},
  number = {1},
  pages = {7--27},
  issn = {09295593},
  doi = {10.1023/A:1008855018923},
  abstract = {There has been increased research interest in systems composed of multiple autonomous mobile \textbackslash nrobots exhibiting cooperative behavior. Groups of mobile robots are constructed, with an aim to studying \textbackslash nsuch issues as group architecture, resource conflict, origin of cooperation, learning, and geometric prob- \textbackslash nlems. As yet, few applications of cooperative robotics have been reported, and supporting theory is still in \textbackslash nits formative stages. In this paper, we give a critical survey of existing works and discuss open problems \textbackslash nin this field, emphasizing the various theoretical issues that arise in the study of cooperative robotics. We \textbackslash ndescribe the intellectual heritages that have guided early research, as well as possible additions to the set \textbackslash nof existing motivations.},
  keywords = {Artificial intelligence,Cooperative robotics,Distributed robotics,Mobile robots,Multiagent systems,Swarm intelligence},
  file = {/home/cameron/Zotero/storage/FVM36EQM/c48.pdf}
}

@article{Casanova2016,
  title = {Solving Dynamic Controllability Problem of Multi-Agent Plans with Uncertainty Using Mixed Integer Linear Programming},
  author = {Casanova, Guillaume and Pralet, C{\'e}dric and Lesire, Charles and Vidal, Thierry},
  year = {2016},
  journal = {Frontiers in Artificial Intelligence and Applications},
  volume = {285},
  pages = {930--938},
  issn = {09226389},
  doi = {10.3233/978-1-61499-672-9-930},
  abstract = {Executing multi-agent missions requires managing the uncertainty about uncontrollable events. When communications are intermittent, it additionally requires for each agent to act only based on its local view of the problem, that is independently of events which are controlled or observed by the other agents. In this paper, we propose a new framework for dealing with such contexts, with a focus on mission plans involving temporal constraints. This framework, called Multi-agent Simple Temporal Network with Uncertainty (MaSTNU), is a combination between Multi-agent Simple Temporal Network (MaSTN) and Simple Temporal Network with Uncertainty (STNU). We define the dynamic controllability property for MaSTNU, and a method for computing offline valid execution strategies which are then dispatched between agents. This method is based on a mixed-integer linear programming formulation and can also be used to optimize criteria such as the temporal flexibility of multi-agent plans.},
  isbn = {9781614996712},
  file = {/home/cameron/Zotero/storage/5ABXGD6H/ea0885ea3a031e396e97a720e19196934423.pdf}
}

@article{Cashmore2016,
  title = {A Compilation of the Full {{PDDL}}+ Language into {{SMT}}},
  author = {Cashmore, Michael and Fox, Maria and Long, Derek and Magazzeni, Daniele},
  year = {2016},
  journal = {AAAI Workshop - Technical Report},
  volume = {WS-16-01 -},
  pages = {583--591},
  abstract = {Planning in hybrid systems is important for dealing with realworld applications. PDDL+ supports this representation of domains with mixed discrete and continuous dynamics, and supports events and processes modeling exogenous change. Motivated by numerous SAT-based planning approaches, we propose an approach to PDDL+ planning through SMT, describing an SMT encoding that captures all the features of the PDDL+ problem as published by Fox and Long (2006). The encoding can be applied on domains with nonlinear continuous change. We apply this encoding in a simple planning algorithm, demonstrating excellent results on a set of benchmark problems.},
  isbn = {9781577357599},
  keywords = {Planning for Hybrid Systems: Technical Report WS-1},
  file = {/home/cameron/Zotero/storage/PKP52DS3/Cashmore et al. - 2016 - A compilation of the full PDDL language into SMT.pdf}
}

@article{Cassez2005,
  title = {Efficient On-the-Fly Algorithms for the Analysis of Timed Games},
  author = {Cassez, Franck and David, Alexandre and Fleury, Emmanuel and Larsen, Kim G. and Lime, Didier},
  year = {2005},
  journal = {Lecture Notes in Computer Science},
  volume = {3653},
  number = {august},
  pages = {66--80},
  issn = {03029743},
  doi = {10.1007/11539452_9},
  abstract = {In this paper, we propose the first efficient on-the-fly algorithm for solving games based on timed game automata with respect to reachability and safety properties The algorithm we propose is a symbolic extension of the on-the-fly algorithm suggested by Liu \& Smolka [15] for linear-time model-checking of finite-state systems. Being on-the-fly, the symbolic algorithm may terminate long before having explored the entire state-space. Also the individual steps of the algorithm are carried out efficiently by the use of so-called zones as the underlying data structure. Various optimizations of the basic symbolic algorithm are proposed as well as methods for obtaining time-optimal winning strategies (for reachability games). Extensive evaluation of an experimental implementation of the algorithm yields very encouraging performance results. \textcopyright{} Springer-Verlag Berlin Heidelberg 2005.},
  file = {/home/cameron/Zotero/storage/5HXYNI5D/Cassez et al. - 2005 - Efficient on-the-fly algorithms for the analysis of timed games.pdf}
}

@article{Castaldo2015,
  title = {Acute Mental Stress Assessment via Short Term {{HRV}} Analysis in Healthy Adults: {{A}} Systematic Review},
  author = {Castaldo, Rossana and Melillo, Paolo and Pecchia, Leandro},
  year = {2015},
  journal = {IFMBE Proceedings},
  volume = {45},
  pages = {1--4},
  publisher = {{Elsevier Ltd}},
  issn = {16800737},
  doi = {10.1007/978-3-319-11128-5_1},
  abstract = {Mental stress reduces performances, on the work place and in daily life, and is one of the first causes of cognitive dysfunctions, cardiovascular disorders and depression. This study systematically reviewed existing literature investigating, in healthy subjects, the associations between acute mental stress and short term Heart Rate Variability (HRV) measures in time, frequency and non-linear domain. The goal of this study was to provide reliable information about the trends and the pivot values of HRV measures during mental stress. A systematic review and meta-analysis of the evidence was conducted, performing an exhaustive research of electronic repositories and linear researching references of papers responding to the inclusion criteria. After removing duplicates and not pertinent papers, journal papers describing well-designed studies that analyzed rigorously HRV were included if analyzed the same population of healthy subjects at rest and during mental stress. 12 papers were shortlisted, enrolling overall 758 volunteers and investigating 22 different HRV measures, 9 of which reported by at least 2 studies and therefore meta-analyzed in this review. Four measures in time and non-linear domains, associated with a normal degree of HRV variations resulted significantly depressed during stress. The power of HRV fluctuations at high frequencies was significantly depressed during stress, while the ratio between low and high frequency resulted significantly increased, suggesting a sympathetic activation and a parasympathetic withdrawal during acute mental stress. Finally, among the 15 non-linear measures extracted, only 2 were reported by at least 2 studies, therefore pooled, and only one resulted significantly depressed, suggesting a reduced chaotic behaviour during mental stress. HRV resulted significantly depressed during mental stress, showing a reduced variability and less chaotic behaviour. The pooled frequency domain measures demonstrated a significant autonomic balance shift during acute mental stress towards the sympathetic activation and the parasympathetic withdrawal. Pivot values for the pooled mean differences of HRV measures are provided. Further studies investigating HRV non-linear measures during mental stress are still required. However, the method proposed to transform and then meta-analyze the HRV measures can be applied to other fields where HRV proved to be clinically significant.},
  isbn = {9783319111278},
  pmid = {9261072},
  keywords = {Heart Rate Variability,Mental stress,Short term HRV measures,Systematic review},
  file = {/home/cameron/Zotero/storage/D2534UNB/Castaldo-et-al-2015-HRV-meta-analysis.pdf}
}

@article{CFDP,
  title = {{{CCSDS File Delivery Protocol}} ({{CFDP}})},
  author = {{Consultative Committee for Space Data Systems}},
  year = {2020},
  number = {July},
  pages = {151},
  publisher = {{Systems, Consultative Committee for Space Data}},
  address = {{Washington DC. USA}},
  abstract = {This document has been approved for publication by the Management Council of the Consultative Committee for Space Data Systems (CCSDS) and represents the consensus technical agreement of the participating CCSDS Member Agencies. The procedure for review and authorization of CCSDS documents is detailed in Organization and Processes for the Consultative Committee for Space Data Systems (CCSDS A02.1-Y-4), and the record of Agency participation in the authorization of this document can be obtained from the CCSDS Secretariat at the e-mail address below.},
  file = {/home/cameron/Zotero/storage/2B3BPNS5/m-api-78c3d382-026d-76bd-558a-d95bb8cdd709.pdf}
}

@article{Chakraborti2018,
  title = {Human-{{Aware Planning Revisited}} : {{A Tale}} of {{Three Models}}},
  author = {Chakraborti, Tathagata and Sreedharan, Sarath and Kambhampati, Subbarao},
  year = {2018},
  journal = {ICAPS Workshop on Explainable AI Planning (XAIP)},
  abstract = {Human-aware planning requires an agent to be aware of the mental model of the humans, in addition to their physical or capability model. This not only allows an agent to envisage the desired roles of the human in a joint plan but also anticipate how its plan will be perceived by the latter. The human mental model becomes especially useful in the context of an explainable planning (XAIP) agent since an explanatory process cannot be a soliloquy, i.e. it must incorporate the hu-man's beliefs and expectations of the planner. In this paper, we survey our recent efforts in this direction. Cognitive AI teaming (Chakraborti et al. 2017a) requires a planner to perform argumentation over a set of models during the plan generation process. This is illustrated in Figure 1. Here, M R is the model of the agent embodying the planner (e.g. a robot), and M H is the model of the human in the loop. Further, M R h is the model the human thinks the robot has, and M H r is the model that the robot thinks the human has. Finally, \texttildelow{} M R h is the robot's approximation of M R h ; for the rest of the paper we will be using M R h to refer to both since, for all intents and purposes, this is all the robot has access to. Note that the human mental model M R h is in addition to the (robot's belief of the) human model M H r traditionally encountered in human-robot teaming (HRT) settings and is, in essence, the fundamental thesis of the recent works on plan explanations (Chakraborti et al. 2017b) and explicable planning (Zhang et al. 2017). The need for expli-cable planning or plan explanations occurs when the models-M R and M R h-diverge so that the optimal plans in the respective models may not be the same and hence optimal behavior of the robot in its own model is inexplicable to the human. This is also true for discrepancies between M H and M H r when the robot might reveal unrealistic expectations of the human in a joint plan. An explainable planning (XAIP) agent (Fox et al. 2017; Langley et al. 2017; Weld and Bansal 2018) should be able to able to deal with such model differences and participate in explanatory dialog with the human such that both of them can be on the same page during a collaborative activity. This is referred to as model reconciliation (Chakraborti et al. 2017b) and forms the core of the explanatory process of an XAIP agent. In this paper, we look at the scope of problems engendered by this multi-model setting and describe Figure 1: Argumentation over multiple models during the deliberative process of a human-aware planner (e.g. robot). the recent work in this direction. Specifically-We outline the scope of behaviors engendered by human-aware planning, including joint planning as studied in teaming using the human model, as well as explicable planning with the human mental model;-We situate the plan explanation problem in the context of perceived inexplicability of the robot's plans or behaviors due to differences in these models;-We discuss how the plan explanation process can be seen as one of model reconciliation where M R h (and/or M H r) is brought closer to M R (M H);-We discuss how explicability and explanation costs can be traded off during plan generation;-We discuss how this process can be adapted to handle uncertainty or multiple humans in the loop;-We discuss results of a user study that testify to the usefulness of the model reconciliation process;-We point to ongoing work in the space of abstractions and deception using the human mental model.},
  file = {/home/cameron/Zotero/storage/G2QK4AP7/Chakraborti, Sreedharan, Kambhampati - 2018 - Human-Aware Planning Revisited A Tale of Three Models.pdf}
}

@article{Chan2011,
  title = {Towards Open and Equitable Access to Research and Knowledge for Development},
  author = {Chan, Leslie and Kirsop, Barbara and Arunachalam, Subbiah},
  year = {2011},
  journal = {PLoS Medicine},
  volume = {8},
  number = {3},
  pages = {1--4},
  issn = {15491277},
  doi = {10.1371/journal.pmed.1001016},
  pmid = {21483470},
  file = {/home/cameron/Zotero/storage/F9NEU3WA/R02- Chan et. al. Open and Equitable Access.pdf}
}

@article{Chen1993,
  title = {Finding {{Antipodal Point Grasps}} on {{Irregularly Shaped Objects}}},
  author = {Chen, I. Ming and Burdick, Joel W.},
  year = {1993},
  journal = {IEEE Transactions on Robotics and Automation},
  volume = {9},
  number = {4},
  pages = {507--512},
  issn = {1042296X},
  doi = {10.1109/70.246063},
  abstract = {Two-finger antipodal point grasping of arbitrarily shaped smooth 2-D and 3-D objects is considered. An object function is introduced that maps a finger contact space to the object surface. Conditions are developed to identify the feasible grasping region, F, in the finger contact space. A ``grasping energy function,'' E is introduced that which is proportional to the distance between two grasping points. The antipodal points correspond to critical points of E in T. Optimization and/or continuation techniques are used to find these critical points. In particular, global optimization techniques are applied to find the ``maximal'' or ``minimal'' grasp. Further, modeling techniques are introduced for representing 2-D and 3-D objects using B-spline curves and spherical product surfaces. \textcopyright{} 1993 IEEE},
  file = {/home/cameron/Zotero/storage/Z5DASQKB/Chen, Burdick - 1993 - Finding Antipodal Point Grasps on Irregularly Shaped Objects.pdf}
}

@article{Chen2020,
  title = {Scalable and Safe Multi-Agent Motion Planning with Nonlinear Dynamics and Bounded Disturbances},
  author = {Chen, Jingkai and Li, Jiaoyang and Fan, Chuchu and Williams, Brian},
  year = {2020},
  journal = {arXiv},
  eprint = {2012.09052},
  eprinttype = {arxiv},
  issn = {23318422},
  abstract = {We present a scalable and effective multi-agent safe motion planner that enables a group of agents to move to their desired locations while avoiding collisions with obstacles and other agents, with the presence of rich obstacles, high-dimensional, nonlinear, nonholonomic dynamics, actuation limits, and disturbances. We address this problem by finding a piecewise linear path for each agent such that the actual trajectories following these paths are guaranteed to satisfy the reach-and-avoid requirement. We show that the spatial tracking error of the actual trajectories of the controlled agents can be precomputed for any qualified path that considers the minimum duration of each path segment due to actuation limits. Using these bounds, we find a collision-free path for each agent by solving Mixed Integer-Linear Programs and coordinate agents by using the priority-based search. We demonstrate our method by benchmarking in 2D and 3D scenarios with ground vehicles and quadrotors, respectively, and show improvements over the solving time and the solution quality compared to two state-of-the-art multi-agent motion planners.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/YMGFY2CU/Chen et al. - 2020 - Scalable and safe multi-agent motion planning with nonlinear dynamics and bounded disturbances.pdf}
}

@article{chen2021,
  title = {Cooperative {{Task}} and {{Motion Planning}} for {{Multi-Arm Assembly Systems}}},
  author = {Chen, Jingkai and Li, Jiaoyang and Huang, Yijiang and Garrett, Caelan and Sun, Dawei and Fan, Chuchu and Hofmann, Andreas and Mueller, Caitlin and Koenig, Sven and Williams, Brian C},
  year = {2021},
  number = {030118},
  file = {/home/cameron/Zotero/storage/6RITN6BY/21-2620_01_MS.pdf}
}

@book{Chen2021,
  title = {Optimal {{Mixed Discrete-Continuous Planning}} for {{Linear Hybrid Systems}}},
  author = {Chen, Jingkai and Williams, Brian and Fan, Chuchu},
  year = {2021},
  journal = {Proceedings of ACM Conference (Conference'17)},
  volume = {1},
  eprint = {2102.08261},
  eprinttype = {arxiv},
  publisher = {{Association for Computing Machinery}},
  abstract = {Planning in hybrid systems with both discrete and continuous control variables is important for dealing with real-world applications such as extra-planetary exploration and multi-vehicle transportation systems. Meanwhile, generating high-quality solutions given certain hybrid planning specifications is crucial to building high-performance hybrid systems. However, since hybrid planning is challenging in general, most methods use greedy search that is guided by various heuristics, which is neither complete nor optimal and often falls into blind search towards an infinite-action plan. In this paper, we present a hybrid automaton planning formalism and propose an optimal approach that encodes this planning problem as a Mixed Integer Linear Program (MILP) by fixing the action number of automaton runs. We also show an extension of our approach for reasoning over temporally concurrent goals. By leveraging an efficient MILP optimizer, our method is able to generate provably optimal solutions for complex mixed discrete-continuous planning problems within a reasonable time. We use several case studies to demonstrate the extraordinary performance of our hybrid planning method and show that it outperforms a state-of-the-art hybrid planner, Scotty, in both efficiency and solution qualities.},
  archiveprefix = {arXiv},
  keywords = {2021,acm reference format,and chuchu fan,brian c,hybrid planning,jingkai chen,linear hybrid systems,Linear Hybrid Systems; Hybrid Planning; Optimizati,optimal mixed,optimization,williams},
  file = {/home/cameron/Zotero/storage/FV4QA9E9/Chen, Williams, Fan - 2021 - Optimal Mixed Discrete-Continuous Planning for Linear Hybrid Systems.pdf}
}

@inproceedings{Chen2021a,
  title = {Optimal Mixed Discrete-Continuous Planning for Linear Hybrid Systems},
  booktitle = {{{HSCC}} 2021 - {{Proceedings}} of the 24th {{International Conference}} on {{Hybrid Systems}}: {{Computation}} and {{Control}} (Part of {{CPS-IoT Week}})},
  author = {Chen, Jingkai and Williams, Brian C. and Fan, Chuchu},
  year = {2021},
  volume = {1},
  eprint = {2102.08261},
  eprinttype = {arxiv},
  publisher = {{Association for Computing Machinery}},
  doi = {10.1145/3447928.3456654},
  abstract = {Planning in hybrid systems with both discrete and continuous control variables is important for dealing with real-world applications such as extra-planetary exploration and multi-vehicle transportation systems. Meanwhile, generating high-quality solutions given certain hybrid planning specifications is crucial to building high-performance hybrid systems. However, since hybrid planning is challenging in general, most methods use greedy search that is guided by various heuristics, which is neither complete nor optimal and often falls into blind search towards an infinite-action plan. In this paper, we present a hybrid automaton planning formalism and propose an optimal approach that encodes this planning problem as a Mixed Integer Linear Program (MILP) by fixing the action number of automaton runs. We also show an extension of our approach for reasoning over temporally concurrent goals. By leveraging an efficient MILP optimizer, our method is able to generate provably optimal solutions for complex mixed discrete-continuous planning problems within a reasonable time. We use several case studies to demonstrate the extraordinary performance of our hybrid planning method and show that it outperforms a state-of-the-art hybrid planner, Scotty, in both efficiency and solution qualities.},
  archiveprefix = {arXiv},
  isbn = {978-1-4503-8339-4},
  keywords = {hybrid planning,linear hybrid systems,optimization},
  file = {/home/cameron/Zotero/storage/7QGJWAYY/Chen, Williams, Fan - 2021 - Optimal mixed discrete-continuous planning for linear hybrid systems.pdf}
}

@book{Chi2017,
  title = {Convex {{Optimization}}},
  author = {Boyd, Stephen P.},
  year = {2017},
  journal = {Convex Optimization for Signal Processing and Communications},
  doi = {10.1201/9781315366920-5},
  abstract = {In many engineered systems, optimization is used for decision making at time-scales rang-ing from real-time operation to long-term plan-ning. This process often involves solving sim-ilar optimization problems over and over again with slightly modified input parameters, often under stringent time requirements. We consider the problem of using the information available through this solution process to directly learn the optimal solution as a function of the input parameters, thus reducing the need of solving computationally expensive large-scale paramet-ric programs in real time. Our proposed method is based on learning relevant sets of active con-straints, from which the optimal solution can be obtained efficiently. Using active sets as features preserves information about the physics of the system, enables more interpretable learning poli-cies, and inherently accounts for relevant safety constraints. Further, the number of relevant ac-tive sets is finite, which make them simpler ob-jects to learn. To learn the relevant active sets, we propose a streaming algorithm backed up by the-oretical results. Through extensive experiments on benchmarks of the Optimal Power Flow prob-lem, we observe that often only a few active sets are relevant in practice, suggesting that this is the appropriate level of abstraction for a learning al-gorithm to target.},
  isbn = {978-0-521-83378-3},
  file = {/home/cameron/Zotero/storage/8UZJ4L7P/Boyd - 2017 - Convex Optimization.pdf}
}

@article{Chien2014,
  title = {Onboard {{Autonomy}} on the {{Intelligent Payload EXperiment}} ({{IPEX}}) {{Cubesat Mission}} : {{A}} Pathfinder for the Proposed {{HyspIRI Mission Intelligent Payload Module}}},
  author = {Chien, Steve and Doubleday, Joshua and Thompson, David R and Wagstaff, Kiri L and Bellardo, John and Francis, Craig and Baumgarten, Eric and Williams, Austin and Yee, Edmund and Fluitt, Daniel and Stanton, Eric and {Piug-suari}, Jordi},
  year = {2014},
  journal = {Proc. Intl Symposium on Artificial Intelligence, Robotics, and Automation for Space 2014},
  abstract = {The Intelligent Payload Experiment (IPEX) is a cubesat that successfully launched in December 2013 and is currently flight validating autonomous operations for onboard instrument processing and product generation for the Intelligent Payload Module (IPM) of the Hyperspectral Infra-red Imager (HyspIRI) mission concept. We first describe the ground and flight operations concept for HyspIRI IPM operations. We then describe the ground and flight operations concept for the IPEX mission and IPEX operations are validating the proposed HyspIRI IPM operations. We then detail the current status of the IPEX mission and results from the mission thus far.},
  file = {/home/cameron/Zotero/storage/5A7PKTCF/7475910349026a3e58a6df09938f39a4eda4.pdf}
}

@article{Chien2017,
  title = {Robotic Space Exploration Agents},
  author = {Chien, Steve and Wagstaff, Kiri L.},
  year = {2017},
  journal = {Science Robotics},
  volume = {2},
  number = {7},
  pages = {9--11},
  issn = {24709476},
  doi = {10.1126/scirobotics.aan4831},
  abstract = {By making their own exploration decisions, robotic spacecraft can conduct traditional science investigations more efficiently and even achieve otherwise impossible observations, such as responding to a short-lived plume at a comet millions of miles from Earth.},
  file = {/home/cameron/Zotero/storage/TT9LXSLK/eaan4831.full.pdf}
}

@article{Chiou2021a,
  title = {Trusting Automation: {{Designing}} for Responsivity and Resilience},
  author = {Chiou, Erin K. and Lee, John D.},
  year = {2021},
  journal = {Human Factors},
  doi = {10.1177/00187208211009995},
  keywords = {adaptive automation,automation,autonomy,human,intelligent agents,interaction,trust},
  file = {/home/cameron/Zotero/storage/8EPKH2EN/00187208211009995.pdf}
}

@article{Cho2014,
  title = {A Multi-Sensor Fusion System for Moving Object Detection and Tracking in Urban Driving Environments},
  author = {Cho, Hyunggi and Seo, Young Woo and Kumar, B. V.K.Vijaya and Rajkumar, Ragunathan Raj},
  year = {2014},
  journal = {Proceedings - IEEE International Conference on Robotics and Automation},
  pages = {1836--1843},
  issn = {10504729},
  doi = {10.1109/ICRA.2014.6907100},
  abstract = {A self-driving car, to be deployed in real-world driving environments, must be capable of reliably detecting and effectively tracking of nearby moving objects. This paper presents our new, moving object detection and tracking system that extends and improves our earlier system used for the 2007 DARPA Urban Challenge. We revised our earlier motion and observation models for active sensors (i.e., radars and LIDARs) and introduced a vision sensor. In the new system, the vision module detects pedestrians, bicyclists, and vehicles to generate corresponding vision targets. Our system utilizes this visual recognition information to improve a tracking model selection, data association, and movement classification of our earlier system. Through the test using the data log of actual driving, we demonstrate the improvement and performance gain of our new tracking system.},
  isbn = {9781479936847},
  file = {/home/cameron/Zotero/storage/AM9SCCJD/icra-14-sensor-fusion.pdf}
}

@article{Chouhan2015,
  title = {{{DMAPP}}: {{A}} Distributed Multi-Agent Path Planning Algorithm},
  author = {Chouhan, Satyendra Singh and Niyogi, Rajdeep},
  year = {2015},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {9457},
  pages = {123--135},
  issn = {16113349},
  doi = {10.1007/978-3-319-26350-2_11},
  abstract = {Multi-agent path planning is a very challenging problem that has several applications. It has received a lot of attention in the last decade. Multi-agent optimal path planning is computationally intractable. Some algorithms have been suggested that may not return optimal plans but are useful in practice. These works mostly use centralized algorithms to compute plans. However in a multi-agent setting it would be more appropriate for the agents, with limited information, to compute the plans. In this paper, we suggest a distributed multi-agent path planning algorithm DMAPP, where all the phases are distributed. We have implemented DMAPP and have compared its performance with some existing algorithms. The results show the effectiveness of our approach.},
  isbn = {9783319263496},
  keywords = {Distributed decision making,Multi-agent path planning,Plan restructuring},
  file = {/home/cameron/Zotero/storage/J23A6W4J/dmapp.pdf}
}

@article{Chouhan2017,
  title = {{{DiMPP}}: A Complete Distributed Algorithm for Multi-Agent Path Planning},
  author = {Chouhan, Satyendra Singh and Niyogi, Rajdeep},
  year = {2017},
  journal = {Journal of Experimental and Theoretical Artificial Intelligence},
  volume = {29},
  number = {6},
  pages = {1129--1148},
  issn = {13623079},
  doi = {10.1080/0952813X.2017.1310142},
  abstract = {Multi-agent path planning (MAPP) is a challenging task that aims to find conflict free paths for all the agents in a given domain. Priority-based decoupled approach is one of the several approaches to solve a MAPP problem. It works as follows: first, find paths of individual agents and then restructure these paths based on some priority of the agents. Most of the existing decoupled approaches use centralised algorithms. However, multi-agent systems are inherently distributed, where agents have limited information and each agent may not know the total number of agents in the system. Some of these aspects are incorporated in DMAPP. DMAPP is an existing fully distributed algorithm that works in three phases: (i) individual path planning, (ii) priority decision-making and (iii) plan restructuring. However, DMAPP is incomplete, i.e. DMAPP may fail to find a solution, even if it exists. In this paper, we present a new distributed algorithm (DiMPP) which is complete. The computer simulations performed on some well-known benchmark domains reveal that DiMPP outperforms DMAPP in the number of problem instances solved. For larger problem instances, the time taken by DiMPP is orders of magnitude less than that of some existing centralised algorithms.},
  keywords = {decoupled approach,Multi-agent path planning,plan restructuring,prioritised path planning},
  file = {/home/cameron/Zotero/storage/9WRXWVSG/dimpp.pdf}
}

@article{Chung2001,
  title = {Improving {{Model-based Mode Estimation}} through {{Offline Compilation}}},
  author = {Chung, Seung and Eepoel, John Van and Williams, Brian C},
  year = {2001},
  journal = {Proceedings of the Sixth International Symposium on Artificial Intelligence, Robotics and Automation in Space},
  pages = {8},
  abstract = {Many recent and future space missions point to the need for increased autonomy in spacecraft with an emphasis on more capable fault diagnostic systems. The most widely used fault diagnostic systems are rule-based. Rule-based systems have quick response to events and clearly present to engineers the predefined reactions to events. These systems, however, require engineers to manually generate all necessary rules and these do not convey the assumed model the engineers used to generate the rules. Contrarily, model-based systems eliminate the need to manually generate the rules. Most model-based system such as GDE [3], Sherlock [4], and Livingstone [6], however, may not provide quick response and do not specify the rules for engineers to review and verify. Mini-ME addresses the issues of both rule-based and model-based approaches and provides an alternative solution to fault diagnosis. Mini-ME provides quick response by shifting computationally expensive tasks of model-based diagnosis offline. Additionally, it offers the capability to inspect and verify the rules.},
  keywords = {diagnosis,mode estimation,model-based,verification},
  file = {/home/cameron/Zotero/storage/M69VK29X/Chung, Eepoel, Williams - 2001 - Improving Model-based Mode Estimation through Offline Compilation.pdf}
}

@article{Chung2019,
  title = {Julia's {{Efficient Algorithm}} for {{Subtyping Unions}} and {{Covariant Tuples}}},
  author = {Chung, Benjamin and Vitek, Jan and Nardelli, Francesco Zappa},
  abstract = {The Julia programming language supports multiple dispatch and provides a rich type annotation language to specify method applicability. When multiple methods are applicable for a given call, Julia relies on subtyping between method signatures to pick the correct method to invoke. Julia's subtyping algorithm is surprisingly complex, and determining whether it is correct remains an open question. In this paper, we focus on one piece of this problem: the interaction between union types and covariant tuples. Previous work normalized unions inside tuples to disjunctive normal form. However, this strategy has two drawbacks: complex type signatures induce space explosion, and interference between normalization and other features of Julia's type system. In this paper, we describe the algorithm that Julia uses to compute subtyping between tuples and unions \textendash{} an algorithm that is immune to space explosion and plays well with other features of the language. We prove this algorithm correct and complete against a semantic-subtyping denotational model in Coq.},
  keywords = {Subtyping,Type systems,Union types},
  file = {/home/cameron/Zotero/storage/8XIETKTR/m-api-6a4ec908-d004-f3b5-31a9-f93cc9a03ba9.pdf}
}

@article{Cimatti2012,
  title = {Solving Temporal Problems with Uncertainty Using {{SMT}}: {{Strong Controllability}}},
  author = {Cimatti, Alessandro and Micheli, Andrea and Roveri, Marco},
  year = {2012},
  journal = {Constraints},
  volume = {20},
  number = {1},
  pages = {1--29},
  issn = {15729354},
  doi = {10.1007/s10601-014-9167-5},
  abstract = {Temporal Problems (TPs) represent constraints over the timing of activities, as arising in many applications such as scheduling and temporal planning. A TP with uncertainty (TPU) is characterized by activities with uncontrollable duration. Different classes of TPU are possible, depending on the Boolean structure of the constraints: we have simple (STPU), constraint satisfaction (TCSPU), and disjunctive (DTPU) temporal problems with uncertainty. In this paper we tackle the problem of strong controllability, i.e. finding an assignment to all the controllable time points, such that the constraints are fulfilled under any possible assignment of uncontrollable time points. Our approach casts the problem in the framework of Satisfiability Modulo Theory (SMT), where the uncertainty of durations can be modeled by means of universal quantifiers. The use of quantifier elimination techniques leads to quantifier-free encodings, which are in turn solved with efficient SMT solvers. We obtain the first practical and comprehensive solution for strong controllability. We provide a family of efficient encodings, that are able to exploit the specific structure of the problem. The approach has been implemented, and experimentally evaluated over a large set of benchmarks. The results clearly demonstrate that the proposed approach is feasible, and outperforms the best state-of-the-art competitors, when available.},
  keywords = {Satisfiability modulo theory,Strong controllability,Temporal problems,Temporal reasoning under uncertainty},
  file = {/home/cameron/Zotero/storage/KUEB9NRJ/Cimatti, Micheli, Roveri - 2012 - Solving temporal problems with uncertainty using SMT Strong Controllability.pdf}
}

@article{Cimatti2012a,
  title = {Solving Temporal Problems Using {{SMT}}: {{Weak}} Controllability},
  author = {Cimatti, Alessandro and Micheli, A. and Roveri, M.},
  year = {2012},
  journal = {Proceedings of the National Conference on Artificial Intelligence},
  volume = {1},
  pages = {448--454},
  abstract = {Temporal problems with uncertainty are a well established formalism to model time constraints of a system interacting with an uncertain environment. Several works have addressed the definition and the solving of controllability problems, and three degrees of controllability have been proposed: weak, strong, and dynamic. In this work we focus on weak controllability: we address both the decision and the strategy extraction problems. Extracting a strategy means finding a function from assignments to uncontrollable time points to assignments to controllable time points that fulfills all the temporal constraints. We address the two problems in the satisfiability modulo theory framework. We provide a clean and complete formalization of the problems, and we propose novel techniques to extract strategies. We also provide experimental evidence of the scalability and efficiency of the proposed techniques. Copyright \textcopyright{} 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.},
  isbn = {9781577355687},
  keywords = {and Search (Main Trac,Constraints,Satisfiability},
  file = {/home/cameron/Zotero/storage/VC452U3Z/Cimatti, Micheli, Roveri - 2012 - Solving temporal problems using SMT Weak controllability.pdf}
}

@article{Cimatti2013,
  title = {The {{MathSAT5 SMT}} Solver},
  author = {Cimatti, Alessandro and Griggio, Alberto and Schaafsma, Bastiaan Joost and Sebastiani, Roberto},
  year = {2013},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {7795 LNCS},
  pages = {93--107},
  issn = {03029743},
  doi = {10.1007/978-3-642-36742-7_7},
  abstract = {MathSAT is a long-term project, which has been jointly carried on by FBK-IRST and University of Trento, with the aim of developing and maintaining a state-of-the-art SMT tool for formal verification (and other applications). MathSAT5 is the latest version of the tool. It supports most of the SMT-LIB theories and their combinations, and provides many functionalities (like e.g. unsat cores, interpolation, AllSMT). MathSAT5 improves its predecessor MathSAT4 in many ways, also providing novel features: first, a much improved incrementality support, which is vital in SMT applications; second, a full support for the theories of arrays and floating point; third, sound SAT-style Boolean formula preprocessing for SMT formulae; finally, a framework allowing users for plugging their custom tuned SAT solvers. MathSAT5 is freely available, and it is used in numerous internal projects, as well as by a number of industrial partners. \textcopyright{} 2013 Springer-Verlag.},
  isbn = {9783642367410},
  file = {/home/cameron/Zotero/storage/GWS6Q9QC/Cimatti et al. - 2013 - The MathSAT5 SMT solver.pdf}
}

@article{Cimatti2014,
  title = {Sound and Complete Algorithms for Checking the Dynamic Controllability of Temporal Networks with Uncertainty, Disjunction and Observation},
  author = {Cimatti, Alessandro and Hunsberger, Luke and Micheli, Andrea and Posenato, Roberto and Roveri, Marco},
  year = {2014},
  journal = {Proceedings of the International Workshop on Temporal Representation and Reasoning},
  number = {August},
  pages = {27--36},
  doi = {10.1109/TIME.2014.21},
  abstract = {Temporal networks are data structures for representing and reasoning about temporal constraints on activities. Many kinds of temporal networks have been defined in the literature, differing in their expressiveness. The simplest kinds of networks have polynomial algorithms for determining their consistency or controllability, but corresponding algorithms for more expressive networks (e.g., Those that include observation nodes or disjunctive constraints) have so far been unavailable. However, recent work has introduced a new approach to such algorithms based on translating temporal networks into Timed Game Automata (TGAs) and then using off-the-shelf software to synthesize execution strategies - or determine that none exist. So far, that approach has only been used on Simple Temporal Networks with Uncertainty, for which polynomial algorithms already exist. This paper extends the temporal-network-to-TGA approach to accommodate observation nodes and disjunctive constraints. Insodoing the paper presents, for the first time, sound and complete algorithms for checking the dynamic controllability of these more expressive networks. The translations also highlight the theoretical relationships between various kinds of temporal networks and the TGA model. The new algorithms have immediate applications in the workflow models being developed to automate business processes, including in the health-care domain.},
  isbn = {9781479942275},
  file = {/home/cameron/Zotero/storage/WUEGDWCG/Cimatti et al. - 2014 - Sound and complete algorithms for checking the dynamic controllability of temporal networks with uncertainty, di.pdf}
}

@article{Cimatti2014a,
  title = {Using Timed Game Automata to Synthesize Execution Strategies for Simple Temporal Networks with Uncertainty},
  author = {Cimatti, Alessandro and Hunsberger, Luke and Micheli, Andrea and Roveri, Marco},
  year = {2014},
  journal = {Proceedings of the National Conference on Artificial Intelligence},
  volume = {3},
  pages = {2242--2249},
  abstract = {A Simple Temporal Network with Uncertainty (STNU) is a structure for representing and reasoning about temporal constraints in domains where some temporal durations are not controlled by the executor. The most important property of an STNU is whether it is dynamically controllable (DC); that is, whether there exists a strategy for executing the controllable time-points that guarantees that all constraints will be satisfied no matter how the uncontrollable durations turn out. This paper provides a novel mapping from STNUs to Timed Game Automata (TGAs) that: (1) explicates the deep theoretical relationships between STNUs and TGAs; and (2) enables the memoryless strategies generated from the TGA to be transformed into equivalent STNU execution strategies that reduce the real-time computational burden for the executor. The paper formally proves that the STNU-to-TGA encoding properly captures the execution semantics of STNUs.},
  isbn = {9781577356790},
  keywords = {Planning and Scheduling},
  file = {/home/cameron/Zotero/storage/5XC7IHYL/Cimatti et al. - 2014 - Using timed game automata to synthesize execution strategies for simple temporal networks with uncertainty.pdf}
}

@article{Cimatti2015,
  title = {An {{SMT-based}} Approach to Weak Controllability for Disjunctive Temporal Problems with Uncertainty},
  author = {Cimatti, Alessandro and Micheli, Andrea and Roveri, Marco},
  year = {2015},
  journal = {Artificial Intelligence},
  volume = {224},
  pages = {1--27},
  publisher = {{Elsevier B.V.}},
  issn = {00043702},
  doi = {10.1016/j.artint.2015.03.002},
  abstract = {The framework of temporal problems with uncertainty (TPU) is useful to express temporal constraints over a set of activities subject to uncertain (and uncontrollable) duration. In this work, we focus on the most general class of TPU, namely disjunctive TPU (DTPU), and consider the case of weak controllability, that allows one to model problems arising in practical scenarios (e.g. on-line scheduling). We first tackle the decision problem, i.e. whether there exists a schedule of the activities that, depending on the uncertainty, satisfies all the constraints. We propose a logical approach, based on the reduction to a problem of Satisfiability Modulo Theories (SMT), in the theory of Linear Real Arithmetic with Quantifiers. This results in the first implemented solver for weak controllability of DTPUs. Then, we tackle the problem of synthesizing control strategies for scheduling the activities. We focus on strategies that are amenable for efficient execution. We prove that linear strategies are not always sufficient, even in the sub-case of simple TPU (STPU), while piecewise-linear strategies, that are multiple conditionally-applied linear strategies, are always sufficient. We present several algorithms for the synthesis of linear and piecewise-linear strategies, in case of STPU and of DTPU. All the algorithms are implemented on top of SMT solvers. We provide experimental evidence of the scalability of the proposed techniques, with dramatic speed-ups in strategy execution compared to on-line reasoning.},
  keywords = {Satisfiability modulo theory,Strategy synthesis,Temporal problems,Weak controllability},
  file = {/home/cameron/Zotero/storage/PHAAXZA3/Cimatti, Micheli, Roveri - 2015 - An SMT-based approach to weak controllability for disjunctive temporal problems with uncertainty.pdf}
}

@article{Cimatti2016,
  title = {Dynamic Controllability of Disjunctive Temporal Networks: {{Validation}} and Synthesis of Executable Strategies},
  author = {Cimatti, Alessandro and Micheli, Andrea and Roveri, Marco},
  year = {2016},
  journal = {30th AAAI Conference on Artificial Intelligence, AAAI 2016},
  pages = {3116--3122},
  abstract = {The Temporal Network with Uncertainty (TNU) modeling framework is used to represent temporal knowledge in presence of qualitative temporal uncertainty. Dynamic Controllability (DC) is the problem of deciding the existence of a strategy for scheduling the controllable time points of the network observing past happenings only. In this paper, we address the DC problem for a very general class of TNU, namely Disjunctive Temporal Network with Uncertainty. We make the following contributions. First, we define strategies in the form of an executable language; second, we propose the first decision procedure to check whether a given strategy is a solution for the DC problem; third we present an efficient algorithm for strategy synthesis based on techniques derived from Timed Games and Satisfiability Modulo Theory. The experimental evaluation shows that the approach is superior to the state-of-the-art.},
  isbn = {9781577357605},
  file = {/home/cameron/Zotero/storage/QPUBKZJM/Cimatti, Micheli, Roveri - 2016 - Dynamic controllability of disjunctive temporal networks Validation and synthesis of executable strate.pdf}
}

@article{Clark2010,
  title = {Revolution in Field Science: {{Apollo}} Approach to Inaccessible Surface Exploration},
  author = {Clark, P. E.},
  year = {2010},
  journal = {Earth, Moon and Planets},
  volume = {106},
  number = {2},
  pages = {133--157},
  issn = {01679295},
  doi = {10.1007/s11038-010-9354-3},
  abstract = {The extraordinary challenge mission designers, scientists, and engineers, faced in planning the first human expeditions to the surface of another solar system body led to the development of a distinctive and even revolutionary approach to field work. Not only were those involved required to deal effectively with the extreme limitation in resources available for and access to a target as remote as the lunar surface; they were required to developed a rigorous approach to science activities ranging from geological field work to deploying field instruments. Principal aspects and keys to the success of the field work are discussed here, including the highly integrated, intensive, and lengthy science planning, simulation, and astronaut training; the development of a systematic scheme for description and documentation of geological sites and samples; and a flexible yet disciplined methodology for site documentation and sample collection. The capability for constant communication with a `backroom' of geological experts who make requests and weigh in on surface operations was innovative and very useful in encouraging rapid dissemination of information to the greater community in general. An extensive archive of the Apollo era science activity related documents provides evidence of the principal aspects and keys to the success of the field work. The Apollo Surface Journal allows analysis of the astronaut's performance in terms of capability for traveling on foot, documentation and sampling of field stations, and manual operation of tools and instruments, all as a function of time. The application of these analysis as `lessons learned' for planning the next generation of human or robotic field science activities on the Moon and elsewhere are considered here as well. [ABSTRACT FROM AUTHOR]},
  isbn = {1103801093543},
  keywords = {Apollo program,Field geology,Field science,Lunar and planetary exploration,Lunar science,Robotic precursors,Science planning,Surface science},
  file = {/home/cameron/Zotero/storage/WLDRTTYY/Clark_2010_Revolution_in_Field_Science_Apollo.pdf}
}

@article{Clark2011,
  title = {A New Paradigm for Robotic Rovers},
  author = {Clark, P. E. and Curtis, S. A. and Rilee, M. L.},
  year = {2011},
  journal = {Physics Procedia},
  volume = {20},
  pages = {308--318},
  issn = {18753892},
  doi = {10.1016/j.phpro.2011.08.028},
  abstract = {We are in the process of developing rovers with extreme mobility needed to explore remote, rugged terrain. We call these systems Tetrahedral Explorer Technologies (TETs). Architecture is based on conformable tetrahedra, the simplest space-filling form, as building blocks, single or networked, where apices act as nodes from which struts reversibly deploy. The tetrahedral framework acts as a simple skeletal muscular structure. We have already prototyped a simple robotic walker from a single reconfigurable tetrahedron capable of tumbling and a more evolved 12Tetrahedral Walker, the Autonomous Landed Investigator (ALI), which has interior nodes for payload, more continuous motion, and is commandable through a user friendly interface. ALI is an EMS level mission concept which would allow autonomous in situ exploration of the lunar poles within the next decade. ALI would consist of one or more 12tetrahedral walkers capable of rapid locomotion with the many degrees of freedom and equipped for navigation in the unilluminated, inaccessible and thus largely unexplored rugged terrains where lunar resources are likely to be found: the Polar Regions. ALI walkers would act as roving reconnaissance teams for unexplored regions, analyzing samples along the way. \textcopyright{} 2011 Published by Elsevier B.V. Selection and/or peer-review under responsibility of Institute for Advanced Studies in the Space, Propulsion and Energy Science.},
  isbn = {1301286745},
  keywords = {Extreme Mobility,Robotics,Rovers,Space Exploration},
  file = {/home/cameron/Zotero/storage/3FQGRD5V/1-s2.0-S1875389211005979-main.pdf}
}

@techreport{coan,
  title = {Exploration {{EVA Systems}} \& {{Operations ISS EVA Flight Operations Operational Field Testing}} for {{Human Exploration}} (a.k.a., {{NASA Analog Projects}})},
  author = {Coan, David A. and Graff, Trevor and Young, Kelsey and Beaton, Kara and Chappell, Steve},
  file = {/home/cameron/Zotero/storage/5IE4AVLT/integrated-analogs.pdf}
}

@article{Coan2006,
  title = {Essential {{Commonality}} for {{Effective Future Extravehicular Activity Operations}}},
  author = {Coan, David and Bell, Ernest},
  year = {2006},
  journal = {SpaceOps 2006 Conference},
  doi = {10.2514/6.2006-5953},
  isbn = {978-1-62410-051-2},
  file = {/home/cameron/Zotero/storage/ICBZ8ZIH/effective-future-evas.pdf}
}

@inproceedings{coan2006,
  title = {Essential Commonality for Effective Future Extravehicular Activity Operations},
  booktitle = {{{SpaceOps}} 2006 {{Conference}}},
  author = {Coan, David A. and Bell, Ernest R.},
  year = {2006},
  doi = {10.2514/6.2006-5953},
  abstract = {During future manned exploration of space, missions will require astronauts to perform Extravehicular Activities (EVAs, i.e. spacewalks) both in space (zero-gravity) and on the surface of another planetary body (the moon or Mars). These EVAs will take place after long periods in space and on unique vehicles and structures. Considering the remoteness and time spans in which these vehicles will operate, EVA compatible systems should utilize common worksites, tools, hardware, procedures, and components as much as possible to increase the efficiency of training and proficiency in operations. \textcopyright{} 2006 by the American Institute of Aeronautics and Astronautics, Inc.},
  isbn = {978-1-62410-051-2},
  file = {/home/cameron/Zotero/storage/KBCKFPHG/effective-future-evas.pdf}
}

@article{coan2018,
  title = {Operational {{Field Testing}} for {{Human Exploration WHAT ARE OPERATIONAL}}},
  author = {Coan, David},
  year = {2018},
  file = {/home/cameron/Zotero/storage/LNPA6APC/integrated-analogs.pdf}
}

@techreport{Coan2020,
  title = {Exploration {{EVA System Concept}} of {{Operations Summary}} for {{Artemis Phase}} 1 {{Lunar Surface Mission}}},
  author = {Coan, David A.},
  year = {2020},
  pages = {1--35},
  address = {{Houston, TX}},
  institution = {{NASA}},
  file = {/home/cameron/Zotero/storage/JGHAWNJX/m-api-6cc6b499-3577-ceb1-35df-23960bed36b9.pdf}
}

@techreport{Coan2020a,
  title = {Exploration {{EVA System Concept}} of {{Operations}}},
  author = {Coan, David A.},
  year = {2020},
  number = {EVA-EXP-0042 Revision B},
  pages = {1--175},
  address = {{Houston, TX}},
  institution = {{National Aeronautics and Space Administration}},
  abstract = {The Exploration Extravehicular Activity (xEVA) System concept of operations (con ops) captures the National Aeronautics and Space Administration's (NASA's) current aimsfuture missions to all potential Exploration destinations. This document captures the mission architectures, stakeholder expectations, and high level definitions of the capabilities and interfaces associated with the xEVA System. This includes missions to Gateway in cislunar space, the lunar surface, a redirected asteroid in cislunar space, Near Earth Asteroids (NEA), Mars' orbit, the moons of Mars (Phobos and Deimos), and the surface of Mars. These missions, which include microgravity, milli-gravity, and partial-gravity surface EVAs, will involve a variety of engineering (maintenance, contingenc y, pioneering, construction) and science tasks. This document also captures information concerning vehicles and habitats with which the xEVA System will interface. The concepts of operations (con ops) detailed in this document are informed by the Artemis Program and a multitude of Exploration studies, and are also influenced by various integrated operational analog testing.},
  file = {/home/cameron/Zotero/storage/EGZJ44YM/Coan - 2020 - Exploration EVA System Concept of Operations.pdf}
}

@article{cockell2019,
  title = {A {{Low-Diversity Microbiota Inhabits Extreme Terrestrial Basaltic Terrains}} and {{Their Fumaroles}} : {{Implications}} for the {{Exploration}} of {{Mars}}},
  author = {Cockell, Charles S and Harrison, Jesse P and Stevens, Adam H. and Payler, Samuel J. and Hughes, Scott S. and Nawotniak, Shannon E Kobs and Brady, Allyson L. and Elphic, Richard C and Haberle, Christopher W and Sehlke, Alexander and Beaton, Kara H. and Abercromby, Andrew F.J. and Schwendner, Petra and Wadsworth, Jennifer and Landenmark, Hanna and Cane, Rosie and Dickinson, Andrew W and Nicholson, Natasha and Perera, Liam and Lim, Darlene S.S.},
  year = {2019},
  volume = {19},
  number = {3},
  pages = {284--299},
  doi = {10.1089/ast.2018.1870},
  file = {/home/cameron/Zotero/storage/3JJWMGEE/ast.2018.1870.pdf}
}

@article{Combi2014,
  title = {An {{Algorithm}} for {{Checking}} the {{Dynamic Controllability}} of a {{Conditional Simple Temporal Network}} with {{Uncertainty}} - {{Revisited}}},
  author = {Combi, Carlo and Hunsberger, Luke and Posenato, Roberto},
  year = {2014},
  journal = {Communications in Computer and Information Science},
  volume = {449},
  number = {February},
  pages = {314--331},
  issn = {18650929},
  doi = {10.1007/978-3-662-44440-5_19},
  abstract = {A Simple Temporal Network with Uncertainty (STNU) is a framework for representing and reasoning about temporal problems involving actions whose durations are bounded but uncontrollable. A dynamically controllable STNU is one for which there exists a strategy for executing its time-points that guarantees that all of the temporal constraints in the network will be satisfied no matter how the uncontrollable durations turn out. A Conditional Simple Temporal Network with Uncertainty (CSTNU) augments an STNU to include observation nodes, the execution of which incrementally and dynamically determines the set of constraints that must be satisfied. Previously, we generalized the notion of dynamic controllability to cover CSTNUs and presented a sound algorithm for determining whether arbitrary CSTNUs are dynamically controllable. That algorithm extends edge-generation/constraintpropagation rules from an existing DC-checking algorithm for STNUs with new rules required to deal with the observation nodes. This paper revisits that algorithm, modifying some of its rules to cover more cases, while preserving the soundness of the algorithm.},
  isbn = {9783662444399},
  keywords = {Temporal Controllability,Temporal Network,Temporal Uncertainty,Temporal Workflow},
  file = {/home/cameron/Zotero/storage/7LH7A32P/algorithm4CSTNU_ICAART13_SUBMITTED_FINAL.pdf}
}

@techreport{conrad2011,
  title = {Drake: {{An Efficient Executive}} for {{Temporal Plans}} with {{Choice}}},
  author = {Conrad, Patrick R and Williams, Brian C.},
  year = {2011},
  journal = {Journal of Artificial Intelligence Research},
  volume = {42},
  pages = {607--659},
  abstract = {This work presents Drake, a dynamic executive for temporal plans with choice. Dynamic plan execution strategies allow an autonomous agent to react quickly to unfolding events, improving the robustness of the agent. Prior work developed methods for dynamically dispatching Simple Temporal Networks, and further research enriched the expressive-ness of the plans executives could handle, including discrete choices, which are the focus of this work. However, in some approaches to date, these additional choices induce significant storage or latency requirements to make flexible execution possible. Drake is designed to leverage the low latency made possible by a preprocessing step called compilation, while avoiding high memory costs through a compact representation. We leverage the concepts of labels and environments, taken from prior work in Assumption-based Truth Maintenance Systems (ATMS), to concisely record the implications of the discrete choices, exploiting the structure of the plan to avoid redundant reasoning or storage. Our labeling and maintenance scheme, called the Labeled Value Set Maintenance System, is distinguished by its focus on properties fundamental to temporal problems, and, more generally, weighted graph algorithms. In particular, the maintenance system focuses on maintaining a minimal representation of non-dominated constraints. We benchmark Drake's performance on random structured problems, and find that Drake reduces the size of the compiled representation by a factor of over 500 for large problems, while incurring only a modest increase in run-time latency, compared to prior work in compiled executives for temporal plans with discrete choices.},
  file = {/home/cameron/Zotero/storage/MBX6JN8F/DrakeJAIR Final.pdf}
}

@article{Corallo2020,
  title = {Bringing {{GNU Emacs}} to {{Native Code}}},
  author = {Corallo, Andrea and Nassi, Luca and Manca, Nicola},
  year = {2020},
  journal = {arXiv},
  eprint = {2004.02504},
  eprinttype = {arxiv},
  issn = {23318422},
  doi = {10.5281/zenodo.3736363},
  abstract = {Emacs Lisp (Elisp) is the Lisp dialect used by the Emacs text editor family. GNU Emacs can currently execute Elisp code either interpreted or byte-interpreted after it has been compiled to byte-code. In this work we discuss the implementation of an optimizing compiler approach for Elisp targeting native code. The native compiler employs the byte-compiler's internal representation as input and exploits libgccjit to achieve code generation using the GNU Compiler Collection (GCC) infrastructure. Generated executables are stored as binary files and can be loaded and unloaded dynamically. Most of the functionality of the compiler is written in Elisp itself, including several optimization passes, paired with a C back-end to interface with the GNU Emacs core and libgccjit. Though still a work in progress, our implementation is able to bootstrap a functional Emacs and compile all lexically scoped Elisp files, including the whole GNU Emacs Lisp Package Archive (ELPA) [6]. Native-compiled Elisp shows an increase of performance ranging from 2.3x up to 42x with respect to the equivalent byte-code, measured over a set of small benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Elisp,GCC,GNU Emacs,Libgccjit},
  file = {/home/cameron/Zotero/storage/AVW3C47B/2004.02504.pdf}
}

@article{Crisp2003,
  title = {Mars {{Exploration Rover}} Mission},
  author = {Crisp, Joy A. and Adler, Mark and Matijevic, Jacob R. and Squyres, Steven W. and Arvidson, Raymond E. and Kass, David M.},
  year = {2003},
  journal = {Journal of Geophysical Research E: Planets},
  volume = {108},
  number = {12},
  issn = {01480227},
  doi = {10.1045/january2004-featured.collection},
  abstract = {In January 2004 the Mars Exploration Rover mission will land two rovers at two different landing sites that show possible evidence for past liquid-water activity. The spacecraft design is based on the Mars Pathfinder configuration for cruise and entry, descent, and landing. Each of the identical rovers is equipped with a science payload of two remote-sensing instruments that will view the surrounding terrain from the top of a mast, a robotic arm that can place three instruments and a rock abrasion tool on selected rock and soil samples, and several onboard magnets and calibration targets. Engineering sensors and components useful for science investigations include stereo navigation cameras, stereo hazard cameras in front and rear, wheel motors, wheel motor current and voltage, the wheels themselves for digging, gyros, accelerometers, and reference solar cell readings. Mission operations will allow commanding of the rover each Martian day, or sol, on the basis of the previous sol's data. Over a 90-sol mission lifetime, the rovers are expected to drive hundreds of meters while carrying out field geology investigations, exploration, and atmospheric characterization. The data products will be delivered to the Planetary Data System as integrated batch archives. Copyright 2003 by the American Geophysical Union.},
  keywords = {Mars,Mars Exploration Rover,Rover},
  file = {/home/cameron/Zotero/storage/RM7PX5TR/2002JE002038.pdf}
}

@article{Cui2019,
  title = {Dynamic Controllability of Controllable Conditional Temporal Problems with Uncertainty},
  author = {Cui, Jing and Haslum, Patrik},
  year = {2019},
  journal = {Journal of Artificial Intelligence Research},
  volume = {64},
  pages = {445--495},
  issn = {10769757},
  doi = {10.1613/jair.1.11375},
  abstract = {Dynamic Controllability (DC) of a Simple Temporal Problem with Uncertainty (STPU) uses a dynamic decision strategy, rather than a fixed schedule, to tackle temporal uncertainty. We extend this concept to the Controllable Conditional Temporal Problem with Uncertainty (CCTPU), which extends the STPU by conditioning temporal constraints on the assignment of controllable discrete variables. We define dynamic controllability of a CCTPU as the existence of a strategy that decides on both the values of discrete choice variables and the scheduling of controllable time points dynamically. This contrasts with previous work, which made a static assignment of choice variables and dynamic decisions over time points only. We propose an algorithm to find such a fully dynamic strategy. The algorithm computes the ``envelope'' of outcomes of temporal uncertainty in which a particular assignment of discrete variables is feasible, and aggregates these over all choices. When an aggregated envelope covers all uncertain situations of the CCTPU, the problem is dynamically controllable. However, the algorithm is complete only under certain assumptions. Experiments on an existing set of CCTPU benchmarks show that there are cases in which making both discrete and temporal decisions dynamically it is feasible to satisfy the problem constraints while assigning the discrete variables statically it is not.},
  file = {/home/cameron/Zotero/storage/IL9NZWM8/Cui, Haslum - 2019 - Dynamic controllability of controllable conditional temporal problems with uncertainty.pdf}
}

@article{cvach2012,
  title = {Monitor Alarm Fatigue: {{An}} Integrative Review},
  author = {Cvach, Maria},
  year = {2012},
  month = jul,
  journal = {Biomedical Instrumentation and Technology},
  volume = {46},
  number = {4},
  pages = {268--277},
  issn = {08998205},
  doi = {10.2345/0899-8205-46.4.268},
  abstract = {Alarm fatigue is a national problem and the number one medical device technology hazard in 2012. The problem of alarm desensitization is multifaceted and related to a high false alarm rate, poor positive predictive value, lack of alarm standardization, and the number of alarming medical devices in hospitals today. This integrative review synthesizes research and non-research findings published between 1/1/2000 and 10/1/2011 using The Johns Hopkins Nursing Evidence-Based Practice model. Seventy-two articles were included. Research evidence was organized into five main themes: excessive alarms and effects on staff; nurse's response to alarms; alarm sounds and audibility; technology to reduce false alarms; and alarm notification systems. Non-research evidence was divided into two main themes: strategies to reduce alarm desensitization, and alarm priority and notification systems. Evidence-based practice recommendations and gaps in research are summarized. \textcopyright{} Copyright AAMI 2012.},
  pmid = {22839984},
  file = {/home/cameron/Zotero/storage/X8W3QK4D/alarm-fatigue-and-patient-safety.pdf}
}

@book{CWilliams2016,
  title = {Teacher {{Pioneers}}},
  editor = {{Williams-Pierce}, Caro},
  year = {2016},
  publisher = {{Carnegie Mellon: ETC Press}},
  address = {{Pittsburgh, PA}},
  doi = {10.1184/R1/6686936},
  isbn = {978-1-365-31815-3},
  file = {/home/cameron/Zotero/storage/TRXCE22I/TeacherPioneers.pdf}
}

@article{Dan,
  title = {D r a f t {{IP Geolocation}} through {{Reverse DNS}}},
  author = {Dan, Ovidiu and Parikh, Vaibhav and Davison, Brian D},
  eprint = {1811.04288v1},
  eprinttype = {arxiv},
  abstract = {IP Geolocation databases are widely used in online services to map end user IP addresses to their geographical locations. However, they use proprietary geolocation methods and in some cases they have poor accuracy. We propose a systematic approach to use publicly accessible reverse DNS hostnames for geolocating IP addresses. Our method is designed to be combined with other geolocation data sources. We cast the task as a machine learning problem where for a given hostname, we generate and rank a list of potential location candidates. We evaluate our approach against three state of the art academic baselines and two state of the art commercial IP geolocation databases. We show that our work significantly outperforms the academic baselines, and is complementary and competitive with commercial databases. To aid reproducibility, we open source our entire approach.},
  archiveprefix = {arXiv},
  keywords = {â€¢ Networks â†’ Location based services,â€¢ Social and professional topics â†’ Geographic char,CCS CONCEPTS â€¢ Information systems â†’ Location base,KEYWORDS IP geolocation; hostname geolocation; geo,Network measurement,Public Inter-net},
  file = {/home/cameron/Zotero/storage/FHW4SWIW/1811.04288.pdf}
}

@article{Dantas2018,
  title = {Improving Time Series Forecasting: {{An}} Approach Combining Bootstrap Aggregation, Clusters and Exponential Smoothing},
  author = {Dantas, Tiago Mendes and Cyrino Oliveira, Fernando Luiz},
  year = {2018},
  journal = {International Journal of Forecasting},
  volume = {34},
  number = {4},
  pages = {748--761},
  publisher = {{Elsevier B.V.}},
  issn = {01692070},
  doi = {10.1016/j.ijforecast.2018.05.006},
  abstract = {Some recent papers have demonstrated that combining bagging (bootstrap aggregating) with exponential smoothing methods can produce highly accurate forecasts and improve the forecast accuracy relative to traditional methods. We therefore propose a new approach that combines the bagging, exponential smoothing and clustering methods. The existing methods use bagging to generate and aggregate groups of forecasts in order to reduce the variance. However, none of them consider the effect of covariance among the group of forecasts, even though it could have a dramatic impact on the variance of the group, and therefore on the forecast accuracy. The proposed approach, referred to here as Bagged.Cluster.ETS, aims to reduce the covariance effect by using partitioning around medoids (PAM) to produce clusters of similar forecasts, then selecting several forecasts from each cluster to create a group with a reduced variance. This approach was tested on various different time series sets from the M3 and CIF 2016 competitions. The empirical results have shown a substantial reduction in the forecast error, considering sMAPE and MASE.},
  keywords = {Bagging methods,Clustering time series,Exponential smoothing,Partitioning around medoids,Variance reduction},
  file = {/home/cameron/Zotero/storage/K6YJM24J/improving-time-series.pdf}
}

@article{dantas2018,
  title = {Improving Time Series Forecasting: {{An}} Approach Combining Bootstrap Aggregation, Clusters and Exponential Smoothing},
  author = {Dantas, Tiago Mendes and Cyrino Oliveira, Fernando Luiz},
  year = {2018},
  month = oct,
  journal = {International Journal of Forecasting},
  volume = {34},
  number = {4},
  pages = {748--761},
  publisher = {{Elsevier B.V.}},
  issn = {01692070},
  doi = {10.1016/j.ijforecast.2018.05.006},
  abstract = {Some recent papers have demonstrated that combining bagging (bootstrap aggregating) with exponential smoothing methods can produce highly accurate forecasts and improve the forecast accuracy relative to traditional methods. We therefore propose a new approach that combines the bagging, exponential smoothing and clustering methods. The existing methods use bagging to generate and aggregate groups of forecasts in order to reduce the variance. However, none of them consider the effect of covariance among the group of forecasts, even though it could have a dramatic impact on the variance of the group, and therefore on the forecast accuracy. The proposed approach, referred to here as Bagged.Cluster.ETS, aims to reduce the covariance effect by using partitioning around medoids (PAM) to produce clusters of similar forecasts, then selecting several forecasts from each cluster to create a group with a reduced variance. This approach was tested on various different time series sets from the M3 and CIF 2016 competitions. The empirical results have shown a substantial reduction in the forecast error, considering sMAPE and MASE.},
  keywords = {Bagging methods,Clustering time series,Exponential smoothing,Partitioning around medoids,Variance reduction},
  file = {/home/cameron/Zotero/storage/X7VR4XD7/improving-time-series.pdf}
}

@article{Daum2017,
  title = {Multitarget-{{Multisensor Tracking}} : {{Principles}} and {{Techniques}} [ {{Book Review}} ] {{Book Review}} `` {{Multitarget-Multisensor Tracking}} : {{Principles}} and {{Techniques}} ''},
  author = {Daum, Fred and Company, Raytheon},
  year = {2017},
  number = {March 1996},
  doi = {10.1109/MAES.1996.484305},
  file = {/home/cameron/Zotero/storage/UUYDTBVB/Multitarget-Multisensor_Tracking_Principles_and_Te.pdf}
}

@article{Davis1960,
  title = {A {{Computing Procedure}} for {{Quantification Theory}}},
  author = {Davis, Martin and Putnam, Hilary},
  year = {1960},
  journal = {Journal of the ACM (JACM)},
  volume = {7},
  number = {3},
  pages = {201--215},
  issn = {1557735X},
  doi = {10.1145/321033.321034},
  abstract = {The hope that mathematical methods employed in the investigation of formal logic would lead to purely computational methods for obtaining mathematical theorems goes back to Leibniz and has been revived by Peano around the turn of the century and by Hilbert's school in the 1920's. Hilbert, noting that all of classical mathematics could be formalized within quantification theory, declared that the problem of finding an algorithm for determining whether or not a given formula of quantification theory is valid was the central problem of mathematical logic. And indeed, at one time it seemed as if investigations of this èˆ decisionèˆ¡ problem were on the verge of success. However, it was shown by Church and by Turing that such an algorithm can not exist. This result led to considerable pessimism regarding the possibility of using modern digital computers in deciding significant mathematical questions. However, recently there has been a revival of interest in the whole question. Specifically, it has been realized that while no decision procedure exists for quantification theory there are many proof procedures availableèˆ’that is, uniform procedures which will ultimately locate a proof for any formula of quantification theory which is valid but which will usually involve seeking èˆ foreverèˆ¡ in the case of a formula which is not validèˆ’and that some of these proof procedures could well turn out to be feasible for use with modern computing machinery. Hao Wang [9] and P. C. Gilmore [3] have each produced working programs which employ proof procedures in quantification theory. Gilmore's program employs a form of a basic theorem of mathematical logic due to Herbrand, and Wang's makes use of a formulation of quantification theory related to those studied by Gentzen. However, both programs encounter decisive difficulties with any but the simplest formulas of quantification theory, in connection with methods of doing propositional calculus. Wang's program, because of its use of Gentzen-like methods, involves exponentiation on the total number of truth-functional connectives, whereas Gilmore's program, using normal forms, involves exponentiation on the number of clauses present. Both methods are superior in many cases to truth table methods which involve exponentiation on the total number of variables present, and represent important initial contributions, but both run into difficulty with some fairly simple examples.In the present paper, a uniform proof procedure for quantification theory is given which is feasible for use with some rather complicated formulas and which does not ordinarily lead to exponentiation. The superiority of the present procedure over those previously available is indicated in part by the fact that a formula on which Gilmore's routine for the IBM 704 causes the machine to computer for 21 minutes without obtaining a result was worked successfully by hand computation using the present method in 30 minutes. Cf. 6, below.It should be mentioned that, before it can be hoped to employ proof procedures for quantification theory in obtaining proofs of theorems belonging to èˆ genuineèˆ¡ mathematics, finite axiomatizations, which are èˆ short,èˆ¡ must be obtained for various branches of mathematics. This last question will not be pursued further here; cf., however, Davis and Putnam [2], where one solution to this problem is given for ele]]. \textcopyright{} 1960, ACM. All rights reserved.},
  file = {/home/cameron/Zotero/storage/3LL342G5/Davis, Putnam - 1960 - A Computing Procedure for Quantification Theory.pdf}
}

@article{Davis1962,
  title = {A {{Machine Program}} For},
  author = {Davis, Martin},
  year = {1962},
  journal = {Communications ACM},
  volume = {5},
  pages = {394--397},
  file = {/home/cameron/Zotero/storage/L66UEY3Q/Davis - 1962 - A Machine Program for.pdf}
}

@article{Dechter1991,
  title = {Temporal Constraint Networks},
  author = {Dechter, Rina and Meiri, Itay and Pearl, Judea},
  year = {1991},
  journal = {Artificial Intelligence},
  volume = {49},
  number = {1-3},
  pages = {61--95},
  issn = {00043702},
  doi = {10.1016/0004-3702(91)90006-6},
  abstract = {This paper extends network-based methods of constraint satisfaction to include continuous variables, thus providing a framework for processing temporal constraints. In this framework, called temporal constraint satisfaction problem (TCSP), variables represent time points and temporal information is represented by a set of unary and binary constraints, each specifying a set of permitted intervals. The unique feature of this framework lies in permitting the processing of metric information, namely, assessments of time differences between events. We present algorithms for performing the following reasoning tasks: finding all feasible times that a given event can occur, finding all possible relationships between two given events, and generating one or more scenarios consistent with the information provided. We distinguish between simple temporal problems (STPs) and general temporal problems, the former admitting at most one interval constraint on any pair of time points. We show that the STP, which subsumes the major part of Vilain and Kautz's point algebra, can be solved in polynomial time. For general TCSPs, we present a decomposition scheme that performs the three reasoning tasks considered, and introduce a variety of techniques for improving its efficiency. We also study the applicability of path consistency algorithms as preprocessing of temporal problems, demonstrate their termination and bound their complexities. \textcopyright{} 1991.},
  file = {/home/cameron/Zotero/storage/C743G3Y9/Dechter, Meiri, Pearl - 1991 - Temporal constraint networks.pdf;/home/cameron/Zotero/storage/MMRKGLAQ/Dechter, Meiri, Pearl - 1991 - Temporal Constraint Networks.pdf}
}

@article{DeKleer1987,
  title = {Diagnosing {{Multiple Faults}}},
  author = {{de Kleer}, Johan and Williams, Brian C.},
  year = {1987},
  journal = {Artificial Intelligence},
  volume = {32},
  number = {1},
  pages = {97--130},
  issn = {00043702},
  doi = {10.1016/0004-3702(87)90063-4},
  abstract = {Diagnostic tasks require determining the differences between a model of an artifact and the artifact itself. The differences between the manifested behavior of the artifact and the predicted behavior of the model guide the search for the differences between the artifact and its model. The diagnostic procedure presented in this paper is model-based, inferring the behavior of the composite device from knowledge of the structure and function of the individual components comprising the device. The system (GDE-general diagnostic engine) has been implemented and tested on many examples in the domain of troubleshooting digital circuits. This research makes several novel contributions: First, the system diagnoses failures due to multiple faults. Second, failure candidates are represented and manipulated in terms of minimal sets of violated assumptions, resulting in an efficient diagnostic procedure. Third, the diagnostic procedure is incremental, exploiting the iterative nature of diagnosis. Fourth, a clear separation is drawn between diagnosis and behavior prediction, resulting in a domain (and inference procedure) independent diagnostic procedure. Fifth, GDE combines model-based prediction with sequential diagnosis to propose measurements to localize the faults. The normally required conditional probabilities are computed from the structure of the device and models of its components. This capability results from a novel way of incorporating probabilities and information theory into the context mechanism provided by assumption-based truth maintenance. \textcopyright{} 1987.},
  file = {/home/cameron/Zotero/storage/NPEL3PKG/Diagnosing+Multiple+Faults(1).pdf}
}

@inproceedings{DelaTour1990,
  title = {Minimizing the Number of Clauses by Renaming},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {{de la Tour}, Thierry Boy},
  year = {1990},
  volume = {449 LNAI},
  pages = {558--572},
  issn = {16113349},
  doi = {10.1007/3-540-52885-7_114},
  abstract = {The problem of translating a formula into a ``good'' clause form is known to be a very important one in resolution automated theorem proving. Some theorems have such a huge clause form that it is practically impossible to prove them by resolution. Several methods exist to reduce clause forms, some applied before and some after translation. They are generally based on (local or non-local) simplifications. A method consisting in the creation and use of definitions, which we call renaming, has been developed in [5], [4] and [7]. Applying it in an exhaustive way was shown to yield a translation into clause form polynomial in size (see [7]). In this paper we investigate non exhaustive renamings in the purpose of minimizing the number of clauses. We obtain a translation which is shown to be polynomial in size, and optimal in number of clauses when the theorem contains no equivalence. An example shows that this translation is not necessarily optimal in presence of equivalence. Experiments with ``challenge'' examples show the practical efficiency of this translation.},
  isbn = {978-3-540-52885-2},
  file = {/home/cameron/Zotero/storage/ZVFJLQGH/de la Tour - 1990 - Minimizing the number of clauses by renaming.pdf}
}

@article{DellaPenna2012,
  title = {A Universal Planning System for Hybrid Domains},
  author = {Della~Penna, Giuseppe and Magazzeni, Daniele and Mercorio, Fabio},
  year = {2012},
  journal = {Applied Intelligence},
  volume = {36},
  number = {4},
  pages = {932--959},
  issn = {0924669X},
  doi = {10.1007/s10489-011-0306-z},
  abstract = {Many real world problems involve hybrid systems, subject to (continuous) physical effects and controlled by (discrete) digital equipments. Indeed, many efforts are being made to extend the current planning systems and modelling languages to support such kind of domains. However, hybrid systems often present also a nonlinear behaviour and planning with continuous nonlinear change that is still a challenging issue. In this paper we present the UPMurphi tool, a universal planner based on the discretise and validate approach that is capable of reasoning with mixed discrete/continuous domains, fully respecting the semantics of PDDL+. Given an initial discretisation, the hybrid system is discretised and given as input to UPMurphi, which performs universal planning on such an approximated model and checks the correctness of the results. If the validation fails, the approach is repeated by appropriately refining the discretisation. To show the effectiveness of our approach, the paper presents two real hybrid domains where universal planning has been successfully performed using the UPMurphi tool. \textcopyright{} 2011 Springer Science+Business Media, LLC.},
  keywords = {Hybrid systems,Universal planning},
  file = {/home/cameron/Zotero/storage/3KP8I4YR/Della Penna, Magazzeni, Mercorio - 2012 - A universal planning system for hybrid domains.pdf}
}

@inproceedings{deMoura2008,
  title = {Z3: {{An Efficient SMT Solver Leonardo}}},
  booktitle = {{{TACAS}}},
  author = {{de Moura}, Leonardo and Bjorner, Nikolaj},
  year = {2008},
  volume = {LNCS 4693},
  pages = {337--340},
  issn = {10648887},
  doi = {10.1007/s11182-005-0085-2},
  abstract = {Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and unin- terpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.},
  file = {/home/cameron/Zotero/storage/DD8Z4HQT/de Moura, Bjorner - 2008 - Z3 An Efficient SMT Solver Leonardo.pdf}
}

@article{Desaraju2012,
  title = {Decentralized Path Planning for Multi-Agent Teams with Complex Constraints},
  author = {Desaraju, Vishnu R. and How, Jonathan P.},
  year = {2012},
  journal = {Autonomous Robots},
  volume = {32},
  number = {4},
  pages = {385--403},
  issn = {09295593},
  doi = {10.1007/s10514-012-9275-2},
  abstract = {This paper presents a novel approach to address the challenge of planning paths for multi-agent systems subject to complex constraints. The technique, called the Decentralized Multi-Agent Rapidly-exploring Random Tree (DMA-RRT) algorithm, extends the Closed-loop RRT (CL-RRT) algorithm to handle multiple agents while retaining its ability to plan quickly. A core component of the DMA-RRT algorithm is a merit-based token passing coordination strategy that makes use of the tree of feasible trajectories grown in the CL-RRT algorithm to dynamically update the order in which agents replan. The reordering is based on a measure of each agent's incentive to change the plan and allows agents with a greater potential improvement to replan sooner, which is demonstrated to improve the team's overall performance compared to a traditional, scripted replan order. The main contribution of the work is a version of the algorithm, called Cooperative DMA-RRT, which introduces a cooperation strategy that allows an agent to modify its teammates' plans in order to select paths that reduce their combined cost. This modification further improves team performance and avoids certain common deadlock scenarios. The paths generated by both algorithms are proven to satisfy inter-agent constraints, such as collision avoidance, and numerous simulation and experimental results are presented to demonstrate their performance. \textcopyright{} 2011 Springer-Verlag.},
  keywords = {Collision avoidance,Cooperative planning,RRT},
  file = {/home/cameron/Zotero/storage/4R42PQJP/decentralized_path.pdf}
}

@phdthesis{Deyo2018,
  title = {Online {{Risk-Aware Conditional Planning}} with {{Qualitative Autonomous Driving Applications}}},
  author = {Deyo, Matthew Quinn},
  year = {2018},
  abstract = {Driving is often stressful and dangerous due to uncertainty in the actions of nearby vehicles. Having the ability to model driving maneuvers qualitatively and guarantee safety bounds in uncertain traffic scenarios are two steps towards building trust in vehicle autonomy. In this thesis, we present an approach to the problem of Quali- tative Autonomous Driving (QAD) using risk-bounded conditional planning. First, we present Incremental Risk-aware AO* (iRAO*), an online conditional planning al- gorithm that builds off of RAO* for use in larger dynamic systems like driving. An illustrative example is included to better explain the behavior and performance of the algorithm. Second, we present a Chance-Constrained Hybrid Multi-Agent MDP as a framework for modeling our autonomous vehicle in traffic scenarios using qualitative driving maneuvers. Third, we extend our driving model by adding variable duration to maneuvers and develop two approaches to the resulting complexity. We present planning results from various driving scenarios, as well as from scaled instances of the illustrative example, that show the potential for further applications. Finally, we propose a QAD system, using the different tools developed in the context of this thesis, and show how it would fit within an autonomous driving architecture.},
  file = {/home/cameron/Zotero/storage/AC83B43G/1036985630-MIT.pdf}
}

@techreport{dietterich,
  title = {Machine {{Learning}} for {{Sequential Data}}: {{A Review}}},
  author = {Dietterich, Thomas G},
  abstract = {Statistical learning problems in many fields involve sequential data. This paper formalizes the principal learning tasks and describes the methods that have been developed within the machine learning research community for addressing these problems. These methods include sliding window methods, recurrent sliding windows, hidden Markov models , conditional random fields, and graph transformer networks. The paper also discusses some open research issues.},
  file = {/home/cameron/Zotero/storage/2VLQJLFH/mlsd-ssspr.pdf}
}

@article{Dietterich2002,
  title = {(Impo)({{George}} Recom)({{Sequenced}} Prediction){{Machine}} Learning for Sequential Data: {{A}} Review},
  author = {Dietterich, Tg},
  year = {2002},
  journal = {Structural, syntactic, and statistical pattern recognition},
  pages = {1--15},
  issn = {87567016},
  doi = {10.1146/annurev.cs.04.060190.001351},
  abstract = {Statistical learning problems in many fields involve sequen- tial data. This paper formalizes the principal learning tasks and describes the methods that have been developed within the machine learning re- search community for addressing these problems. These methods include sliding window methods, recurrent sliding windows, hidden Markov mod- els, conditional random fields, and graph transformer networks. The pa- per also discusses some open research issues.},
  arxiv = {0-387-31073-8},
  isbn = {1595930469},
  pmid = {18292226},
  file = {/home/cameron/Zotero/storage/5E99K83D/mlsd-ssspr.pdf}
}

@techreport{directorate2017,
  title = {Development {{Specification}} for the {{FN-323}} / 324 , {{Oxygen Ventilation Loop Fan Assembly Revision}} : {{A Development Specification}} for the {{FN-323}} / 324 , {{Oxygen Ventilation Loop Fan Assembly}}},
  author = {Directorate, Engineering and Division, Thermal Systems},
  year = {2017},
  address = {{Houston, TX}},
  institution = {{National Aeronautics and Space Administration}},
  file = {/home/cameron/Zotero/storage/W8Z96EC2/20170010141.pdf}
}

@article{Do2016,
  title = {Towards {{Earth Independence}} -- {{Tradespace Exploration}} of {{Long-Duration Crewed Mars Surface System Architectures}}},
  author = {Do, Sydney},
  year = {2016},
  journal = {Hons. 1 \& Univ. Medal) Aeronautics and Astronautics Massachusetts Institute of Technology},
  number = {2008},
  pages = {1--599},
  abstract = {In recent years, an unprecedented level of interest has grown around the prospect of sending humans to Mars for the exploration and eventual settlement of that planet. With the signing of the 2010 NASA Authorization Act, this goal became the official policy of the United States and consequently, has become the long-term objective of NASA's human spaceflight activities. A review of past Mars mission planning efforts, however, reveals that while numerous analyses have studied the challenges of transporting people to the red planet, relatively little analyses have been performed in characterizing the challenges of sustaining humans upon arrival. In light of this observation, this thesis develops HabNet \textendash{} an integrated Habitation, Environmental Control and Life Support (ECLS), In-Situ Resource Utilization (ISRU), and Supportability analysis framework \textendash{} and applies it to three different Mars mission scenarios to analyze the impacts of different system architectures on the costs of deploying and sustaining a continuous human presence on the surface of Mars. Through these case studies, a number of new insights on the mass-optimality of Mars surface system architectures are derived. The most significant of these is the finding that ECLS architecture mass-optimality is strongly dependent on the cost of ISRU \textendash{} where open-loop ECLS architectures become mass-optimal when the cost of ISRU is low, and ECLS architectures with higher levels of resource recycling become mass-optimal when the cost of ISRU is high. For the Martian surface, the relative abundance of resources equates to a low cost of ISRU, which results in an open-loop ECLS system supplemented with ISRU becoming an attractive, if not dominant surface system architecture, over a range of mission scenarios and ISRU performance levels. This result, along with the others made in this thesis, demonstrates the large potential of integrated system analyses in uncovering previously unseen trends within the Mars mission architecture tradespace. By integrating multiple traditionally disparate spaceflight disciplines into a unified analysis framework, this thesis attempts to make the first steps towards codifying the human spaceflight mission architecting process, with the ultimate goal of enabling the efficient evaluation of the architectural decisions that will shape humanity's expansion into the cosmos.},
  file = {/home/cameron/Zotero/storage/G7MKR69W/961321393-MIT.pdf}
}

@article{Domshlak2015,
  title = {Deterministic Oversubscription Planning as Heuristic Search: {{Abstractions}} and Reformulations},
  author = {Domshlak, Carmel and Mirkis, Vitaly},
  year = {2015},
  journal = {Journal of Artificial Intelligence Research},
  volume = {52},
  pages = {97--169},
  issn = {10769757},
  doi = {10.1613/jair.4443},
  abstract = {While in classical planning the objective is to achieve one of the equally attractive goal states at as low total action cost as possible, the objective in deterministic oversubscription planning (OSP) is to achieve an as valuable as possible subset of goals within a fixed allowance of the total action cost. Although numerous applications in various fields share the latter objective, no substantial algorithmic advances have been made in deterministic OSP. Tracing the key sources of progress in classical planning, we identify a severe lack of effective domain-independent approximations for OSP. With our focus here on optimal planning, our goal is to bridge this gap. Two classes of approximation techniques have been found especially useful in the context of optimal classical planning: those based on state-space abstractions and those based on logical land- marks for goal reachability. The question we study here is whether some similar-in-spirit, yet possibly mathematically different, approximation techniques can be developed for OSP. In the context of abstractions, we define the notion of additive abstractions for OSP, study the complexity of deriving effective abstractions from a rich space of hypotheses, and reveal some substantial, empirically relevant islands of tractability. In the context of landmarks, we show how standard goal-reachability landmarks of certain classical planning tasks can be compiled into the OSP task of interest, resulting in an equivalent OSP task with a lower cost allowance, and thus with a smaller search space. Our empirical evaluation confirms the effectiveness of the proposed techniques, and opens a wide gate for further developments in oversubscription planning.},
  file = {/home/cameron/Zotero/storage/52JBKJWH/Domshlak, Mirkis - 2015 - Deterministic oversubscription planning as heuristic search Abstractions and reformulations.pdf}
}

@techreport{dong,
  title = {Compliant {{Task Execution}} and {{Learning}} for {{Safe Mixed-Initiative Human-Robot Operations}}},
  author = {Dong, Shuonan and Conrad, Patrick R and Shah, Julie A and Williams, Brian C. and Mittman {\textparagraph}, David S and Ingham, Michel D and Verma, Vandana},
  abstract = {We introduce a novel task execution capability that enhances the ability of in-situ crew members to function independently from Earth by enabling safe and efficient interaction with automated systems. This task execution capability provides the ability to (1) map goal-directed commands from humans into safe, compliant, automated actions, (2) quickly and safely respond to human commands and actions during task execution, and (3) specify complex motions through teaching by demonstration. Our results are applicable to future surface robotic systems, and we have demonstrated these capabilities on JPL's All-Terrain Hex-Limbed Extra-Terrestrial Explorer (ATHLETE) robot.},
  file = {/home/cameron/Zotero/storage/J3KD7HPK/11-1172.pdf}
}

@article{Dormand1986,
  title = {A Reconsideration of Some Embedded {{Runge-Kutta}} Formulae},
  author = {Dormand, J. R. and Prince, P. J.},
  year = {1986},
  journal = {Journal of Computational and Applied Mathematics},
  volume = {15},
  number = {2},
  pages = {203--211},
  issn = {03770427},
  doi = {10.1016/0377-0427(86)90027-0},
  abstract = {The RK5(4) and RK6(5) embedded Runge-Kutta formulae are reconsidered with regard to enlarging regions of absolute stability while retaining satisfactory truncation error norms. Results from standard tests for the above pairs are presented in comparison with an efficient RK8(7) embedded formula. \textcopyright{} 1986.},
  keywords = {absolute stability,efficiency curve,Runge-Kutta,truncation error norm},
  file = {/home/cameron/Zotero/storage/RCTUALYN/Dormand, Prince - 1986 - A reconsideration of some embedded Runge-Kutta formulae.pdf}
}

@article{Dormand1989,
  title = {Practical {{Runge}}\textendash{{Kutta Processes}}},
  author = {Dormand, J. R. and Prince, P. J.},
  year = {1989},
  journal = {SIAM Journal on Scientific and Statistical Computing},
  issn = {0196-5204},
  doi = {10.1137/0910057},
  abstract = {The development of embedded Runge\textendash Kutta and Runge\textendash Kutta\textendash Nystrom formulae subject to various criteria is reviewed. An important criterion concerns the cost of achieving a particular global error in the numerical solution. By consideration of local truncation errors in the two formulae of an embedded pair, it is possible to produce a good process. Another criterion involves the provision of continuous solutions. Such a requirement can be at odds with the previous one of basic cost-effectiveness. However, it seems important to provide dense output without excessive cost in new function evaluations. Special RK/RKN formulae are preferable for practical global error estimation using the Zadunaisky pseudo-problem or related technique of solving for the error estimate. Two-term error estimation can be achieved and the pseudo-problem can be based on dense output values.}
}

@article{Doshi2012,
  title = {Decision Making in Complex Multiagent Contexts: {{A}} Tale of Two Frameworks},
  author = {Doshi, Prashant},
  year = {2012},
  journal = {AI Magazine},
  volume = {33},
  number = {4},
  pages = {82--95},
  issn = {07384602},
  doi = {10.1609/aimag.v33i4.2402},
  abstract = {Decision making is a key feature of autonomous systems. It involves choosing optimally between different lines of action in various information contexts that range from perfectly knowing all aspects of the decision problem to having just partial knowledge about it. The physical context often includes other interacting autonomous systems, typically called agents. In this article, I focus on decision making in a multiagent context with partial information about the problem. Relevant research in this complex but realistic setting has converged around two complementary, general frameworks and also introduced myriad specializations on its way. I put the two frameworks, decentralized partially observable Markov decision process (Dec-POMDP) and the interactive partially observable Markov decision process (I-POMDP), in context and review the foundational algorithms for these frameworks, while briefly discussing the advances in their specializations. I conclude by examining the avenues that research pertaining to these frameworks is pursuing. Copyright \textcopyright{} 2012, Association for the Advancement of Artificial Intelligence.},
  keywords = {Winter 2012},
  file = {/home/cameron/Zotero/storage/CVDLFVYN/2402-Article Text-4068-1-10-20121206.pdf}
}

@article{downs,
  title = {Metabolic-Based Logistics Planning Tool | {{PI}}: {{M}}. {{Downs}}},
  author = {Downs, M},
  volume = {d},
  pages = {1--4},
  file = {/home/cameron/Zotero/storage/FSYPSTH9/Technical approach_9.18.18_final.pdf}
}

@article{Eckermann2018,
  title = {{{tinyLTE}}: {{Light}} Weight, {{Ad Hoc}} Deployable Cellular Network for Vehicular Communication},
  author = {Eckermann, Fabian and Gorczak, Philipp and Wietfeld, Christian},
  year = {2018},
  journal = {IEEE Vehicular Technology Conference},
  volume = {2018-June},
  pages = {1--5},
  issn = {15502252},
  doi = {10.1109/VTCSpring.2018.8417761},
  abstract = {\textcopyright{} 2018 IEEE. The application of LTE technology has evolved from infrastructure-based deployments in licensed bands to new use cases covering ad hoc, device-to-device communications and unlicensed band operation. Vehicular communication is an emerging field of particular interest for LTE, covering in our understanding both automotive (cars) as well as unmanned aerial vehicles. Existing commercial equipment is designed for infrastructure making it unsuitable for vehicular applications requiring low weight and unlicensed band support (e.g. 5.9 GHz ITS-band). In this work, we present tinyLTE, a system design which provides fully autonomous, multi-purpose and ultra-compact LTE cells by utilizing existing open source eNB and EPC implementations. Due to its small form factor and low weight, the tinyLTE system enables mobile deployment on board of cars and drones as well as smooth integration with existing roadside infrastructure. Additionally, the standalone design allows for systems to be chained in a multi-hop configuration. The paper describes the lean and low-cost design concept and implementation followed by a performance evaluation for single and two-hop configurations at 5.9 GHz. The results from both lab and field experiments validate the feasibility of the tinyLTE approach and demonstrate its potential to even support real-time vehicular applications (e.g. with a lowest average end-to-end latency of around 7 ms in the lab experiment).},
  isbn = {9781538663554},
  keywords = {Base Stations,Cooperative Communication,Device-to-Device Communication,Edge Computing,LTE,MANET,Mobile Nodes,Open Source Software,Overlay Networks,Relay Networks,Software Radio,VANET,Vehicular Communication},
  file = {/home/cameron/Zotero/storage/A49L4VPG/1802.09262.pdf}
}

@article{Edwards2001,
  title = {{{VIPER}} : {{Virtual Intelligent Planetary Exploration Rover}}},
  author = {Edwards, Laurence and Fl, Lorenzo and Richard, Nguyen and Area, Robotics and Ames, Nasa and Field, Moffett},
  year = {2001},
  keywords = {3d visualization,ecution,plan ex-,planetary rovers,simulation},
  file = {/home/cameron/Zotero/storage/KRT8D3XC/0246 (Edwards).pdf}
}

@article{Een2004,
  title = {An Extensible {{SAT-solver}}},
  author = {E{\'e}n, Niklas and S{\"o}rensson, Niklas},
  year = {2004},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {2919},
  pages = {502--518},
  issn = {16113349},
  doi = {10.1007/978-3-540-24605-3_37},
  abstract = {In this article1, we present a small, complete, and efficient SAT-solver in the style of conflict-driven learning, as exemplified by CHAFF. We aim to give sufficient details about implementation to enable the reader to construct his or her own solver in a very short time. This will allow users of SAT-solvers to make domain specific extensions or adaptions of current state-of-the-art SAT-techniques, to meet the needs of a particular application area. The presented solver is designed with this in mind, and includes among other things a mechanism for adding arbitrary boolean constraints. It also supports solving a series of related SAT-problems efficiently by an incremental SAT-interface. \textcopyright{} Springer-Verlag 2004.},
  isbn = {3540208518},
  file = {/home/cameron/Zotero/storage/CI54GRJY/EÃ©n, SÃ¶rensson - 2004 - An extensible SAT-solver.pdf}
}

@article{Effinger2006,
  title = {Extending Dynamic Backtracking to Solve Weighted Conditional {{CSPs}}},
  author = {Effinger, Robert T. and Williams, Brian C.},
  year = {2006},
  journal = {Proceedings of the National Conference on Artificial Intelligence},
  volume = {1},
  number = {Ginsberg 1993},
  pages = {28--35},
  abstract = {Many planning and design problems can be characterized as optimal search over a constrained network of conditional choices with preferences. To draw upon the advanced methods of constraint satisfaction to solve these types of problems, many dynamic and flexible CSP variants have been proposed. One such variant is the Weighted Conditional CSP (WCCSP). So far, however, little work has been done to extend the full suite of CSP search algorithms to solve these CSP variants. In this paper, we extend Dynamic Backtracking and similar backjumping-based CSP search algorithms to solve WCCSPs by utilizing activity constraints and soft constraints in order to quickly prune infeasible and suboptimal regions of the search space. We provide experimental results on randomly generated WCCSP instances to prove these claims. Copyright \textcopyright{} 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.},
  isbn = {1577352815},
  keywords = {constraint satisfaction,satisfiability},
  file = {/home/cameron/Zotero/storage/RWYJWILG/lec-03-reading-3.pdf}
}

@article{EffingerIV2006,
  title = {Optimal {{Temporal Planning}} at {{Reactive Time Scales}} via {{Dynamic Backtracking Branch}} and {{Bound}}},
  author = {Effinger, Robert T},
  year = {2006},
  pages = {115},
  abstract = {Autonomous robots are being considered for increasingly capable roles in our society, such as urban search and rescue, automation for assisted living, and lunar habitat construction. To fulfill these roles, teams of autonomous robots will need to cooperate together to accomplish complex mission objectives in uncertain and dynamic environments. In these environments, autonomous robots face a host of new challenges, such as responding robustly to timing uncertainties and perturbations, task and coordination failures, and equipment malfunctions. In order to address these challenges, this thesis advocates a novel planning approach, called temporally-flexible contingent planning. A temporally-flexible contingent plan is a compact encoding of methods for achieving the mission objectives which incorporates robustness through flexible task durations, redundant methods, constraints on when methods are applicable, and preferences between methods. This approach enables robots to adapt to unexpected changes on-the-fly by selecting alternative methods at runtime in order to satisfy as best possible the mission objectives. The drawback to this approach, however, is the computational overhead involved in selecting alternative methods at runtime in response to changes. If a robot takes too long to select a new plan, it could fail to achieve its near-term mission objectives and potentially incur damage. To alleviate this problem, and extend the range of applicability of temporally-flexible contingent planning to more demanding real-time systems, this thesis proposes a temporally-flexible contingent plan executive that selects new methods quickly and optimally in response to changes in a robot's health and environment. We enable fast and optimal method selection through two complimentary approaches. First, we frame optimal method selection as a constraint satisfaction problem (CSP) variant, called an Optimal Conditional CSP (OCCSP). Second, we extend fast CSP search algorithms, such as Dynamic Backtracking and Branch-and-Bound Search, to solve OCCSPs. Experiments on an autonomous rover test-bed and on randomly generated plans show that these contributions significantly improve the speed at which robots perform optimal method selection in response to changes in their health status and environment.},
  file = {/home/cameron/Zotero/storage/GRMGEASZ/144580849-MIT.pdf}
}

@article{element,
  title = {{{EVA Biomedical Advisory Algorithm Technology Demonstrator Data Input Module}}},
  author = {Element, Exploration Medical},
  pages = {1--9},
  file = {/home/cameron/Zotero/storage/JSUEATRQ/EVA Biomedical Algorithm_SOW_1-24-07.pdf}
}

@misc{Elliott2004,
  title = {An {{Efficient Projected Minimal Conflict Generator}} for {{Projected Prime Implicate}} and {{Implicant Generation}}},
  author = {Elliott, Paul H},
  year = {2004},
  pages = {111},
  publisher = {{Massachusetts Institute of Technology}},
  abstract = {Performing real-time reasoning on models of physical systems is essential in many situations, especially when human intervention is impossible. Since many deductive reasoning tasks take memory or time that is exponential in the number of variables that appear in the model, efforts need to be made to reduce the size of the models used online. The model can be reduced without sacrificing reasoning ability by targeting the model for a specific task, such as diagnosis or reconfiguration. A model may be reduced through model compilation, an offline process where relations and variables that have no bearing on the particular task are removed. This thesis introduces a novel approach to model compilation, through the generation of projected prime implicates and projected prime implicants. Prime implicates and prime implicants compactly represent the consequences of a logical theory. Projection eliminates model variables and their associated prime implicates or implicants that do not contribute to the particular task. This elimination process reduces the size and number of variables appearing in the model and therefore the complexity of the real-time reasoning problem. This thesis presents a minimal conflict generator that efficiently generates projected prime implicates and projected prime implicants. The projected minimal conflict generator uses a generate-and-test approach, in which the candidate generator finds potential minimal conflicts that are then accepted or rejected by the candidate tester. The candidate generator uses systematic search in combination with an iterative deepening algorithm, in order to reduce the space required by the algorithm to a space that is linear in the number of variables rather than exponential. In order to make the algorithm more time efficient, the candidate generator prunes the search space using previously found implicants, as well as minimal conflicts, which identify the sub-spaces that contain no new minimal conflicts. The candidate tester identifies implicants of the model by testing for validity. The tester uses a clause-directed approach along with finite-domain variables to efficiently test for validity. Combined, these techniques allow the tester to test for validity without assigning a value to every variable.T The conflict generator was evaluated on randomly generated models; problems in which models with 20 variables, 5 domain elements each, were projected onto 5 variables. All projected prime implicates were generated from models with 20 clauses within 2 seconds, and from models with 80 clauses within 13 seconds.},
  keywords = {minimal conflict generator,real-time logical reasoning},
  file = {/home/cameron/Zotero/storage/BGP7CSN6/m-api-59cbb5cc-c044-1e0b-7d32-36df5bdc6b21.pdf}
}

@article{Eppler2013a,
  title = {Desert {{Research}} and {{Technology Studies}} ({{DRATS}}) 2010 Science Operations: {{Operational}} Approaches and Lessons Learned for Managing Science during Human Planetary Surface Missions},
  author = {Eppler, Dean and Adams, Byron and Archer, Doug and Baiden, Greg and Brown, Adrian and Carey, William and Cohen, Barbara and Condit, Chris and Evans, Cindy and Fortezzo, Corey and Garry, Brent and Graff, Trevor and Gruener, John and Heldmann, Jennifer and Hodges, Kip and H{\"o}rz, Friedrich and Hurtado, Jose and Hynek, Brian and Isaacson, Peter and Juranek, Catherine and Klaus, Kurt and Kring, David and Lanza, Nina and Lederer, Susan and Lofgren, Gary and Marinova, Margarita and May, Lisa and Meyer, Jonathan and Ming, Doug and Monteleone, Brian and Morisset, Caroline and Noble, Sarah and Rampe, Elizabeth and Rice, James and Schutt, John and Skinner, James and {Tewksbury-Christle}, Carolyn M. and Tewksbury, Barbara J. and Vaughan, Alicia and Yingst, Aileen and Young, Kelsey},
  year = {2013},
  journal = {Acta Astronautica},
  volume = {90},
  number = {2},
  pages = {224--241},
  issn = {00945765},
  doi = {10.1016/j.actaastro.2012.03.009},
  abstract = {Desert Research and Technology Studies (Desert RATS) is a multi-year series of hardware and operations tests carried out annually in the high desert of Arizona on the San Francisco Volcanic Field. These activities are designed to exercise planetary surface hardware and operations in conditions where long-distance, multi-day roving is achievable, and they allow NASA to evaluate different mission concepts and approaches in an environment less costly and more forgiving than space. The results from the RATS tests allow selection of potential operational approaches to planetary surface exploration prior to making commitments to specific flight and mission hardware development. In previous RATS operations, the Science Support Room has operated largely in an advisory role, an approach that was driven by the need to provide a loose science mission framework that would underpin the engineering tests. However, the extensive nature of the traverse operations for 2010 expanded the role of the science operations and tested specific operational approaches. Science mission operations approaches from the Apollo and Mars-Phoenix missions were merged to become the baseline for this test. Six days of traverse operations were conducted during each week of the 2-week test, with three traverse days each week conducted with voice and data communications continuously available, and three traverse days conducted with only two 1-hour communications periods per day. Within this framework, the team evaluated integrated science operations management using real-time, tactical science operations to oversee daily crew activities, and strategic level evaluations of science data and daily traverse results during a post-traverse planning shift. During continuous communications, both tactical and strategic teams were employed. On days when communications were reduced to only two communications periods per day, only a strategic team was employed. The Science Operations Team found that, if communications are good and down-linking of science data is ensured, high quality science returns is possible regardless of communications. What is absent from reduced communications is the scientific interaction between the crew on the planet and the scientists on the ground. These scientific interactions were a critical part of the science process and significantly improved mission science return over reduced communications conditions. The test also showed that the quality of science return is not measurable by simple numerical quantities but is, in fact, based on strongly non-quantifiable factors, such as the interactions between the crew and the Science Operations Teams. Although the metric evaluation data suggested some trends, there was not sufficient granularity in the data or specificity in the metrics to allow those trends to be understood on numerical data alone.},
  keywords = {Analog testing,Metric evaluation,Planetary science,Planetary surface operations,Science operations},
  file = {/home/cameron/Zotero/storage/J7CTU6UY/Eppler et al. - 2013 - Desert Research and Technology Studies (DRATS) 2010 science operations Operational approaches and lessons learned.pdf}
}

@article{Ericksen2004,
  title = {Right from the Start: {{Exploring}} the Effects of Early Team Events on Subsequent Project Team Development and Performance},
  author = {Ericksen, Jeff and Dyer, Lee},
  year = {2004},
  journal = {Administrative Science Quarterly},
  volume = {49},
  number = {3},
  pages = {438--471},
  issn = {00018392},
  doi = {10.2307/4131442},
  abstract = {This study examines if high- and low-performing project teams differ with respect to how they are mobilized and launched and the effects of their mobilization and launch activities and outputs on subsequent team progress and performance. Comparisons of three high- and three low-performing teams drawn from five major corporations showed that the high performers mobilized relatively quickly, used comprehensive rather than limited mobilization strategies, and conducted participatory rather than programmed launch meetings. This combination of activities produced a constellation of salutary outputs: more time for the teams to do their work, team members with essential task-related competencies and sufficient time to contribute to their projects, and complete rather than partial performance strategies. In turn, the three salutary outputs formed a constellation of key inner resources that propelled the high-performing teams on a virtuous path of reinforcing activities and outputs that, despite difficulties, ultimately led to success, whereas the absence of one or more of these resources led the low-performing teams down a vacuous path of accumulating confusion and inactivity from which they never recovered. \textcopyright{} 2004 by Johnson Graduate School, Cornell University.},
  file = {/home/cameron/Zotero/storage/LPEK39HR/Ericksen, Dyer - 2004 - Right from the start Exploring the effects of early team events on subsequent project team development and perfo.pdf}
}

@article{Evans2019,
  title = {An Interdisciplinary Survey of Network Similarity Methods},
  author = {Evans, Emily and Graham, Marissa},
  year = {2019},
  eprint = {1905.06457},
  eprinttype = {arxiv},
  pages = {1--41},
  abstract = {Comparative graph and network analysis play an important role in both systems biology and pattern recognition, but existing surveys on the topic have historically ignored or underserved one or the other of these fields. We present an integrative introduction to the key objectives and methods of graph and network comparison in each field, with the intent of remaining accessible to relative novices in order to mitigate the barrier to interdisciplinary idea crossover. To guide our investigation, and to quantitatively justify our assertions about what the key objectives and methods of each field are, we have constructed a citation network containing 5,793 vertices from the full reference lists of over two hundred relevant papers, which we collected by searching Google Scholar for ten different network comparison-related search terms. We investigate its basic statistics and community structure, and frame our presentation around the papers found to have high importance according to five different standard centrality measures.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/W4P2HXTA/Evans, Graham - 2019 - An interdisciplinary survey of network similarity methods.pdf}
}

@misc{EVAPhysiology,
  title = {Recommended {{Metabolic Rate Calculations}} \& {{Sensors White Paper}}},
  author = {EVA Physiology, Systems \& Performance (EPSP) Project},
  publisher = {{EVA Physiology, Systems \& Performance (EPSP) Project}},
  abstract = {This white paper represents the EVA Physiology, Systems, \& Performance (EPSP) project recommendations for methods of metabolic rate calculations to support Power, Communication, Avionics, and Informatics (PCAI) Subsystem development of hardware and/or requirements. The PCAI Subsystem is part of the EVA Systems Project Office (ESPO) of the Constellation (Cx) Program. While these recommendations are preliminary due to the limited fidelity of the current Cx Program architecture and the operations concepts that will require physiological monitoring, they represent the best recommendations of the EPSP project to date.},
  file = {/home/cameron/Zotero/storage/YL43Q75L/Met_rate_white_paper_final.pdf}
}

@techreport{evaphysiology,
  title = {Recommended {{Metabolic Rate Calculations}} \& {{Sensors}}},
  author = {EVA Physiology, Systems \& Performance (EPSP) Project},
  pages = {21},
  institution = {{National Aeronautics and Space Administration}},
  abstract = {This white paper represents the EVA Physiology, Systems, \& Performance (EPSP) project recommendations for methods of metabolic rate calculations to support Power, Communication, Avionics, and Informatics (PCAI) Subsystem development of hardware and/or requirements. The PCAI Subsystem is part of the EVA Systems Project Office (ESPO) of the Constellation (Cx) Program. While these recommendations are preliminary due to the limited fidelity of the current Cx Program architecture and the operations concepts that will require physiological monitoring, they represent the best recommendations of the EPSP project to date.},
  file = {/home/cameron/Zotero/storage/HGHHR4BF/Met_rate_white_paper_final.pdf}
}

@inproceedings{f.j.calefiiih.e.genglt.solimans.p.abercrombie2017,
  title = {{{MMGIS}}: {{A MULTI-MISSION GEOGRAPHIC INFORMATION SYSTEM FOR INSITU MARS OPERATIONS}}},
  booktitle = {Lunar and {{Planetary Science XLVIII}}},
  author = {F. J. Calef III, H. E. Gengl, T. Soliman, S. P. Abercrombie, M. W. Powell},
  year = {2017},
  pages = {2--3},
  file = {/home/cameron/Zotero/storage/V6THFW47/2541.pdf}
}

@article{Fang2014,
  title = {Chance-Constrained {{Probabilistic Simple}} Temporal Problems},
  author = {Fang, Cheng and Yu, Peng and Williams, Brian C.},
  year = {2014},
  journal = {Proceedings of the National Conference on Artificial Intelligence},
  volume = {3},
  pages = {2264--2270},
  abstract = {Scheduling under uncertainty is essential to many autonomous systems and logistics tasks. Probabilistic methods for solving temporal problems exist which quantify and attempt to minimize the probability of schedule failure. These methods are overly conservative, resulting in a loss in schedule utility. Chance constrained formalism address over-conservatism by imposing bounds on risk, while maximizing utility subject to these risk bounds. In this paper we present the probabilistic Simple Temporal Network (pSTN), a probabilistic formalism for representing temporal problems with bounded risk and a utility over event timing. We introduce a constrained optimisation algorithm for pSTNs that achieves compactness and efficiency through a problem encoding in terms of a parameterised STNU and its reformulation as a parameterised STN. We demonstrate through a car sharing application that our chance-constrained approach runs in the same time as the previous probabilistic approach, yields solutions with utility improvements of at least 5\% over previous arts, while guaranteeing operation within the specified risk bound.},
  isbn = {9781577356790},
  file = {/home/cameron/Zotero/storage/HABNTG4I/aaai14cf.pdf}
}

@techreport{fayyad1996,
  title = {From {{Data Mining}} to {{Knowledge Discovery}} in {{Databases}}) (\textcopyright{} {{AAAI}})},
  author = {Fayyad, Usama and {Piatetsky-Shapiro}, Gregory and Smyth, Padhraic},
  year = {1996},
  volume = {17},
  abstract = {s Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field. A cross a wide variety of fields, data are being collected and accumulated at a dramatic pace. There is an urgent need for a new generation of computational theories and tools to assist humans in extracting useful information (knowledge) from the rapidly growing volumes of digital data. These theories and tools are the subject of the emerging field of knowledge discovery in databases (KDD). At an abstract level, the KDD field is concerned with the development of methods and techniques for making sense of data. The basic problem addressed by the KDD process is one of mapping low-level data (which are typically too voluminous to understand and digest easily) into other forms that might be more compact (for example, a short report), more abstract (for example, a descriptive approximation or model of the process that generated the data), or more useful (for example , a predictive model for estimating the value of future cases). At the core of the process is the application of specific data-mining methods for pattern discovery and extraction. 1 This article begins by discussing the historical context of KDD and data mining and their intersection with other related fields. A brief summary of recent KDD real-world applications is provided. Definitions of KDD and data mining are provided, and the general mul-tistep KDD process is outlined. This multistep process has the application of data-mining algorithms as one particular step in the process. The data-mining step is discussed in more detail in the context of specific data-mining algorithms and their application. Real-world practical application issues are also outlined. Finally, the article enumerates challenges for future research and development and in particular discusses potential opportunities for AI technology in KDD systems. Why Do We Need KDD? The traditional method of turning data into knowledge relies on manual analysis and interpretation. For example, in the health-care industry, it is common for specialists to periodically analyze current trends and changes in health-care data, say, on a quarterly basis. The specialists then provide a report detailing the analysis to the sponsoring health-care organization ; this report becomes the basis for future decision making and planning for health-care management. In a totally different type of application, planetary geologists sift through remotely sensed images of planets and asteroids, carefully locating and cataloging such geologic objects of interest as impact craters. Be it science, marketing, finance, health care, retail, or any other field, the classical approach to data analysis relies fundamentally on one or more analysts becoming Articles},
  file = {/home/cameron/Zotero/storage/3R5UVTKP/1230-Article Text-1227-1-10-20080129.pdf}
}

@phdthesis{Felker2012,
  title = {Using {{Optimization}} to {{Improve NASA Extravehicular Activity Planning}}},
  author = {Felker, Paul W},
  year = {2012},
  abstract = {Extravehicular Activity (EVA) is a specialized function performed during spaceflights in which two or more astronauts don spacesuits to perform tasks on the exterior of their spacecraft. An extensive and iterative planning process is required to prepare for each highly choreographed EVA operation. The current planning process relies heavily upon time-consuming heuristic approaches by subject matter experts to essentially "hand-build" each EVA plan. This research develops the EVA Planning Model (EPM), a linear, mixed-integer program intended as a proof- of-concept demonstration for employing formal mathematical optimization techniques to EVA planning. The EPM is thoroughly tested to verify that it functions as intended and is evaluated by expert EVA planners using actual task information. We find that the EPM proves the concept that formal mathematical optimization can be used to aid in subject matter experts in EVA development and planning. It is particularly useful in allowing the evaluation of alternative planning inputs and thorough assessment of EVA plan impacts resulting from external changes.},
  school = {Naval Postgraduate School},
  file = {/home/cameron/Zotero/storage/XXPGV59J/Felker2012_Using_Optimization_to_Improve_NASA.pdf}
}

@article{Feng2018,
  title = {Learning to {{Collaborate}}: {{Multi-Scenario Ranking}} via {{Multi-Agent Reinforcement Learning}}},
  author = {Feng, Jun and Li, Heng and Huang, Minlie and Liu, Shichen and Ou, Wenwu and Wang, Zhirong and Zhu, Xiaoyan},
  year = {2018},
  eprint = {1809.06260},
  eprinttype = {arxiv},
  abstract = {Ranking is a fundamental and widely studied problem in scenarios such as search, advertising, and recommendation. However, joint optimization for multi-scenario ranking, which aims to improve the overall performance of several ranking strategies in different scenarios, is rather untouched. Separately optimizing each individual strategy has two limitations. The first one is lack of collaboration between scenarios meaning that each strategy maximizes its own objective but ignores the goals of other strategies, leading to a sub-optimal overall performance. The second limitation is the inability of modeling the correlation between scenarios meaning that independent optimization in one scenario only uses its own user data but ignores the context in other scenarios. In this paper, we formulate multi-scenario ranking as a fully cooperative, partially observable, multi-agent sequential decision problem. We propose a novel model named Multi-Agent Recurrent Deterministic Policy Gradient (MA-RDPG) which has a communication component for passing messages, several private actors (agents) for making actions for ranking, and a centralized critic for evaluating the overall performance of the co-working actors. Each scenario is treated as an agent (actor). Agents collaborate with each other by sharing a global action-value function (the critic) and passing messages that encodes historical information across scenarios. The model is evaluated with online settings on a large E-commerce platform. Results show that the proposed model exhibits significant improvements against baselines in terms of the overall performance.},
  archiveprefix = {arXiv},
  isbn = {9781450356398},
  keywords = {joint,learning to rank,multi-agent learning,reinforcement learning},
  file = {/home/cameron/Zotero/storage/M9RPKSLU/1809.06260.pdf}
}

@article{Fernandez-Gonzalez2015,
  title = {Mixed Discrete-Continuous Heuristic Generative Planning Based on Flow Tubes},
  author = {{Fern{\'a}ndez-Gonz{\'a}lez}, Enrique and Karpas, Erez and Williams, Brian C.},
  year = {2015},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  volume = {2015-Janua},
  pages = {1565--1572},
  issn = {10450823},
  abstract = {Nowadays, robots are programmed with a mix of discrete and continuous low level behaviors by experts in a very time consuming and expensive process. Existing automated planning approaches are either based on hybrid model predictive control techniques, which do not scale well due to time discretization, or temporal planners, which sacrifice plan expressivity by only supporting discretized fixed rates of change in continuous effects. We introduce Scotty, a mixed discrete-continuous generative planner that finds the middle ground between these two. Scotty can reason with linear time evolving effects whose behaviors can be modified by bounded control variables, with no discretization involved. Our planner exploits the expressivity of flow tubes, which compactly encapsulate continuous effects, and the performance of heuristic forward search. The generated solution plans are better suited for robust execution, as executives can use the flexibility in both time and continuous control variables to react to disturbances.},
  isbn = {9781577357384},
  file = {/home/cameron/Zotero/storage/5AZY6DQK/IJCAI15_FernandezKarpasWilliams_scotty.pdf}
}

@techreport{Fernandez-Gonzalez2018,
  title = {{{ScottyActivity}}: {{Mixed Discrete-Continuous Planning}} with {{Convex Optimization}}},
  author = {{Fern{\'a}ndez-Gonz{\'a}lez}, Enrique and Williams, Brian C. and Karpas, Erez},
  year = {2018},
  journal = {Journal of Artificial Intelligence Research},
  volume = {62},
  pages = {579--664},
  abstract = {The state of the art practice in robotics planning is to script behaviors manually, where each behavior is typically generated using trajectory optimization. However, in order for robots to be able to act robustly and adapt to novel situations, they need to plan these activity sequences autonomously. Since the conditions and effects of these behaviors are tightly coupled through time, state and control variables, many problems require that the tasks of activity planning and trajectory optimization are considered together. There are two key issues underlying effective hybrid activity and trajectory planning: the sufficiently accurate modeling of robot dynamics and the capability of planning over long horizons. Hybrid activity and trajectory planners that employ mixed integer programming within a discrete time formulation are able to accurately model complex dynamics for robot vehicles, but are often restricted to relatively short horizons. On the other hand, current hybrid activity planners that employ continuous time formulations can handle longer horizons but they only allow actions to have continuous effects with constant rate of change, and restrict the allowed state constraints to linear inequalities. This is insufficient for many robotic applications and it greatly limits the expressivity of the problems that these approaches can solve. In this work we present the ScottyActivity planner, that is able to generate practical hybrid activity and motion plans over long horizons by employing recent methods in convex optimization combined with methods for planning with relaxed plan graphs and heuristic forward search. Unlike other continuous time planners, ScottyActivity can solve a broad class of robotic planning problems by supporting convex quadratic constraints on state variables and control variables that are jointly constrained and that affect multiple state variables simultaneously. In order to support planning over long horizons, ScottyActivity does not resort to time, state or control variable discretization. While straightforward formulations of consistency checks are not convex and do not scale, we present an efficient convex formulation, in the form of a Second Order Cone Program (SOCP), that is very fast to solve. We also introduce several new realistic domains that demonstrate the capabilities and scalability of our approach, and their simplified linear versions, that we use to compare with other state of the art planners. This work demonstrates the power of integrating advanced convex optimization techniques with discrete search methods and paves the way for extensions dealing with non-convex disjoint constraints, such as obstacle avoidance.},
  file = {/home/cameron/Zotero/storage/HEPPMNCP/ScottyActivity_JAIR_FernandezKarpasWilliams_2018.pdf}
}

@phdthesis{FernandezGonzalez2018,
  title = {Generative {{Multi-Robot Task}} and {{Motion Planning Over Long Horizons}}},
  author = {Fern{\'a}ndez Gonz{\'a}lez, Enrique},
  year = {2018},
  abstract = {The state of the art practice in robotics planning is to script behaviors manually, where each behavior is typically precomputed in advance. However, in order for robots to be able to act robustly and adapt to novel situations, they need to be able to plan sequences of behaviors and activities autonomously. Since the conditions and effects of these behaviors are tightly coupled through time, state and control variables, many problems require that the tasks of activity planning and trajectory optimization are considered together. There are two key issues underlying effective hybrid activity and trajectory planning: the sufficiently accurate modeling of robot dynamics and the capability of planning over long horizons. Hybrid activity and trajectory planners that employ mixed integer programming within a discrete time formulation are able to accurately model complex dynamics for robot vehicles, but are often restricted to relatively short horizons. On the other hand, current hybrid activity planners that employ continuous time formulations can handle longer horizons but they only allow actions to have continuous effects with constant rate of change, and restrict the allowed state constraints to linear inequalities. This greatly limits the expressivity of the problems that these approaches can solve. In this work we present Scotty, a planning system for hybrid activity and trajectory planning problems. Unlike other continuous time planners, Scotty can solve a broad class of expressive robotic planning problems by supporting convex quadratic constraints on state variables and control variables that are jointly constrained and that affect multiple state variables simultaneously. In order to efficiently generate practical plans for coordinated mobile robots over long horizons, our approach employs recent methods in convex optimization combined with methods for planning with relaxed planning graphs and heuristic forward search. The contributions of this thesis are threefold. First, we introduce a convex, goal-directed scheduling and trajectory planning problem. To solve this problem, we present the ScottyConvexPath planner, which reformulates the problem as a Second Order Cone Program (SOCP). Our formulation allows us to efficiently compute robot trajectories with first order dynamics over long horizons. While straightforward formulations are not convex, we present a convex model that does not require state, control or time discretization. Second, we introduce the ScottyActivity planner, a state of the art hybrid activity and trajectory planner that interleaves heuristic forward search with delete relaxations and consistency checks using our convex model. Finally, we present ScottyPath, a qualitative state plan planner that computes control and obstacle-free state trajectories for robots in order to satisfy the temporally extended goals and constraints that ScottyActivity imposes. ScottyPath finds obstacle-free paths in which all robots are guaranteed to always remain within obstacle-free safe regions, which are computed in advance. We introduce several new robotic planning domains, that we use to evaluate the scalability of our planning system and compare the performance of our approach against other prior methods. Our results show that ScottyActivity performs similarly to other state of the art heuristic forward search activity planners, while solving much more expressive robotic planning problems. On the other hand, ScottyPath can generate obstacle-free paths where robots are contained in obstacle-free convex regions more than two orders of magnitude faster than alternative mixed-integer approaches.},
  school = {Massachusetts Institute of Technology},
  file = {/home/cameron/Zotero/storage/5IC9T2DU/m-api-0aba6a64-dd9a-ed01-1250-6315262af51d.pdf}
}

@article{Fikes1971,
  title = {Strips: {{A}} New Approach to the Application of Theorem Proving to Problem Solving},
  author = {Fikes, Richard E. and Nilsson, Nils J.},
  year = {1971},
  journal = {Artificial Intelligence},
  volume = {2},
  number = {3-4},
  pages = {189--208},
  issn = {00043702},
  doi = {10.1016/0004-3702(71)90010-5},
  abstract = {We describe a new problem solver called STRIPS that attempts to find a sequence of operators in a space of world models to transform a given initial world model in which a given goal formula can be proven to be true. STRIPS represents a world model as an arbitrary collection in first-order predicate calculus formulas and is designed to work with models consisting of large numbers of formula. It employs a resolution theorem prover to answer questions of particular models and uses means-ends analysis to guide it to the desired goal-satisfying model. \textcopyright{} 1971.},
  keywords = {heuristic search,Problem solving,robot planning,theorem proving},
  file = {/home/cameron/Zotero/storage/S4K473A5/Fikes, Nilsson - 1971 - Strips A new approach to the application of theorem proving to problem solving.pdf}
}

@article{Fischler1981,
  title = {Random {{Sample Paradigm}} for {{Model Consensus}}: {{A Apphcatlons}} to {{Image Fitting}} with {{Analysis}} and {{Automated Cartography}}},
  author = {Fischler, Martin A and Bolles, Robert C},
  year = {1981},
  journal = {Graphics and Image Processing},
  volume = {24},
  number = {6},
  pages = {381--395},
  issn = {00010782},
  abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced. RANSAC is capable of interpreting/ smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing Permission},
  keywords = {0,1,2,3,5,60,61,71,8,analysis,and phrases,automated cartography,camera calibration,cr categories,determination,image matching,location,model fitting,scene},
  file = {/home/cameron/Zotero/storage/D5YZHAKE/358669.358692.pdf}
}

@article{Fox2003,
  title = {{{PDDL2}}.1: {{An}} Extension to {{PDDL}} for Expressing Temporal Planning Domains},
  author = {Fox, Maria and Long, Derek},
  year = {2003},
  journal = {Journal of Artificial Intelligence Research},
  volume = {20},
  eprint = {1106.4561},
  eprinttype = {arxiv},
  pages = {61--124},
  issn = {10769757},
  doi = {10.1613/jair.1129},
  abstract = {In recent years research in the planning community has moved increasingly towards application of planners to realistic problems involving both time and many types of resources. For example, interest in planning demonstrated by the space research community has inspired work in observation scheduling, planetary rover exploration and spacecraft control domains. Other temporal and resource-intensive domains including logistics planning, plant control and manufacturing have also helped to focus the community on the modelling and reasoning issues that must be confronted to make planning technology meet the challenges of application. The International Planning Competitions have acted as an important motivating force behind the progress that has been made in planning since 1998. The third competition (held in 2002) set the planning community the challenge of handling time and numeric resources. This necessitated the development of a modelling language capable of expressing temporal and numeric properties of planning domains. In this paper we describe the language, PDDL2.1, that was used in the competition. We describe the syntax of the language, its formal semantics and the validation of concurrent plans. We observe that PDDL2.1 has considerable modelling power -exceeding the capabilities of current planning technology -and presents a number of important challenges to the research community. \textcopyright{} 2003 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/KXZV2M3C/lec-07-reading-3.pdf}
}

@article{Fox2003a,
  title = {{{PDDL2}}.1: {{An}} Extension to {{PDDL}} for Expressing Temporal Planning Domains},
  author = {Fox, Maria and Long, Derek},
  year = {2003},
  journal = {Journal of Artificial Intelligence Research},
  volume = {20},
  eprint = {1106.4561},
  eprinttype = {arxiv},
  pages = {61--124},
  issn = {10769757},
  doi = {10.1613/jair.1129},
  abstract = {In recent years research in the planning community has moved increasingly towards application of planners to realistic problems involving both time and many types of resources. For example, interest in planning demonstrated by the space research community has inspired work in observation scheduling, planetary rover exploration and spacecraft control domains. Other temporal and resource-intensive domains including logistics planning, plant control and manufacturing have also helped to focus the community on the modelling and reasoning issues that must be confronted to make planning technology meet the challenges of application. The International Planning Competitions have acted as an important motivating force behind the progress that has been made in planning since 1998. The third competition (held in 2002) set the planning community the challenge of handling time and numeric resources. This necessitated the development of a modelling language capable of expressing temporal and numeric properties of planning domains. In this paper we describe the language, PDDL2.1, that was used in the competition. We describe the syntax of the language, its formal semantics and the validation of concurrent plans. We observe that PDDL2.1 has considerable modelling power -exceeding the capabilities of current planning technology -and presents a number of important challenges to the research community. \textcopyright{} 2003 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/9MPXKCCB/lec-07-reading-3.pdf}
}

@article{Fox2006,
  title = {Modelling Mixed Discrete-Continuous Domains for Planning},
  author = {Fox, Maria and Long, Derek},
  year = {2006},
  journal = {Journal of Artificial Intelligence Research},
  volume = {27},
  pages = {235--297},
  issn = {10769757},
  doi = {10.1613/jair.2044},
  abstract = {In this paper we present PDDL+, a planning domain description language for modelling mixed discrete-continuous planning domains. We describe the syntax and modelling style of PDDL+, showing that the language makes convenient the modelling of complex time-dependent effects. We provide a formal semantics for PDDL+ by mapping planning instances into constructs of hybrid automata. Using the syntax of HAs as our semantic model we construct a semantic mapping to labelled transition systems to complete the formal interpretation of PDDL+ planning instances. An advantage of building a mapping from PDDL+ to HA theory is that it forms a bridge between the Planning and Real Time Systems research communities. One consequence is that we can expect to make use of some of the theoretical properties of HAs. For example, for a restricted class of HAs the Reachability problem (which is equivalent to Plan Existence) is decidable. PDDL+ provides an alternative to the continuous durative action model of PDDL2.1, adding a more flexible and robust model of time-dependent behaviour. \textcopyright{} 2006 AI Access Foundation. All rights reserved.},
  file = {/home/cameron/Zotero/storage/EJFC9YCJ/Fox, Long - 2006 - Modelling mixed discrete-continuous domains for planning.pdf}
}

@article{Francis2017,
  title = {{{AEGIS Autonomous Targeting}} for {{ChemCam}} on {{Mars Science Laboratory}}: {{Deployment}} and {{Results}} of {{Initial Science Team Use}}},
  author = {Francis, R. and Estlin, T. and Doran, G. and Johnstone, S. and Gaines, D. and Verma, V. and Burl, M. and Frydenvang, J. and Monta{\~n}o, S. and Wiens, R. C. and Schaffer, S. and Gasnault, O. and DeFlores, L. and Blaney, D. and Bornstein, B.},
  year = {2017},
  journal = {Science Robotics},
  volume = {2},
  number = {7},
  pages = {12},
  issn = {24709476},
  doi = {10.1126/scirobotics.aan4582},
  abstract = {Limitations on interplanetary communications create operations latencies and slow progress in planetary surface missions, with particular challenges to narrow\textendash field-of-view science instruments requiring precise targeting. The AEGIS (Autonomous Exploration for Gathering Increased Science) autonomous targeting system has been in routine use on NASA's Curiosity Mars rover since May 2016, selecting targets for the ChemCam remote geochemical spectrometer instrument. AEGIS operates in two modes; in autonomous target selection, it identifies geological targets in images from the rover's navigation cameras, choosing for itself targets that match the parameters specified by mission scientists the most, and immediately measures them with ChemCam, without Earth in the loop. In autonomous pointing refinement, the system corrects small pointing errors on the order of a few milliradians in observations targeted by operators on Earth, allowing very small features to be observed reliably on the first attempt. AEGIS consistently recognizes and selects the geological materials requested of it, parsing and interpreting geological scenes in tens to hundreds of seconds with very limited computing resources. Performance in autonomously selecting the most desired target material over the last 2.5 kilometers of driving into previously unexplored terrain exceeds 93\% (where \textasciitilde 24\% is expected without intelligent targeting), and all observations resulted in a successful geochemical observation. The system has substantially reduced lost time on the mission and markedly increased the pace of data collection with ChemCam. AEGIS autonomy has rapidly been adopted as an exploration tool by the mission scientists and has influenced their strategy for exploring the rover's environment.},
  file = {/home/cameron/Zotero/storage/5QS77DCD/eaan4582.full.pdf}
}

@article{Frankland2017,
  title = {Traditional versus Open Access Scholarly Journal Publishing: {{An}} Economic Perspective},
  author = {Frankland, Julia and Ray, Margaret A.},
  year = {2017},
  journal = {Journal of Scholarly Publishing},
  volume = {49},
  number = {1},
  pages = {5--25},
  issn = {17101166},
  doi = {10.3138/jsp.49.1.5},
  abstract = {The debate surrounding open access journal publishing is part of a broader debate related to the electronic dissemination of information. Compared to print journals, electronic journals have lower publishing costs and allow for expanded access to scholarly research. However, open access publishing introduces an added cost of evaluating an ever-increasing number of published sources and the potential for misinformation. This paper analyses the traditional and open access scholarly publishing models from an economic perspective. Analysing the alternative market structures of these models can help to identify strategies to maximize net benefits in the scholarly publishing market.},
  keywords = {Economics,Efficiency,Market Model,Open Access Publishing,Technological Change},
  file = {/home/cameron/Zotero/storage/SRSD3QJE/R04- Frankland, Ray, Open Scholarship Econ.pdf}
}

@article{Frehse2011,
  title = {{{SpaceEx}}: {{Scalable}} Verification of Hybrid Systems},
  author = {Frehse, Goran and Le Guernic, Colas and Donz{\'e}, Alexandre and Cotton, Scott and Ray, Rajarshi and Lebeltel, Olivier and Ripado, Rodolfo and Girard, Antoine and Dang, Thao and Maler, Oded},
  year = {2011},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {6806 LNCS},
  pages = {379--395},
  issn = {16113349},
  doi = {10.1007/978-3-642-22110-1_30},
  abstract = {We present a scalable reachability algorithm for hybrid systems with piecewise affine, non-deterministic dynamics. It combines polyhedra and support function representations of continuous sets to compute an over-approximation of the reachable states. The algorithm improves over previous work by using variable time steps to guarantee a given local error bound. In addition, we propose an improved approximation model, which drastically improves the accuracy of the algorithm. The algorithm is implemented as part of SpaceEx, a new verification platform for hybrid systems, available at spaceex.imag.fr. Experimental results of full fixed-point computations with hybrid systems with more than 100 variables illustrate the scalability of the approach. \textcopyright{} 2011 Springer-Verlag.},
  isbn = {9783642221095},
  file = {/home/cameron/Zotero/storage/WRE3LLQK/Frehse et al. - 2011 - SpaceEx Scalable verification of hybrid systems.pdf}
}

@article{Gao,
  title = {Satisfiability {{Modulo ODEs SMT}} over the {{Reals}} with {{ODEs}}},
  author = {Gao, Sicun and Clarke, Edmund},
  pages = {1--18},
  file = {/home/cameron/Zotero/storage/2BZNIALU/Gao, Clarke - Unknown - Satisfiability Modulo ODEs SMT over the Reals with ODEs.pdf}
}

@article{Gao2012,
  title = {Delta-Decidability over the Reals},
  author = {Gao, Sicun and Avigad, Jeremy and Clarke, Edmund M.},
  year = {2012},
  journal = {Proceedings of the 2012 27th Annual ACM/IEEE Symposium on Logic in Computer Science, LICS 2012},
  volume = {1041377},
  number = {1041377},
  eprint = {1204.6671},
  eprinttype = {arxiv},
  pages = {305--314},
  doi = {10.1109/LICS.2012.41},
  abstract = {Given any collection F of computable functions over the reals, we show that there exists an algorithm that, given any sentence A containing only bounded quantifiers and functions in F, and any positive rational number delta, decides either "A is true", or "a delta-strengthening of A is false". Moreover, if F can be computed in complexity class C, then under mild assumptions, this "delta-decision problem" for bounded Sigma-k-sentences resides in Sigma-k(C). The results stand in sharp contrast to the well-known undecidability of the general first-order theories with these functions, and serve as a theoretical basis for the use of numerical methods in decision procedures for formulas over the reals. \textcopyright{} 2012 IEEE.},
  archiveprefix = {arXiv},
  isbn = {9780769547695},
  keywords = {Computable Analysis,Decision Procedures,First-order Theories over the Reals},
  file = {/home/cameron/Zotero/storage/QL96N74P/Gao, Avigad, Clarke - 2012 - Delta-decidability over the reals.pdf}
}

@article{Gao2012a,
  title = {{$\delta$}-{{Complete}} Decision Procedures for Satisfiability over the Reals},
  author = {Gao, Sicun and Avigad, Jeremy and Clarke, Edmund M.},
  year = {2012},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {7364 LNAI},
  number = {1041377},
  eprint = {1204.3513},
  eprinttype = {arxiv},
  pages = {286--300},
  issn = {03029743},
  doi = {10.1007/978-3-642-31365-3_23},
  abstract = {We introduce the notion of "{$\delta$}-complete decision procedures" for solving SMT problems over the real numbers, with the aim of handling a wide range of nonlinear functions including transcendental functions and solutions of Lipschitz-continuous ODEs. Given an SMT problem {$\varphi$} and a positive rational number {$\delta$}, a {$\delta$}-complete decision procedure determines either that {$\varphi$} is unsatisfiable, or that the "{$\delta$}-weakening" of {$\varphi$} is satisfiable. Here, the {$\delta$}-weakening of {$\varphi$} is a variant of {$\varphi$} that allows {$\delta$}-bounded numerical perturbations on {$\varphi$}. We establish the existence and complexity of {$\delta$}-complete decision procedures for bounded SMT over reals with functions mentioned above. We propose to use {$\delta$}-completeness as an ideal requirement for numerically-driven decision procedures. As a concrete example, we formally analyze the DPLL{$\langle$}ICP{$\rangle$} framework, which integrates Interval Constraint Propagation in DPLL(T), and establish necessary and sufficient conditions for its {$\delta$}-completeness. We discuss practical applications of {$\delta$}-complete decision procedures for correctness-critical applications including formal verification and theorem proving. \textcopyright{} 2012 Springer-Verlag.},
  archiveprefix = {arXiv},
  isbn = {9783642313646},
  file = {/home/cameron/Zotero/storage/W8VEGYNE/Gao, Avigad, Clarke - 2012 - Î´-Complete decision procedures for satisfiability over the reals.pdf}
}

@article{Gao2020,
  title = {Robotic Table Tennis with Model-Free Reinforcement Learning},
  author = {Gao, Wenbo and Graesser, Laura and Choromanski, Krzysztof and Song, Xingyou and Lazic, Nevena and Sanketi, Pannag and Sindhwani, Vikas and Jaitly, Navdeep},
  year = {2020},
  journal = {IEEE International Conference on Intelligent Robots and Systems},
  eprint = {2003.14398},
  eprinttype = {arxiv},
  pages = {5556--5563},
  issn = {21530866},
  doi = {10.1109/IROS45743.2020.9341191},
  abstract = {We propose a model-free algorithm for learning efficient policies capable of returning table tennis balls by controlling robot joints at a rate of 100Hz. We demonstrate that evolutionary search (ES) methods acting on CNN-based policy architectures for non-visual inputs and convolving across time learn compact controllers leading to smooth motions. Furthermore, we show that with appropriately tuned curriculum learning on the task and rewards, policies are capable of developing multi-modal styles, specifically forehand and backhand stroke, whilst achieving 80\% return rate on a wide range of ball throws. We observe that multi-modality does not require any architectural priors, such as multi-head architectures or hierarchical policies.},
  archiveprefix = {arXiv},
  isbn = {9781728162126},
  file = {/home/cameron/Zotero/storage/VLDJV28L/Gao et al. - 2020 - Robotic table tennis with model-free reinforcement learning.pdf}
}

@article{Gao2020a,
  title = {Dynamic {{Control}} of {{Probabilistic Simple Temporal Networks}}},
  author = {Gao, Michael and Popowski, Lindsay and Boerkoel, James C.},
  year = {2020},
  journal = {AAAI 2020 - 34th AAAI Conference on Artificial Intelligence},
  pages = {9851--9858},
  issn = {2159-5399},
  doi = {10.1609/aaai.v34i06.6538},
  abstract = {The controllability of a temporal network is defined as an agent s ability to navigate around the uncertainty in its schedule and is well-studied for certain networks of temporal constraints. However, many interesting real-world problems can be better represented as Probabilistic Simple Temporal Networks (PSTNs) in which the uncertain durations are represented using potentially-unbounded probability density functions. This can make it inherently impossible to control for all eventualities. In this paper, we propose two new dynamic controllability algorithms that attempt to maximize the likelihood of successfully executing a schedule within a PSTN. The first approach, which we call MIN-LOSS DC, finds a dynamic scheduling strategy that minimizes loss of control by using a conflict-directed search to decide where to sacrifice the control in a way that optimizes overall success. The second approach, which we call MAX-GAIN DC, works in the other direction: it finds a dynamically controllable schedule and then attempts to progressively strengthen it by capturing additional uncertainty. Our approaches are the first known that work by finding maximally dynamically controllable schedules. We empirically compare our approaches against two existing PSTN offline dispatch approaches and one online approach and show that our MIN-LOSS DC algorithm outperforms the others in terms of maximizing execution success while maintaining competitive runtimes.},
  isbn = {9781577358350},
  keywords = {Planning; Routing; and Scheduling},
  file = {/home/cameron/Zotero/storage/6LVPTA3A/Gao, Popowski, Boerkoel - 2020 - Dynamic Control of Probabilistic Simple Temporal Networks.pdf}
}

@article{Garrett2021,
  title = {Integrated {{Task}} and {{Motion Planning}}},
  author = {Garrett, Caelan Reed and Chitnis, Rohan and Holladay, Rachel and Kim, Beomjoon and Silver, Tom and Kaelbling, Leslie Pack and {Lozano-P{\'e}rez}, Tom{\'a}s},
  year = {2021},
  journal = {Annual Review of Control, Robotics, and Autonomous Systems},
  volume = {4},
  number = {1},
  eprint = {2010.01083},
  eprinttype = {arxiv},
  pages = {265--293},
  issn = {2573-5144},
  doi = {10.1146/annurev-control-091420-084139},
  abstract = {The problem of planning for a robot that operates in environments containing a large number of objects, taking actions to move itself through the world as well as to change the state of the objects, is known as task and motion planning (TAMP). TAMP problems contain elements of discrete task planning, discrete\textendash continuous mathematical programming, and continuous motion planning and thus cannot be effectively addressed by any of these fields directly. In this article, we define a class of TAMP problems and survey algorithms for solving them, characterizing the solution methods in terms of their strategies for solving the continuous-space subproblems and their techniques for integrating the discrete and continuous components of the search.},
  archiveprefix = {arXiv},
  keywords = {automated planning,manipulation planning,motion,planning,robotics,task and motion planning},
  file = {/home/cameron/Zotero/storage/SKZGUC5R/Garrett et al. - 2021 - Integrated Task and Motion Planning.pdf}
}

@inproceedings{Gent1993,
  title = {Towards an {{Understanding}} of {{Hill-Climbing Procedures}} for {{SAT}}},
  booktitle = {{{AAAI}}},
  author = {Gent, Ian P and Walsh, Toby},
  year = {1993},
  pages = {28--33},
  abstract = {Recently several local hill-climbing procedures for propositional satisfiability have been proposed which are able to solve large and difficult problems beyond the reach of conventional algorithms like Davis-Putnam. By the introduction of some new variants of these procedures, we provide strong experimental evidence to support our conjecture that neither greediness nor randomness is import- ant in these procedures. One of the variants in- troduced seems to offer significant improvements over earlier procedures. In addition, we investig- ate experimentally how performance depends on their parameters. Our results suggest that run- time scales less than simply exponentially in the problem size},
  file = {/home/cameron/Zotero/storage/4GRIJCC6/Gent, Walsh - 1993 - Towards an Understanding of Hill-Climbing Procedures for SAT.pdf}
}

@article{geology2019,
  title = {Basaltic {{Terrains}} in {{Idaho}} and {{Hawai}}`i as {{Planetary Analogs}} for {{Mars Geology}} and {{Astrobiology}} 1},
  author = {Geology, Mars and Hughes, Scott S. and Haberle, Christopher W and Nawotniak, Shannon E Kobs and Sehlke, Alexander and Garry, W. Brent and Elphic, Richard C and Payler, Samuel J. and Stevens, Adam H. and Cockell, Charles S and Brady, Allyson L. and Heldmann, Jennifer L and Lim, Darlene S.S.},
  year = {2019},
  volume = {19},
  number = {3},
  pages = {260--283},
  doi = {10.1089/ast.2018.1847},
  file = {/home/cameron/Zotero/storage/GCQVMV9S/ast.2018.1847.pdf}
}

@article{Ghedini2015,
  title = {Improving Robustness in Multi-Robot Networks},
  author = {Ghedini, Cinara and Secchi, Cristian and Ribeiro, Carlos H.C. and Sabattini, Lorenzo},
  year = {2015},
  journal = {IFAC-PapersOnLine},
  volume = {48},
  number = {19},
  pages = {63--68},
  issn = {24058963},
  doi = {10.1016/j.ifacol.2015.12.011},
  abstract = {This paper addresses the topological robustness of robot networks under failures; a subject often neglected in the literature. Robots are likely to fail due to several causes, which may lead to a poorly connected or a fragmented network. Our purpose is to discuss how to design resilient robot networks. For that, we first demonstrate the problem analyzing the results from a protocol to simulate failures of both central and random (w.r.t. topology) robots. Then, we propose mechanisms for detecting the probability of a robot being in a fragile local configuration and for improving its local robustness. The procedures rely solely on local information: each robot estimates its probability of being in a harmful configuration based on the positions of its neighbors. Such probability is estimated as the number of paths connecting a robot to its 2-hop neighbors by the number of paths existing in the subgraph encompassing its 1-hop and 2-hop neighborhoods. For reversing an adverse configuration, robots change their position to an average position towards their 2-hop neighbors with fewer alternative paths. The results showed that the proposed mechanism is efficient for detecting fragile topological configurations and for improving the overall network robustness.},
  keywords = {Adaptive robot control,Multi cooperative robot control,Networked robots},
  file = {/home/cameron/Zotero/storage/69LFXTIW/upload.pdf}
}

@article{Gill2005,
  title = {{{SNOPT}}: {{An SQP}} Algorithm for Large-Scale Constrained Optimization},
  author = {Gill, Philip E. and Murray, Walter and Saunders, Michael A.},
  year = {2005},
  journal = {SIAM Review},
  volume = {47},
  number = {1},
  pages = {99--131},
  issn = {00361445},
  doi = {10.1137/S0036144504446096},
  abstract = {Sequential quadratic programming (SQP) methods have proved highly effective for solving constrained optimization problems with smooth nonlinear functions in the objective and constraints. Here we consider problems with general inequality constraints (linear and nonlinear). We assume that first derivatives are available and that the constraint gradients are sparse. Second derivatives are assumed to be unavailable or too expensive to calculate. We discuss an SQP algorithm that uses a smooth augmented Lagrangian merit function and makes explicit provision for infeasibility in the original problem and the QP subproblems. The Hessian of the Lagrangian is approximated using a limited-memory quasi-Newton method. SNOPT is a particular implementation that uses a reduced-Hessian semidefinite QP solver (SQOPT) for the QP subproblems. It is designed for problems with many thousands of constraints and variables but is best suited for problems with a moderate number of degrees of freedom (say, up to 2000). Numerical results are given for most of the CUTEr and COPS test collections (about 1020 examples of all sizes up to 40000 constraints and variables, and up to 20000 degrees of freedom). \textcopyright{} 2005 Society for Industrial and Applied Mathematics.},
  keywords = {Large-scale optimization,Limited-memory methods,Nonlinear inequality constraints,Nonlinear programming,Quasi-Newton methods,Sequential quadratic progarmming},
  file = {/home/cameron/Zotero/storage/NAX9UKCQ/Gill, Murray, Saunders - 2005 - SNOPT An SQP algorithm for large-scale constrained optimization.pdf}
}

@article{Golden1987,
  title = {The {{Orienteering Problem}}},
  author = {Golden, Bruce L. and Levy, Larry and Vohra, Rakesh},
  year = {1987},
  journal = {Naval Research Logistics (NRL)},
  volume = {34},
  number = {3},
  pages = {307--318},
  issn = {15206750},
  doi = {10.1002/1520-6750(198706)34:3<307::AID-NAV3220340302>3.0.CO;2-D},
  abstract = {Orienteering is a sport in which start and end points are specified along with other locations. These other locations have associated scores. Competitors seek to visit, in a fixed amount of time, a subset of these locations on the way from the start point to the end point in order to maximize the total score. An effective center-of-gravity heuristic is presented that outperforms heuristics from the literature. Copyright \textcopyright{} 1987 Wiley Periodicals, Inc., A Wiley Company},
  file = {/home/cameron/Zotero/storage/8FB6SBWK/orienteering.pdf}
}

@article{Gopalswamy2018,
  title = {Infrastructure {{Enabled Autonomy}}: {{A Distributed Intelligence Architecture}} for {{Autonomous Vehicles}}},
  author = {Gopalswamy, Swaminathan and Rathinam, Sivakumar},
  year = {2018},
  journal = {IEEE Intelligent Vehicles Symposium, Proceedings},
  volume = {2018-June},
  eprint = {1802.04112},
  eprinttype = {arxiv},
  pages = {986--992},
  doi = {10.1109/IVS.2018.8500436},
  abstract = {Multiple studies have illustrated the potential for dramatic societal, environmental and economic benefits from significant penetration of autonomous driving. However, all the current approaches to autonomous driving require the automotive manufacturers to shoulder the primary responsibility and liability associated with replacing human perception and decision making with automation, potentially slowing the penetration of autonomous vehicles, and consequently slowing the realization of the societal benefits of autonomous vehicles. We propose here a new approach to autonomous driving that will re-balance the responsibility and liabilities associated with autonomous driving between traditional automotive manufacturers, private infrastructure players, and third-party players. Our proposed distributed intelligence architecture leverages the significant advancements in connectivity and edge computing in the recent decades to partition the driving functions between the vehicle, edge computers on the road side, and specialized third-party computers that reside in the vehicle. Infrastructure becomes a critical enabler for autonomy. With this Infrastructure Enabled Autonomy (IEA) concept, the traditional automotive manufacturers will only need to shoulder responsibility and liability comparable to what they already do today, and the infrastructure and third-party players will share the added responsibility and liabilities associated with autonomous functionalities. We propose a Bayesian Network Model based framework for assessing the risk benefits of such a distributed intelligence architecture. An additional benefit of the proposed architecture is that it enables 'autonomy as a service' while still allowing for private ownership of automobiles.},
  archiveprefix = {arXiv},
  isbn = {9781538644522},
  keywords = {autonomous vehicles,connectivity,distributed intelligence architecture,edge computing,infrastructure},
  file = {/home/cameron/Zotero/storage/NSW3P2GM/1802.04112.pdf}
}

@book{Graham1993,
  title = {On {{Lisp}}},
  author = {Graham, Paul},
  year = {1993},
  publisher = {{Prentice Hall}},
  file = {/home/cameron/Zotero/storage/W7IEP54G/Graham - Unknown - On Lisp.pdf}
}

@article{Granvilliers2006,
  title = {Algorithm 852: {{RealPaver}}: {{An}} Interval Solver Using Constraint Satisfaction Techniques},
  author = {Granvilliers, Laurent and Benhamou, Fr{\'e}d{\'e}ric},
  year = {2006},
  journal = {ACM Transactions on Mathematical Software},
  volume = {32},
  number = {1},
  pages = {138--156},
  issn = {00983500},
  doi = {10.1145/1132973.1132980},
  abstract = {RealPaver is an interval software for modeling and solving nonlinear systems. Reliable approximations of continuous or discrete solution sets are computed using Cartesian products of intervals. Systems are given by sets of equations or inequality constraints over integer and real variables. Moreover, they may have different natures, being square or nonsquare, sparse or dense, linear, polynomial, or involving transcendental functions. The modeling language permits stating constraint models and tuning parameters of solving algorithms which efficiently combine interval methods and constraint satisfaction techniques. Several consistency techniques (box, hull, and 3B) are implemented. The distribution includes C sources, executables for different machine architectures, documentation, and benchmarks. The portability is ensured by the GNU C compiler. \textcopyright{} 2006 ACM.},
  keywords = {Constraint satisfaction,Interval arithmetic,Interval Newton,Local consistency,Nonlinear system},
  file = {/home/cameron/Zotero/storage/9ZTMWV98/Granvilliers, Benhamou - 2006 - Algorithm 852 RealPaver An interval solver using constraint satisfaction techniques.pdf}
}

@techreport{Green2019,
  title = {Forward to the {{Moon}}: {{NASA}}'s {{Strategic Plan}} for {{Human Exploration}}},
  author = {Green, James L.},
  year = {2019},
  pages = {20},
  address = {{Kyoto, Japan}},
  institution = {{NASA}},
  file = {/home/cameron/Zotero/storage/BMY9DIJ5/NASA - 2019 - Forward to the Moon NASA's Strategic Plan for Human Exploration(2).pdf;/home/cameron/Zotero/storage/BXGE38P9/NASA - 2019 - Forward to the Moon NASA's Strategic Plan for Human Exploration(2).pdf}
}

@phdthesis{Groer2008,
  title = {{{PARALLEL AND SERIAL ALGORITHMS FOR VEHICLE ROUTING PROBLEMS}}},
  author = {Gro{\"e}r, Christopher},
  year = {2008},
  abstract = {The vehicle routing problem (VRP) is a widely studied combinatorial optimization problem that has many applications. Due to its intrinsic difficulty and the size of problems encountered in practice, most solution methods for the VRP are heuristic in nature and lead to high quality, yet probably not optimal solutions. When one considers the additional constraints that can be encountered in practice, the need for high quality heuristic methods is clear. We present two new variations of the VRP suggested to us by industry contacts, the Consistent VRP and the Balanced Billing Cycle VRP. We develop solution algorithms that incorporate heuristic methods as well as integer programming. Additionally, we develop a highly effective cooperative parallel algorithm for the classical VRP that generates new best solutions to a number of well-studied benchmark instances. We present extensive computational results and describe the C/C++ library that we developed to solve these vehicle routing problems. We describe the features and design philosophy behind this library and discuss how the framework can be used to implement additional heuristic algorithms and incorporate additional constraints.},
  school = {University of Maryland},
  file = {/home/cameron/Zotero/storage/ZYZNJFKM/m-api-1f7b094c-738d-8ab5-4cac-ec568b8b30d5.pdf}
}

@article{Grohs2019,
  title = {Deep Neural Network Approximations for {{Monte Carlo}} Algorithms},
  author = {Grohs, Philipp and Jentzen, Arnulf and Salimova, Diyora},
  year = {2019},
  month = aug,
  eprint = {1908.10828},
  eprinttype = {arxiv},
  abstract = {In the past few years deep artificial neural networks (DNNs) have been successfully employed in a large number of computational problems including, e.g., language processing, image recognition, fraud detection, and computa- tional advertisement. Recently, it has been proposed in the literature to employ deep neural networks (DNNs) together with stochastic gradient descent methods to approximate solutions of PDEs. There are also a few results in the literature which prove that DNNs can approximate solutions of certain PDEs without the curse of dimensionality in the sense that the number of real parameters used to describe the DNN grows at most polynomially both in the PDE dimension and the reciprocal of the prescribed approximation accuracy. One key argument in most of these results is, first, to use a Monte Carlo approximation scheme which can approximate the solution of the PDE under consideration at a fixed space-time point without the curse of dimensionality and, thereafter, to prove that DNNs are flexible enough to mimic the behaviour of the used approximation scheme. Having this in mind, one could aim for a general abstract result which shows under suitable assumptions that if a certain function can be approximated by any kind of (Monte Carlo) approximation scheme without the curse of dimensionality, then this function can also be approximated with DNNs without the curse of dimensionality. It is a key contribution of this article to make a first step towards this direction. In particular, the main result of this paper, essentially, shows that if a function can be approximated by means of some suitable discrete approximation scheme without the curse of dimensionality and if there exist DNNs which satisfy certain regularity properties and which approximate this discrete approximation scheme without the curse of dimensionality, then the function itself can also be approximated with DNNs without the curse of dimensionality. As an application of this result we establish that solutions of suitable Kolmogorov PDEs can be approximated with DNNs without the curse of dimensionality.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/59BDY4SY/1908.10828.pdf}
}

@techreport{Group2017,
  title = {Knowledge {{Exchange Approach}} to {{Open Scholarship}}},
  year = {2017},
  number = {August},
  doi = {10.5281/zenodo.826643},
  abstract = {Enabling Open Scholarship is at the heart of the Knowledge Exchange mission statement. A significant part of Knowledge Exchange's activity will aim directly at improving conditions for doing research in an open and digital way. Our latest report on Open Scholarship offers a framework for the work of Knowledge Exchange as well as insights into two new topics we've started to work on: First, the Economy of Open Science, and second, Output and Evaluation from the Researcher's Perspective. This report describes the present situation of Open Scholarship and highlights the main motivations, challenges and obstacles of the transition to Open Scholarship. You will also find the idea of a Knowledge Exchange Open Scholarship Framework, which helps to identify the Arenas (political, economic, social, technology) and the phases of the Research Life Cycle where work needs to be done to better navigate towards Open Scholarship. The report also describes two new important areas of work, first, the Economy of Open Science, and second, Output and Evaluation from the Researcher's Perspective. Both new topics are roughly sketched and more important, the report raises a bunch of questions to better understand "the economy" of Open Scholarship as well as questions to become able to make, mark and rank all types of Output in an Open Scholarship Environment. The report proposes strands of possible works to be carried out by Knowledge Exchange over the next years. Therefore the report highlights gaps in work that addresses the high level issues theory, models and deep understanding of the broad research enterprise as a system and gaps in work that looks beyond single case studies and interventions to try and build understanding from the bottom up. Overall the report can be read like a roadmap for the transition towards Open Scholarship and enhances our conference and report on "Pathways to Open Scholarship".},
  keywords = {data_management,program,research_data},
  file = {/home/cameron/Zotero/storage/EE3N59LU/R03 - Open Scholarship.pdf}
}

@article{Gunawan2016,
  title = {Orienteering {{Problem}}: {{A}} Survey of Recent Variants, Solution Approaches and Applications},
  author = {Gunawan, Aldy and Lau, Hoong Chuin and Vansteenwegen, Pieter},
  year = {2016},
  journal = {European Journal of Operational Research},
  volume = {255},
  number = {2},
  pages = {315--332},
  publisher = {{Elsevier B.V.}},
  issn = {03772217},
  doi = {10.1016/j.ejor.2016.04.059},
  abstract = {The Orienteering Problem (OP) has received a lot of attention in the past few decades. The OP is a routing problem in which the goal is to determine a subset of nodes to visit, and in which order, so that the total collected score is maximized and a given time budget is not exceeded. A number of typical variants has been studied, such as the Team OP, the (Team) OP with Time Windows and the Time Dependent OP. Recently, a number of new variants of the OP was introduced, such as the Stochastic OP, the Generalized OP, the Arc OP, the Multi-agent OP, the Clustered OP and others. This paper focuses on a comprehensive and thorough survey of recent variants of the OP, including the proposed solution approaches. Moreover, the OP has been used as a model in many different practical applications. The most recent applications of the OP, such as the Tourist Trip Design Problem and the mobile-crowdsourcing problem are discussed. Finally, we also present some promising topics for future research.},
  keywords = {Orienteering Problem,Practical Applications,Scheduling,Survey},
  file = {/home/cameron/Zotero/storage/NP4SSCMS/journalversion.pdf}
}

@article{Guo2002,
  title = {A {{Distributed}} and {{Optimal Motion Planning Approach}} for {{Multiple Mobile Robots}}},
  author = {Guo, Yi and Parker, Lynne E},
  year = {2002},
  journal = {Proceedings of the IEEE},
  pages = {1013625},
  file = {/home/cameron/Zotero/storage/SYMPKKS4/optimal_motion.pdf}
}

@article{Gurney2017,
  title = {Elliptic Curves with Complex Multiplication and \$\textbackslash{{Lambda}}\$-Structures},
  author = {Gurney, Lance},
  year = {2017},
  number = {Cm},
  eprint = {1710.08674},
  eprinttype = {arxiv},
  pages = {1--11},
  abstract = {This thesis examines the relationship between elliptic curves with complex multiplication and Lambda structures. Our main result is to show that the moduli stack of elliptic curves with complex multiplication, and the universal elliptic curve with complex multiplication over it, both admit Lambda structures and that the structure morphism is a Lambda morphism. This implies that elliptic curves with complex multiplication can be canonically lifted to the Witt vectors of the base (these are big and global Witt vectors). We also show that elliptic curves with complex multiplication of Shimura type are precisely those admitting Lambda structures and that a large class of these elliptic curves admit global minimal models. Along the way, we present a detailed study of families of elliptic curves with complex multiplication over arbitrary bases, give new derivations of the reciprocity maps associated to local fields and imaginary quadratic fields, construct a new flat, affine and pro-smooth rigidification of the moduli stack of elliptic curves with complex multiplication and exhibit a relationship between perfect Lambda schemes and periods, both \$p\$-adic and analytic.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/MI2AYPFG/18.784p.pdf}
}

@phdthesis{Guy2016,
  title = {Techniques for {{Fault Detection}} and {{Visualization}} of {{Telemetry Dependence Relationships}} for {{Root Cause Fault Analysis}} in {{Complex Systems}}},
  author = {Guy, Nathaniel},
  year = {2016},
  doi = {10.1117/12.807037},
  abstract = {This thesis explores new ways of looking at telemetry data, from a time-correlative perspective, in order to see patterns within the data that may suggest root causes of system faults. It was thought initially that visualizing an animated Pearson Correlation Coefficient (PCC) matrix for telemetry channels would be sufficient to give new under- standing; however, testing showed that the high dimensionality and inability to easily look at change over time in this approach impeded understanding. Different correlative techniques, combined with the time curve visualization proposed by Bach et al (2015), were adapted to visualize both raw telemetry and telemetry data correlations. Review revealed that these new techniques give insights into the data, and an intuitive grasp of data families, which show the effectiveness of this approach for enhancing system understanding and assisting with root cause analysis for complex aerospace systems.},
  school = {University of Washington},
  file = {/home/cameron/Zotero/storage/2WP846CS/Techniques for Fault Detection and Visualiation of Telemetry Dependence Relationships (1).pdf}
}

@techreport{guy2016,
  title = {Techniques for {{Fault Detection}} and {{Visualization}} of {{Telemetry Dependence Relationships}} for {{Root Cause Fault Analysis}} in {{Complex Systems}}},
  author = {Guy, Nathaniel},
  year = {2016},
  file = {/home/cameron/Zotero/storage/BPANG84D/Guy_washington_0250O_15584.pdf}
}

@techreport{guy2016a,
  title = {Techniques for {{Fault Detection}} and {{Visualization}} of {{Telemetry Dependence Relationships}} for {{Root Cause Fault Analysis}} in {{Complex Systems}}},
  author = {Guy, Nathaniel},
  year = {2016},
  file = {/home/cameron/Zotero/storage/BL9C8H4U/Techniques for Fault Detection and Visualiation of Telemetry Dependence Relationships.pdf}
}

@article{Hansen2001,
  title = {Hansen, {{Zilberstein}} - 2001 - {{LAO}}{${_\ast}$} {{A}} Heuristic Search Algorithm That Finds Solutions with Loops.Pdf},
  author = {Hansen, Eric A and Zilberstein, Shlomo},
  year = {2001},
  volume = {129},
  pages = {35--62},
  keywords = {dynamic programming,heuristic search,markov decision problems},
  file = {/home/cameron/Zotero/storage/RQVNRQ28/LAO.pdf}
}

@inproceedings{Harvey2013,
  title = {The {{General-Use Nodal Network Solver}} ( {{GUNNS}} ) {{Modeling Package}} for {{Space Vehicle Flow System Simulation}}},
  booktitle = {{{AIAA Modeling}} and {{Simulation Technologies}}},
  author = {Harvey, Jason and Moore, Michael},
  year = {2013},
  file = {/home/cameron/Zotero/storage/JNQKL86C/m-api-0d644f08-7677-451f-1fd7-74a95d6ad321.pdf}
}

@inproceedings{Haslum2000,
  title = {Heuristics for {{Optimal Planning}}},
  booktitle = {{{AIPS}}},
  author = {Haslum, Patrik and Geffner, Hector},
  year = {2000},
  pages = {140--149},
  keywords = {2000,aaai,aips 2000 proceedings,all rights reserved,copyright,m,org,www},
  file = {/home/cameron/Zotero/storage/X2JILIIH/m-api-1a17e602-7e2f-eceb-3ca3-73e3be715389.pdf}
}

@article{Haslum2006,
  title = {Admissible {{Heuristics}} for {{Automated Planning}}},
  author = {Haslum, Patrik},
  year = {2006},
  journal = {Link\"oping Studies in Science and Technology},
  number = {1004},
  issn = {0345-7524},
  abstract = {The problem of domain-independent automated planning has been a topic of research in Artificial Intelligence since the very beginnings of the field. Due to the desire not to rely on vast quantities of problem specific knowledge, the most widely adopted approach to automated planning is search. The topic of this thesis is the development of methods for achieving effective search control for domain-independent optimal planning through the construction of admissible heuristics. The particular planning problem considered is the so called ``classical'' AI planning problem, which makes several restricting assumptions. Optimality with respect to two measures of plan cost are considered: in planning with additive cost, the cost of a plan is the sum of the costs of the actions that make up the plan, which are assumed independent, while in planning with time, the cost of a plan is the total execution time \textendash{} makespan \textendash{} of the plan. The makespan optimization objective can not, in general, be formulated as a sum of independent action costs and therefore necessitates a problem model slightly different from the classical one. A further small extension to the classical model is made with the introduction of two forms of capacitated resources. Heuristics are developed mainly for regression planning, but based on principles general enough that heuristics for other planning search spaces can be derived on the same basis. The thesis describes a collection of methods, including the hm, additive hm and improved pattern database heuristics, and the relaxed search and boosting techniques for improving heuristics through limited search, and presents two extended experimental analyses of the developed methods, one comparing heuristics for planning with additive cost and the other concerning the relaxed search technique in the context of planning with time, aimed at discovering the characteristics of problem domains that determine the relative effectiveness of the compared methods. Results indicate that some plausible such characteristics have been found, but are not entirely conclusive.},
  isbn = {91-85497-28-2},
  file = {/home/cameron/Zotero/storage/7L32XTTD/Haslum - 2006 - Admissible Heuristics for Automated Planning.pdf}
}

@misc{Hautaluoma2019,
  title = {New {{VIPER}} Lunar Rover to Map Water Ice on the Moon},
  author = {Hautaluoma, Grey and Johnson, Alana},
  year = {2019},
  journal = {phys.org},
  number = {October},
  pages = {3},
  howpublished = {https://phys.org/news/2019-10-viper-lunar-rover-ice-moon.html},
  file = {/home/cameron/Zotero/storage/3X3TZC3V/2019-10-viper-lunar-rover-ice-moon.pdf}
}

@article{Havelund2000,
  title = {Formal {{Analysis}} of the {{Remote Agent---Before}} and {{After Flight}}},
  author = {Havelund, K and Lowry, M and Park, S and Pecheur, C and Penix, J and Visser, W and White, Jon L},
  year = {2000},
  journal = {Proceedings of the NASA Langley Formal Methods Workshop},
  file = {/home/cameron/Zotero/storage/NCWJ8LMP/0176 (Havelund).pdf}
}

@article{Havelund2001,
  title = {Formal Analysis of a Space-Craft Controller Using {{SPIN}}},
  author = {Havelund, Klaus and Lowry, Mike and Penix, John},
  year = {2001},
  journal = {IEEE Transactions on Software Engineering},
  volume = {27},
  number = {8},
  pages = {749--765},
  issn = {00985589},
  doi = {10.1109/32.940728},
  abstract = {This paper documents an application of the finite state model checker SPIN to formally analyze a multithreaded plan execution module. The plan execution module is one component of NASA's New Millennium Remote Agent, an artificial intelligence-based space-craft control system architecture which launched in October of 1998 as part of the DEEP SPACE 1 mission. The bottom layer of the plan execution module architecture is a domain specific language, named ESL (Executive Support Language), implemented as an extension to multithreaded COMMON LISP. ESL supports the construction of reactive control mechanisms for autonomous robots and space-craft. For this case study, we translated the ESL services for managing interacting parallel goal-and-event driven processes into the PROMELA input language of SPIN. A total of five previously undiscovered concurrency errors were identified within the implementation of ESL. According to the Remote Agent programming team, the effort has had a major impact, locating errors that would not have been located otherwise and, in one case, identifying a major design flaw. In fact, in a different part of the system, a concurrency bug identical to one discovered by this study escaped testing and caused a deadlock during an in-flight experiment 96 million kilometers from earth. The work additionally motivated the introduction of procedural abstraction in terms of inline procedures into SPIN.},
  keywords = {Concurrent programs,Model checking,Model extraction,Program abstraction,Program verification,Space-craft software,Temporal logic},
  file = {/home/cameron/Zotero/storage/VDRQMTFH/Havelund, Lowry, Penix - 2001 - Formal analysis of a space-craft controller using SPIN.pdf}
}

@article{Healy2013,
  title = {Choosing a {{Workflow}}},
  author = {Healy, Kieran},
  year = {2013},
  doi = {10.1093/bib/bbq084.},
  file = {/home/cameron/Zotero/storage/S28TRFMS/Healy - 2013 - Choosing a Workflow.pdf}
}

@article{helbing1995,
  title = {Social Force Model for Pedestrian Dynamics},
  author = {Helbing, Dirk and Moln{\'a}r, P{\'e}ter},
  year = {1995},
  journal = {Physical Review E},
  volume = {51},
  number = {5},
  eprint = {cond-mat/9805244},
  eprinttype = {arxiv},
  pages = {4282--4286},
  issn = {1063651X},
  doi = {10.1103/PhysRevE.51.4282},
  abstract = {It is suggested that the motion of pedestrians can be described as if they would be subject to ''social forces.'' These ''forces'' are not directly exerted by the pedestrians' personal environment, but they are a measure for the internal motivations of the individuals to perform certain actions (movements). The corresponding force concept is discussed in more detail and can also be applied to the description of other behaviors. In the presented model of pedestrian behavior several force terms are essential: first, a term describing the acceleration towards the desired velocity of motion; second, terms reflecting that a pedestrian keeps a certain distance from other pedestrians and borders; and third, a term modeling attractive effects. The resulting equations of motion of nonlinearly coupled Langevin equations. Computer simulations of crowds of interacting pedestrians show that the social force model is capable of describing the self-organization of several observed collective effects of pedestrian behavior very realistically. \textcopyright{} 1995 The American Physical Society.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/JKPDNNDW/9805244.pdf}
}

@techreport{Heldmann,
  title = {{{MOJAVE VOLATILES PROSPECTOR}} ({{MVP}}): {{SCIENCE AND OPERATIONS RESULTS FROM A LUNAR POLAR ROVER ANALOG FIELD CAMPAIGN}}},
  author = {Heldmann, Jennifer L. and Colaprete, Anthony and Cook, Amanda and Roush, Ted and Deans, Matthew C. and Elphic, Richard C. and Lim, Darlene S.S. and Skok, J. R. and Button, Nicole E. and Karunatillake, S. and Garcia, G.},
  year = {2019},
  volume = {94035},
  pages = {2},
  address = {{Moffett Field, CA}},
  institution = {{NASA Ames Research Center}},
  abstract = {The Mojave Volatiles Prospector (MVP) project is a science-driven field program with the goal of producing critical knowledge for conduct- ing robotic exploration of the Moon. MVP feeds sci- ence, payload, and operational lessons learned to the development of a real-time, short-duration lunar polar volatiles prospecting mission. MVP achieved these goals through a simulated lunar rover mission to inves- tigate the composition and distribution of surface and subsurface volatiles in a natural and a priori unknown environment within the Mojave Desert, improving our understanding of how to find, characterize, and access volatiles on the Moon.},
  file = {/home/cameron/Zotero/storage/T8334AST/m-api-18f0115b-87e2-4a15-0bda-b00e47545569.pdf}
}

@article{Heldmann2016,
  title = {Lunar Polar Rover Science Operations: {{Lessons}} Learned and Mission Architecture Implications Derived from the {{Mojave Volatiles Prospector}} ({{MVP}}) Terrestrial Field Campaign},
  author = {Heldmann, Jennifer L. and Colaprete, Anthony and Elphic, Richard C. and Lim, Darlene and Deans, Matthew and Cook, Amanda and Roush, Ted and Skok, J. R. and Button, Nicole E. and Karunatillake, S. and Stoker, Carol and Marquez, Jessica J. and Shirley, Mark and Kobayashi, Linda and Lees, David and Bresina, John and Hunt, Rusty},
  year = {2016},
  journal = {Advances in Space Research},
  volume = {58},
  number = {4},
  pages = {545--559},
  issn = {18791948},
  doi = {10.1016/j.asr.2016.05.011},
  abstract = {The Mojave Volatiles Prospector (MVP) project is a science-driven field program with the goal of producing critical knowledge for conducting robotic exploration of the Moon. Specifically, MVP focuses on studying a lunar mission analog to characterize the form and distribution of lunar volatiles. Although lunar volatiles are known to be present near the poles of the Moon, the three dimensional distribution and physical characteristics of lunar polar volatiles are largely unknown. A landed mission with the ability to traverse the lunar surface is thus required to characterize the spatial distribution of lunar polar volatiles. NASA's Resource Prospector (RP) mission is a lunar polar rover mission that will operate primarily in sunlit regions near a lunar pole with near-real time operations to characterize the vertical and horizontal distribution of volatiles. The MVP project was conducted as a field campaign relevant to the RP lunar mission to provide science, payload, and operational lessons learned to the development of a real-time, short-duration lunar polar volatiles prospecting mission. To achieve these goals, the MVP project conducted a simulated lunar rover mission to investigate the composition and distribution of surface and subsurface volatiles in a natural environment with an unknown volatile distribution within the Mojave Desert, improving our understanding of how to find, characterize, and access volatiles on the Moon.},
  keywords = {Missions,Moon,Rover,Volatiles},
  file = {/home/cameron/Zotero/storage/PTPXBF7Y/1-s2.0-S0273117716301971-main.pdf}
}

@inproceedings{Henzinger1996,
  title = {The {{Theory}} of {{Hybrid Automata}}},
  booktitle = {Proceedings of the 11th {{Annual IEEE Symposium}} on {{Logic}} in {{Computer Science}}},
  author = {Henzinger, Thomas A},
  year = {1996},
  eprint = {1011.1669v3},
  eprinttype = {arxiv},
  pages = {278--292},
  issn = {09240136},
  abstract = {Wesummarize several recent results about hybrid automata. Our goal is to demonstrate that concepts from the theory of discrete concurrent systems can give insights into partly continuous systems, and that methods for the verification of finite-state systems can be used to analyze certain systems with uncountable state spaces.},
  archiveprefix = {arXiv},
  isbn = {978-3-642-25387-4},
  pmid = {25246403},
  file = {/home/cameron/Zotero/storage/UHRVDP7Q/Henzinger - 1996 - The Theory of Hybrid Automata.pdf}
}

@article{Henzinger2000,
  title = {The {{Theory}} of {{Hybrid Automata}}},
  author = {Henzinger, Thomas A},
  year = {2000},
  journal = {Verification of Digital and Hybrid Systems},
  number = {Lics 96},
  pages = {1--30},
  doi = {10.1007/978-3-642-59615-5},
  abstract = {A hybrid automaton is a formal model for a mixed discrete-continuous system. We classify hybrid automata acoording to what questions about their behavior can be answered algorithmically. The classification reveals structure on mixed discrete-continuous state spaces that was previously studied on purely discrete state spaces only. In particular, various classes of hybrid automata induce finitary trace equivalence (or similarity, or bisimilarity) relations on an uncountable state space, thus permitting the application of various model-checking techniques that were originally developed for finite-state systems.},
  file = {/home/cameron/Zotero/storage/7JC7PM45/Henzinger - 2000 - Verification of Digital and Hybrid Systems.pdf}
}

@book{Hillier2015,
  title = {Introduction to {{Operations Research}}},
  author = {Hillier, Frederick S. and Lieberman, Gerald J.},
  year = {2015},
  issn = {15372723},
  doi = {10.1080/00401706.1968.10490578},
  isbn = {978-0-07-352345-3},
  file = {/home/cameron/Zotero/storage/JC6WC8Z6/Introduction to Operations Research 10th Ed [2015].pdf}
}

@article{Ho2019,
  title = {Multi-Agent Path Finding for {{UAV}} Traffic Management},
  author = {Ho, Florence and Goncalves, Artur and Salta, Ana and Cavazza, Marc and Geraldes, Ruben and Prendinger, Helmut},
  year = {2019},
  journal = {18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS)},
  pages = {131--139},
  abstract = {Unmanned aerial vehicles (UAVs) are expected to provide a wide range of services, whereby UAV fleets will be managed by several independent service providers in shared low-altitude airspace. One important element, or redundancy, for safe and efficient UAV operation is pre-flight Conflict Detection and Resolution (CDR) methods that generate conflict-free paths for UAVs before the actual flight. Multi-Agent Path Finding (MAPF) has already been successfully applied to comparable problems with ground robots. However, most MAPF methods were tested with simplifying assumptions which do not reflect important characteristics of many real-world domains, such as delivery by UAVs where heterogeneous agents need to be considered, and new requests for flight operations are received continuously. In this paper, we extend CBS and ECBS to efficiently incorporate heterogeneous agents with computational geometry and we reduce the search space with spatio-temporal pruning. Moreover, our work introduces a ``batching'' method into CBS and ECBS to address increased amounts of requests for delivery operations in an efficient manner. We compare the performance of our ``batching'' approach in terms of runtime and solution cost to a ``first-come first-served'' approach. Our scenarios are based on a study on UAV usage predicted for 2030 in a real area in Japan. Our simulations indicate that our proposed ECBS based ``batching'' approach is more time efficient than incremental planning based on Cooperative A*, and hence can meet the requirements of timely and accurate response on delivery requests to users of such UTM services.},
  keywords = {Hetero- geneous Agents,Multi-Agent Path Finding,Pre-Flight Con- flict Detection and Resolution,Unmanned Aircraft System Traffic Management},
  file = {/home/cameron/Zotero/storage/YXJHFUKH/p131.pdf}
}

@article{Hodges2011,
  title = {A New Paradigm for Advanced Planetary Fi Eld Geology Developed through Analog Experiments on {{Earth}}},
  author = {Hodges, K. V. and Schmitt, H. H.},
  year = {2011},
  journal = {Special Paper of the Geological Society of America},
  volume = {483},
  number = {December},
  pages = {17--31},
  issn = {00721077},
  doi = {10.1130/2011.2483(02)},
  abstract = {Field geological research, as traditionally practiced on Earth, is an extremely fl exible science. Although fi eld geologists plan their traverses ahead of time-nowadays with the advantage of remote-sensing data-initial plans are continually modifi ed in response to observations, such that traverses evolve over time. This research modality differs from that utilized in extreme environments on Earth (e.g., on the ocean fl oor), on the Martian surface by the mobile laboratories Spirit and Opportunity, and by the Apollo astronauts during their explorations of the Moon. Harsh and alien conditions, time constraints, and resource limitations have led to the development of operational modes that provide a constrained and usually lower science return than traditional fi eld geology. However, emerging plans for renewed human exploration of the Moon, Mars, and near-Earth asteroids serve as an opportunity to invent a new paradigm for advanced planetary fi eld geology that embraces coordinated human and robotic research activities. This approach will introduce an operational fl exibility that is more like that of traditional fi eld geology on Earth. In addition, human and robotic collaborations, combined with the integration of new "smart" tools, should provide an augmented reality that leads to even greater science return than traditional fi eld geology. In order to take full advantage of these opportunities when planetary fi eld geology again becomes practical, it is imperative for fi eld geologists on Earth to begin right now to learn how best to incorporate advanced technologies into their research. Geologic studies of analog sites on Earth that employ new technologyenabled strategies rather than traditional research methods provide ideal opportunities to test and refi ne emerging designs for advanced planetary fi eld geologic studies, as well as to gain new insights into terrestrial geologic processes. These operational experiments will be most informative if they embrace the entire geologic research process-including problem defi nition, fi eld observation, and laboratory analysis-and not simply fi eld work. The results of such comprehensive research can be used to inform the design of a maximally effective training regimen for future astronaut explorers. \textcopyright{} 2011 The Geological Society of America.},
  isbn = {9780813724836},
  file = {/home/cameron/Zotero/storage/7EYXKFHU/Hodges, Schmitt - 2011 - A new paradigm for advanced planetary fi eld geology developed through analog experiments on Earth.pdf}
}

@book{Hoffman2019,
  title = {Programming {{WebAssembly}} with {{Rust}}},
  author = {Hoffman, Kevin and Stewart, Andrea},
  year = {2019},
  isbn = {978-989-654-082-1},
  file = {/home/cameron/Zotero/storage/39LDUW6Q/m-api-681a543d-407c-95c3-4648-29fc083a4c1d.pdf}
}

@article{Hoffmann2001,
  title = {The {{FF}} Planning System: {{Fast}} Plan Generation through Heuristic Search},
  author = {Hoffmann, J{\"o}rg and Nebel, Bernhard},
  year = {2001},
  journal = {Journal of Artificial Intelligence Research},
  volume = {14},
  eprint = {1106.0675},
  eprinttype = {arxiv},
  pages = {263--312},
  issn = {10769757},
  doi = {10.1613/jair.855},
  abstract = {We describe and evaluate the algorithmic techniques that are used in the FF planning system. Like the HSP system, FF relies on forward state space search, using a heuristic that estimates goal distances by ignoring delete lists. Unlike HSP's heuristic, our method does not assume facts to be independent. We introduce a novel search strategy that combines hill-climbing with systematic search, and we show how other powerful heuristic information can be extracted and used to prune the search space. FF was the most successful automatic planner at the recent AIPS-2000 planning competition. We review the results of the competition, give data for other benchmark domains, and investigate the reasons for the runtime performance of FF compared to HSP. \textcopyright 2001 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/43SY9IDG/hoffmannebel-FF-jair01.pdf}
}

@article{Hoffmann2004,
  title = {Ordered Landmarks in Planning},
  author = {Hoffmann, J{\"o}rg and Porteous, Julie and Sebastia, Laura},
  year = {2004},
  journal = {Journal of Artificial Intelligence Research},
  volume = {22},
  eprint = {1107.0052},
  eprinttype = {arxiv},
  pages = {215--278},
  issn = {10769757},
  doi = {10.1613/jair.1492},
  abstract = {Many known planning tasks have inherent constraints concerning the best order in which to achieve the goals. A number of research efforts have been made to detect such constraints and to use them for guiding search, in the hope of speeding up the planning process. We go beyond the previous approaches by considering ordering constraints not only over the (top-level) goals, but also over the sub-goals that will necessarily arise during planning. Landmarks are facts that must be true at some point in every valid solution plan. We extend Koehler and Hoffmann's definition of reasonable orders between top level goals to the more general case of landmarks. We show how landmarks can be found, how their reasonable orders can be approximated, and how this information can be used to decompose a given planning task into several smaller sub-tasks. Our methodology is completely domain- and planner-independent. The implementation demonstrates that the approach can yield significant runtime performance improvements when used as a control loop around state-of-the-art sub-optimal planning systems, as exemplified by FF and LPG. \textcopyright{} 2004 AI Access Foundation. All rights reserved.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/74DX5BZC/lec-07-reading-2.pdf}
}

@article{Hoffmann2004a,
  title = {Ordered Landmarks in Planning},
  author = {Hoffmann, J{\"o}rg and Porteous, Julie and Sebastia, Laura},
  year = {2004},
  journal = {Journal of Artificial Intelligence Research},
  volume = {22},
  eprint = {1107.0052},
  eprinttype = {arxiv},
  pages = {215--278},
  issn = {10769757},
  doi = {10.1613/jair.1492},
  abstract = {Many known planning tasks have inherent constraints concerning the best order in which to achieve the goals. A number of research efforts have been made to detect such constraints and to use them for guiding search, in the hope of speeding up the planning process. We go beyond the previous approaches by considering ordering constraints not only over the (top-level) goals, but also over the sub-goals that will necessarily arise during planning. Landmarks are facts that must be true at some point in every valid solution plan. We extend Koehler and Hoffmann's definition of reasonable orders between top level goals to the more general case of landmarks. We show how landmarks can be found, how their reasonable orders can be approximated, and how this information can be used to decompose a given planning task into several smaller sub-tasks. Our methodology is completely domain- and planner-independent. The implementation demonstrates that the approach can yield significant runtime performance improvements when used as a control loop around state-of-the-art sub-optimal planning systems, as exemplified by FF and LPG. \textcopyright{} 2004 AI Access Foundation. All rights reserved.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/6D3DP35E/document.pdf}
}

@article{Hofmann2006,
  title = {Exploiting Spatial and Temporal Flexibility for Plan Execution of Hybrid, {{Under-actuated}} Systems},
  author = {Hofmann, Andreas G. and Williams, Brian C.},
  year = {2006},
  journal = {Proceedings of the National Conference on Artificial Intelligence},
  volume = {1},
  pages = {948--955},
  abstract = {Robotic devices, such as rovers and autonomous spacecraft, have been successfully controlled by plan execution systems that use plans with temporal flexibility to dynamically adapt to temporal disturbances. To date these execution systems apply to discrete systems that abstract away the detailed dynamic constraints of the controlled device. To control dynamic, under-actuated devices, such as agile bipedal walking machines, we extend this execution paradigm to incorporate detailed dynamic constraints. Building upon prior work on dispatchable plan execution, we introduce a novel approach to flexible plan execution of hybrid under-actuated systems that achieves robustness by exploiting spatial as well as temporal plan flexibility. To accomplish this, we first transform the high-dimensional system into a set of low dimensional, weakly coupled systems. Second, to coordinate these systems such that they achieve the plan in real-time, we compile a plan into a concurrent timed flow tube description. This description represents all feasible control trajectories and their temporal coordination constraints, such that each trajectory satisfies all plan and dynamic constraints. Finally, the problem of runtime plan dispatching is reduced to maintaining state trajectories in their associated flow tubes, while satisfying the coordination constraints. This is accomplished through an efficient local search algorithm that adjusts a small number of control parameters in real-time. The first step has been published previously; this paper focuses on the last two steps. The approach is validated on the execution of a set of bipedal walking plans, using a high fidelity simulation of a biped. Copyright \textcopyright{} 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.},
  isbn = {1577352815},
  keywords = {computer vision,robotics},
  file = {/home/cameron/Zotero/storage/DP6IQVQK/lec-05-reading-2.pdf}
}

@article{Hofmann2006a,
  title = {Robust Execution of Temporally Flexible Plans for Bipedal Walking Devices},
  author = {Hofmann, Andreas and Williams, Brian},
  year = {2006},
  journal = {ICAPS 2006 - Proceedings, Sixteenth International Conference on Automated Planning and Scheduling},
  volume = {2006},
  pages = {386--389},
  abstract = {Robotic wheeled rovers have been successfully controlled by activity execution systems that use plans with temporal flexibility to adapt to disturbances. These systems abstract away the detailed dynamic constraints of the controlled device. To control dynamic, devices, such as agile bipeds, we extend this execution paradigm to incorporate detailed dynamic constraints. Building upon prior work on dispatchable plan execution, we introduce a novel approach to flexible plan execution that achieves robustness by exploiting spatial as well as temporal plan flexibility. To accomplish this, we first transform the high-dimensional system into a set of low dimensional, weakly-coupled systems. Second, to coordinate these systems, we compile a plan into a flow tube description, representing all legal trajectories and their temporal coordination. Finally, the problem of runtime plan dispatching is reduced to maintaining state trajectories in their associated flow tubes. The approach is validated using a high fidelity biped simulation.},
  keywords = {Proceedings of the Sixteenth International Confere},
  file = {/home/cameron/Zotero/storage/JPP828BI/Hofmann, Williams - 2006 - Robust execution of temporally flexible plans for bipedal walking devices.pdf}
}

@article{Honig2017,
  title = {Summary: {{Multi-agent}} Path Finding with Kinematic Constraints},
  author = {Honig, Wolfgang and Kumar, T. K.Satish and Cohen, Liron and Ma, Hang and Xu, Hong and Ayanian, Nora and Koenig, Sven},
  year = {2017},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  volume = {0},
  number = {Icaps},
  pages = {4869--4873},
  issn = {10450823},
  doi = {10.24963/ijcai.2017/684},
  abstract = {Multi-Agent Path Finding (MAPF) is well studied in both AI and robotics. Given a discretized environment and agents with assigned start and goal locations, MAPF solvers from AI find collision-free paths for hundreds of agents with user-provided sub-optimality guarantees. However, they ignore that actual robots are subject to kinematic constraints (such as velocity limits) and suffer from imperfect plan-execution capabilities. We therefore introduce MAPF-POST to postprocess the output of a MAPF solver in polynomial time to create a plan-execution schedule that can be executed on robots. This schedule works on non-holonomic robots, considers kinematic constraints, provides a guaranteed safety distance between robots, and exploits slack to avoid time-intensive replanning in many cases. We evaluate MAPF-POST in simulation and on differential-drive robots, showcasing the practicality of our approach.},
  isbn = {9780999241103},
  file = {/home/cameron/Zotero/storage/I95R547E/13183-57781-1-PB (1).pdf}
}

@article{Horkan2014,
  title = {Alarm Fatigue and Patient Safety.},
  author = {Horkan, Alicia M.},
  year = {2014},
  journal = {The Pennsylvania nurse},
  volume = {41},
  number = {1},
  pages = {83, 84, 85},
  issn = {0031-4617},
  abstract = {Nephrology nurses need to be aware of the potential consequences of alarm fatigue and implement processes to reduce or eliminate the incidence of alarm fatigue. Research indicates that alarms are a necessary component of ensuring patient safety. Development of alarm proto- cols and training of personnel may facilitate reduction of adverse events related to alarm fatigue. Evidence does not exist to support specific guidelines for individualizing alarm settings, which supports the need for research by nephrology nurses regarding safe parameters for alarms that may be customized for patients on dialysis. Reduction of alarm fatigue is a responsibility to be shared among all members of clinical staff and manage- ment. Nephrology nurses need to be aware that alarm management involves more than setting parameters. Patient assessment, monitoring, and appropriate interven- tion may be considered the first steps to alarm manage- ment, elimination of alarm fatigue, and ensuring patient safety.},
  isbn = {1526-744x},
  pmid = {21560964},
  keywords = {a 36-year-old female receives,alarm fatigue,avf,false alarms,forearm arteriovenous fistula,hemodialysis through a right,medical errors,nuisance alarms,patient safety,the nurses working in},
  file = {/home/cameron/Zotero/storage/IPUNIRWC/alarm-fatigue-and-patient-safety.pdf}
}

@article{Horvitz1999,
  title = {Principles of Mixed-Initiative User Interfaces},
  author = {Horvitz, Eric},
  year = {1999},
  journal = {Conference on Human Factors in Computing Systems - Proceedings},
  pages = {159--166},
  doi = {10.1145/302979.303030},
  abstract = {Recent debate has centered on the relative promise of focusing user-interface research on developing new metaphors and tools that enhance users' abilities to directly manipulate objects versus directing effort toward developing interface agents that provide automation. In this paper, we review principles that show promise for allowing engineers to enhance human-computer interaction through an elegant coupling of automated services with direct manipulation. Key ideas will be highlighted in terms of the Lookout system for scheduling and meeting management. Copyright \textcopyright{} 2012 ACM, Inc.},
  isbn = {0201485591},
  keywords = {Decision theory,Direct manipulation,Intelligent agents,Probability,UI design,User modeling},
  file = {/home/cameron/Zotero/storage/9UUXJTPN/Horvitz - 1999 - Principles of mixed-initiative user interfaces.pdf}
}

@article{Howard2020,
  title = {Paxos vs {{Raft}}: {{Have}} We Reached Consensus on Distributed Consensus?},
  author = {Howard, Heidi and Mortier, Richard},
  year = {2020},
  eprint = {2004.05074},
  eprinttype = {arxiv},
  abstract = {Distributed consensus is a fundamental primitive for constructing fault-tolerant, strongly-consistent distributed systems. Though many distributed consensus algorithms have been proposed, just two dominate production systems: Paxos, the traditional, famously subtle, algorithm; and Raft, a more recent algorithm positioned as a more understandable alternative to Paxos. In this paper, we consider the question of which algorithm, Paxos or Raft, is the better solution to distributed consensus? We analyse both to determine exactly how they differ by describing a simplified Paxos algorithm using Raft's terminology and pragmatic abstractions. We find that both Paxos and Raft take a very similar approach to distributed consensus, differing only in their approach to leader election. Most notably, Raft only allows servers with up-to-date logs to become leaders, whereas Paxos allows any server to be leader provided it then updates its log to ensure it is up-to-date. Raft's approach is surprisingly efficient given its simplicity as, unlike Paxos, it does not require log entries to be exchanged during leader election. We surmise that much of the understandability of Raft comes from the paper's clear presentation rather than being fundamental to the underlying algorithm being presented.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/5DURD44Q/2004.05074.pdf}
}

@article{Hsiang2003,
  title = {Online Dispersion Algorithms for Swarms of Robots},
  author = {Hsiang, Tien Ruey and Arkin, Esther M. and Bender, Michael A. and Fekete, S??andor and Mitchell, Joseph S B},
  year = {2003},
  journal = {Proceedings of the Annual Symposium on Computational Geometry},
  number = {3},
  pages = {382--383},
  doi = {10.1145/777847.777854},
  abstract = {Algorithms for dispersing a swarm of primitive robots in a two-dimensional unknown environment R were discussed. Each robot in the swarm was equipped with a simple sensor that is able to view neighboring locations to determine the presence of other robots or obstacles. The video showed how even simple local rules lead to complex emergent behaviors of a swarm of robots.},
  isbn = {1581136633},
  keywords = {Approximation algorithms,NP-hardness,Swarm robotics},
  file = {/home/cameron/Zotero/storage/VWK2SULQ/2003-Online_Dispersion_Algorithms_for_Swarms_of_Robots.pdf}
}

@article{Hsu2012,
  title = {Poincar\'e Plot Indexes of Heart Rate Variability Detect Dynamic Autonomic Modulation during General Anesthesia Induction},
  author = {Hsu, Che Hao and Tsai, Ming Ya and Huang, Go Shine and Lin, Tso Chou and Chen, Kuen Pao and Ho, Shung Tai and Shyu, Liang Yu and Li, Chi Yuan},
  year = {2012},
  journal = {Acta Anaesthesiologica Taiwanica},
  volume = {50},
  number = {1},
  pages = {12--18},
  publisher = {{Elsevier Taiwan LLC}},
  issn = {18754597},
  doi = {10.1016/j.aat.2012.03.002},
  abstract = {Purpose: Beat-to-beat heart rate variability (HRV) is caused by the fluctuating balance of sympathetic and parasympathetic tone. The Poincar\'e plot has been used to evaluate HRV. In this study, we validate that this new method may qualitatively and quantitatively assess the sympathovagal fluctuation in patients during induction of anesthesia with sevoflurane. Methods: Twenty-eight young patients were allocated for the study. The patients received a tilt test and on the next day they sustained anesthesia induced with inhaled anesthetics. Electrocardiography signals from the patients were relayed to an analogue-digital converter. The Poincar\'e plot is quantified by measuring SD1, SD2, and SD1/SD2. Power spectral analyses were performed and LF, HF and HF/LF were calculated. Results: The LF power and the SD2 of the Poincar\'e plot increased while subjects were tilt-up from the supine position. Additionally, a significant correlation were found between LF and SD2, HF and SD1 (p {$<$} 0.05), and LF/HF and SD2/SD1 (p {$<$} 0.01). Sevoflurane inhalation for 10 minutes had no effect on heart rate, but diminished LF, total power and SD1, SD2 of the Poincar\'e plot respectively. However, the LF, SD2 and LF/HF increased; the HF, SD1 and SD1/SD2 ratio decreased after intubation stimulation. Conclusion: Poincar\'e plot and power spectral analysis of HRV during tilt test and sevoflurane induction significantly correlate. Poincar\'e plot analysis is easier and more sensitive at evaluating the sympathovagal balance and observing the beat-to-beat HRV. Copyright \textcopyright{} 2012, Taiwan Society of Anesthesiologists. Published by Elsevier Taiwan LLC. All rights reserved.},
  pmid = {22500908},
  keywords = {general anesthesia,heart rate variability,PoincarÃ© plot},
  file = {/home/cameron/Zotero/storage/9JDAQZ55/poincare_plot.pdf}
}

@article{Huang2019,
  title = {Online {{Risk-Bounded Motion Planning}} for {{Autonomous Vehicles}} in {{Dynamic Environments}}},
  author = {Huang, Xin and Hong, Sungkweon and Hofmann, Andreas and Williams, Brian C.},
  year = {2019},
  eprint = {1904.02341},
  eprinttype = {arxiv},
  abstract = {A crucial challenge to efficient and robust motion planning for autonomous vehicles is understanding the intentions of the surrounding agents. Ignoring the intentions of the other agents in dynamic environments can lead to risky or over-conservative plans. In this work, we model the motion planning problem as a partially observable Markov decision process (POMDP) and propose an online system that combines an intent recognition algorithm and a POMDP solver to generate risk-bounded plans for the ego vehicle navigating with a number of dynamic agent vehicles. The intent recognition algorithm predicts the probabilistic hybrid motion states of each agent vehicle over a finite horizon using Bayesian filtering and a library of pre-learned maneuver motion models. We update the POMDP model with the intent recognition results in real time and solve it using a heuristic search algorithm which produces policies with upper-bound guarantees on the probability of near colliding with other dynamic agents. We demonstrate that our system is able to generate better motion plans in terms of efficiency and safety in a number of challenging environments including unprotected intersection left turns and lane changes as compared to the baseline methods.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/PCCNHPSN/1904.02341.pdf}
}

@article{Huang2019a,
  title = {Hybrid {{Risk-Aware Conditional Planning}} with {{Applications}} in {{Autonomous Vehicles}}},
  author = {Huang, Xin and Jasour, Ashkan and Deyo, Matthew and Hofmann, Andreas and Williams, Brian C.},
  year = {2019},
  journal = {Proceedings of the IEEE Conference on Decision and Control},
  volume = {2018-Decem},
  number = {Cdc},
  pages = {3608--3614},
  publisher = {{IEEE}},
  issn = {07431546},
  doi = {10.1109/CDC.2018.8619771},
  abstract = {In this paper, we address the problem of risk-aware conditional planning where the goal is generating risk bounded motion policies in the presence of uncertainty. The problem is modeled as a chance-constrained Partially Observable Markov Decision Process (CC-POMDP) with one controllable agent and multiple uncontrollable agents, each of which can choose from a set of maneuver actions. The risk is defined as the probability of the controllable agent violating safety constraints. Off-line computations include generating a library of probabilistic maneuvers for the controllable agent and planning an initial motion policy to execute. During runtime, the conditional planner can quickly look up maneuver sequences to ensure risk bounds as the world around our agent evolves. We introduce the iterative RAO{${_\ast}$} heuristic search algorithm, which iteratively generates risk bounded conditional plans over a finite horizon. We demonstrate the performance of the provided approach on two planning problems of autonomous vehicles.},
  isbn = {9781538613955},
  file = {/home/cameron/Zotero/storage/B8N88R98/08619771.pdf}
}

@article{Huang2019b,
  title = {Uncertainty-Aware Driver Trajectory Prediction at Urban Intersections},
  author = {Huang, Xin and McGill, Stephen G. and Williams, Brian C. and Fletcher, Luke and Rosman, Guy},
  year = {2019},
  journal = {Proceedings - IEEE International Conference on Robotics and Automation},
  volume = {2019-May},
  eprint = {1901.05105},
  eprinttype = {arxiv},
  pages = {9718--9724},
  issn = {10504729},
  doi = {10.1109/ICRA.2019.8794282},
  abstract = {Predicting the motion of a driver's vehicle is crucial for advanced driving systems, enabling detection of potential risks towards shared control between the driver and automation systems. In this paper, we propose a variational neural network approach that predicts future driver trajectory distributions for the vehicle based on multiple sensors.Our predictor generates both a conditional variational distribution of future trajectories, as well as a confidence estimate for different time horizons. Our approach allows us to handle inherently uncertain situations, and reason about information gain from each input, as well as combine our model with additional predictors, creating a mixture of experts.We show how to augment the variational predictor with a physics-based predictor, and based on their confidence estimations, improve overall system performance. The resulting combined model is aware of the uncertainty associated with its predictions, which can help the vehicle autonomy to make decisions with more confidence. The model is validated on real-world urban driving data collected in multiple locations. This validation demonstrates that our approach improves the prediction error of a physics-based model by 25\% while successfully identifying the uncertain cases with 82\% accuracy.},
  archiveprefix = {arXiv},
  isbn = {9781538660263},
  file = {/home/cameron/Zotero/storage/KS2YUPBZ/Huang et al. - 2019 - Uncertainty-aware driver trajectory prediction at urban intersections.pdf}
}

@article{Hunsberger2002,
  title = {Algorithms for a Temporal Decoupling Problem in Multi-Agent Planning},
  author = {Hunsberger, Luke},
  year = {2002},
  journal = {Proceedings of the National Conference on Artificial Intelligence},
  pages = {468--475},
  abstract = {The Temporal Decoupling Problem (TDP) arises when a group of agents collaborating on a set of temporally-dependent tasks seek to coordinate their execution of those tasks by applying additional temporal constraints sufficient to ensure that agents working on different tasks may operate independently. This paper: (1) formally defines the TDP, (2) presents theorems that give necessary and sufficient conditions for solutions to the TDP, (3) presents a family of sound and complete algorithms for solving the TDP, and (4) compares the performance of several variations of the basic algorithm. Although this work was motivated by a problem in collaborative multi-agent planning, it represents a contribution to the theory of Simple Temporal Networks that is independent of the motivating application.},
  file = {/home/cameron/Zotero/storage/JSI6EIFE/Hunsberger - 2002 - Algorithms for a temporal decoupling problem in multi-agent planning.pdf}
}

@article{Hunsberger2009,
  title = {Fixing the Semantics for Dynamic Controllability and Providing a More Practical Characterization of Dynamic Execution Strategies},
  author = {Hunsberger, Luke},
  year = {2009},
  journal = {TIME 2009 - 16th International Symposium on Temporal Representation and Reasoning},
  pages = {155--162},
  doi = {10.1109/TIME.2009.25},
  abstract = {Morris, Muscettola and Vidal (MMV) presented an algorithm for checking the dynamic controllability (DC) of temporal networks in which certain temporal durations are beyond the control of the planning agent. Their DC-checking algorithm is based on rules for inferring new constraints based on the real-time context within which execution decisions must be made. This paper presents a counter-example to demonstrate that some of the inference rules are, in fact, not sound. The paper fixes the problem by strengthening the definition of dynamic execution strategies to correctly capture the central prohibition against advance knowledge of future events. The new definition enables MMV's soundness proof to go through with minimal changes. It then uses the stronger definition to derive an equivalent, alternative characterization of dynamic execution strategies that highlights the real-time execution decisions that a planning agent must make. The procedural strategy used by MMV in their completeness proof is shown to satisfy the stronger definition, thus ensuring that the DC-checking algorithm is also complete with respect to the stronger definition. As a result, the paper puts MMV's DC-checking algorithm on a more solid theoretical foundation, while also providing a more practical characterization of dynamic execution strategies. \textcopyright{} 2009 IEEE.},
  isbn = {9780769537276},
  file = {/home/cameron/Zotero/storage/G4TANBA8/Hunsberger - 2009 - Fixing the semantics for dynamic controllability and providing a more practical characterization of dynamic executio.pdf}
}

@article{Hunsberger2012,
  title = {The {{Dynamic Controllability}} of {{Conditional STNs}} with {{Uncertainty}}},
  author = {Hunsberger, Luke and Posenato, Roberto and Combi, Carlo},
  year = {2012},
  eprint = {1212.2005},
  eprinttype = {arxiv},
  pages = {2--4},
  abstract = {Recent attempts to automate business processes and medical-treatment processes have uncovered the need for a formal framework that can accommodate not only temporal constraints, but also observations and actions with uncontrollable durations. To meet this need, this paper defines a Conditional Simple Temporal Network with Uncertainty (CSTNU) that combines the simple temporal constraints from a Simple Temporal Network (STN) with the conditional nodes from a Conditional Simple Temporal Problem (CSTP) and the contingent links from a Simple Temporal Network with Uncertainty (STNU). A notion of dynamic controllability for a CSTNU is defined that generalizes the dynamic consistency of a CTP and the dynamic controllability of an STNU. The paper also presents some sound constraint-propagation rules for dynamic controllability that are expected to form the backbone of a dynamic-controllability-checking algorithm for CSTNUs.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/XSC489TQ/Hunsberger, Posenato, Combi - 2012 - The Dynamic Controllability of Conditional STNs with Uncertainty.pdf}
}

@article{Hunsberger2013,
  title = {A Faster Execution Algorithm for Dynamically Controllable {{STNUs}}},
  author = {Hunsberger, Luke},
  year = {2013},
  journal = {Proceedings of the International Workshop on Temporal Representation and Reasoning},
  pages = {26--33},
  doi = {10.1109/TIME.2013.13},
  abstract = {A Simple Temporal Network with Uncertainty (STNU) is a data structure for representing and reasoning about temporal constraints where the durations of certain temporal intervals - the contingent links - are only discovered during execution. The most important property of anSTNU is whether it is dynamically controllable (DC) - that is, whether there exists a strategy for executing time-points that will guarantee that all constraints will be satisfied no matter how the durations of the contingent links turn out. The fastest DC-checking algorithm reported so far is the O(N4)-time algorithm due to Morris (2006). Huns Berger (2010) presented an O(N4)-time execution algorithm for dynamically controllable STNUs, the fastest reported so far. This paper improves upon that algorithm, presenting an O(N3)-time execution algorithm for DC STNUs. The increase in speed is due to more efficient management of the so-called "wait" constraints, which must be removed from the network whenever the corresponding contingent link completes. \textcopyright{} 2013 IEEE.},
  isbn = {9780769551128},
  file = {/home/cameron/Zotero/storage/QVLQZ6LB/Hunsberger - 2013 - A faster execution algorithm for dynamically controllable STNUs.pdf}
}

@article{Hunsberger2014,
  title = {A Faster Algorithm for Checking the Dynamic Controllability of Simple Temporal Networks with Uncertainty},
  author = {Hunsberger, Luke},
  year = {2014},
  journal = {ICAART 2014 - Proceedings of the 6th International Conference on Agents and Artificial Intelligence},
  volume = {1},
  pages = {63--73},
  doi = {10.5220/0004758100630073},
  abstract = {A Simple Temporal Network (STN) is a structure containing time-points and temporal constraints that an agent can use to manage its activities. A Simple Temporal Network with Uncertainty (STNU) augments an STN to include contingent links that can be used to represent actions with uncertain durations. The most important property of an STNU is whether it is dynamically controllable (DC)-that is, whether there exists a strategy for executing its time-points such that all constraints will necessarily be satisfied no matter how the contingent durations happen to turn out (within their known bounds). The fastest algorithm for checking the dynamic controllability of STNUs reported in the literature so far is the O(N4)-time algorithm due to Morris. This paper presents a new DC-checking algorithm that empirical results confirm is faster than Morris' algorithm, in many cases showing an order of magnitude speed-up. The algorithm employs two novel techniques. First, new constraints generated by propagation are immediately incorporated into the network using a technique called rotating Dijkstra. Second, a heuristic that exploits the nesting structure of certain paths in the STNU graph is used to determine a good order in which to process the contingent links during constraint propagation.},
  isbn = {9789897580154},
  keywords = {Dynamic controllability,Temporal networks,Uncertainty},
  file = {/home/cameron/Zotero/storage/5F7IKJZP/Hunsberger - 2014 - A faster algorithm for checking the dynamic controllability of simple temporal networks with uncertainty.pdf}
}

@article{Hunsberger2016,
  title = {Efficient Execution of Dynamically Controllable Simple Temporal Networks with Uncertainty},
  author = {Hunsberger, Luke},
  year = {2016},
  journal = {Acta Informatica},
  volume = {53},
  number = {2},
  pages = {89--147},
  publisher = {{Springer Berlin Heidelberg}},
  issn = {14320525},
  doi = {10.1007/s00236-015-0227-0},
  abstract = {A simple temporal network with uncertainty (STNU) is a data structure for representing and reasoning about temporal constraints where the durations of certain temporal intervals\textemdash the contingent links\textemdash are only discovered during execution. The most important property of an STNU is whether it is dynamically controllable (DC)\textemdash that is, whether there exists a strategy for executing its time-points that will guarantee that all of its constraints will be satisfied no matter how the durations of the contingent links turn out. The literature on STNUs includes a variety of DC-checking algorithms and execution algorithms. The fastest DC-checking algorithm reported so far is the (Formula presented.) -time algorithm due to Morris (Integration of AI and OR techniques in constraint programming\textemdash 11th international conference, CPAIOR 2014, volume 8451 of Lecture Notes in Computer Science. Springer, Berlin, pp 464\textendash 479,~2014). The fastest execution algorithm for dynamically controllable STNUs is the (Formula presented.) -time algorithm due to Hunsberger (Proceedings of the 20th international symposium on temporal representation and reasoning (TIME-2013). IEEE Computer Society, Washington, 2013). This paper begins by providing the first comprehensive, rigorous, and yet streamlined treatment of the theoretical foundations of STNUs, including execution semantics, dynamic controllability, and a set of results that have been collected into what has recently been called the fundamental theorem of STNUs. The paper carefully argues from basic definitions to proofs of the major theorems on which all of the important algorithmic work on STNUs depends. Although many parts of this presentation have appeared in various forms, in various papers, the scattered nature of the STNU literature has allowed too many holes in the theory to persist, and has relied all too often on proof sketches that leave important details unexamined. The presentation combines results from many sources, while also introducing novel approaches and proofs. The paper concludes by presenting a modified version of a recent algorithm for managing the execution of dynamically controllable STNUs, the fastest reported so far in the literature. The modified version organizes its computations more efficiently and corrects an oversight in the original algorithm.},
  file = {/home/cameron/Zotero/storage/86GVNPBI/Hunsberger - 2016 - Efficient execution of dynamically controllable simple temporal networks with uncertainty.pdf}
}

@article{Hurtado2013,
  title = {Field Geologic Observation and Sample Collection Strategies for Planetary Surface Exploration: {{Insights}} from the 2010 {{Desert RATS}} Geologist Crewmembers},
  author = {Hurtado, Jos{\'e} M. and Young, Kelsey and Bleacher, Jacob E. and Garry, W. Brent and Rice, James W.},
  year = {2013},
  journal = {Acta Astronautica},
  volume = {90},
  number = {2},
  pages = {344--355},
  issn = {00945765},
  doi = {10.1016/j.actaastro.2011.10.015},
  abstract = {Observation is the primary role of all field geologists, and geologic observations put into an evolving conceptual context will be the most important data stream that will be relayed to Earth during a planetary exploration mission. Sample collection is also an important planetary field activity, and its success is closely tied to the quality of contextual observations. To test protocols for doing effective planetary geologic field- work, the Desert RATS (Research and Technology Studies) project deployed two prototype rovers for two weeks of simulated exploratory traverses in the San Francisco volcanic field of northern Arizona. The authors of this paper represent the geologist crewmembers who participated in the 2010 field test. We document the procedures adopted for Desert RATS 2010 and report on our experiences regarding these protocols. Careful consideration must be made of various issues that impact the interplay between field geologic observations and sample collection, including time management; strategies related to duplication of samples and observations; logistical constraints on the volume and mass of samples and the volume/transfer of data collected; and paradigms for evaluation of mission success. We find that the 2010 field protocols brought to light important aspects of each of these issues, and we recommend best practices and modifications to training and operational protocols to address them. Underlying our recommendations is the recognition that the capacity of the crew to "flexibly execute" their activities is paramount. Careful design of mission parameters, especially field geologic protocols, is critical for enabling the crews to successfully meet their science objectives. \textcopyright{} 2011 IAA. Published by Elsevier Ltd. All rights reserved.},
  isbn = {7996805555},
  keywords = {Extravehicular activity (EVA),Field geology,Field geology methods,Planetary analog,Planetary field geology,Sample collection},
  file = {/home/cameron/Zotero/storage/5RJL8PTS/Hurtado et al. - 2013 - Field geologic observation and sample collection strategies for planetary surface exploration Insights from the.pdf}
}

@article{Hyvarinen2000,
  title = {Independent Component Analysis: Algorithms and Applications},
  author = {Hyvarinen, A. and Oja, E.},
  year = {2000},
  journal = {Neural Networks},
  volume = {13},
  eprint = {1504.05070},
  eprinttype = {arxiv},
  pages = {411--430},
  issn = {18064892},
  doi = {10.7819/rbgn.v19i63.1905},
  abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation ofmultivariate data, i.e. random vectors. For reasons ofcomputational and conceptual simplicity, the representation is often sought as a linear transformation ofthe original data. In other words, each component ofthe representation is a linear combination ofthe original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation ofnon-Gaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications ofICA, and our recent work on the},
  archiveprefix = {arXiv},
  isbn = {3589451327},
  pmid = {10946390},
  keywords = {blind signal separation,factor analysis,independent component analysis,projection pursuit,representation,source separation},
  file = {/home/cameron/Zotero/storage/BRBYNLSP/independent-component-analysis.pdf}
}

@article{Ingham2001,
  title = {A {{Reactive Model-based Programming Language}} for {{Robotic Space Explorers Model-based Programming}}},
  author = {Ingham, Michel and Ragno, Robert and Williams, Brian},
  year = {2001},
  journal = {Proceedings of the 6th International Symposium on Artificial Intelligence, Robotics and Automation in Space},
  abstract = {Model-based autonomous agents have emerged recently as vital technologies in the development of highly autonomous reactive systems, particularly in the aerospace domain. These agents utilize many automated reasoning capabilities, but are complicated to use because of the variety of languages employed for each capability. To address this problem, we introduce model-based programming, a novel approach to designing embedded software systems. In particular, we introduce the Reactive Model-based Programming Language (RMPL), which provides a framework for constraint-based modeling, as well as a suite of reactive programming constructs. To convey the expressiveness of RMPL, we show how it captures the main features of synchronous programming languages and advanced robotic execution languages. This paper focuses on using the rich behavior modeling of RMPL to provide sequencing and robotic execution capabilities for spacecraft.},
  keywords = {execution,model-based,model-based autonomy,programming,synchronous programming},
  file = {/home/cameron/Zotero/storage/MABC8Z5Y/Ingham, Ragno, Williams - 2001 - A Reactive Model-based Programming Language for Robotic Space Explorers Model-based Programming.pdf}
}

@article{Innes2019,
  title = {Zygote: {{A}} Differentiable Programming System to Bridge Machine Learning and Scientific Computing},
  author = {Innes, Mike and Edelman, Alan and Fischer, Keno and Rackauckas, Chris and Saba, Elliot and Shah, Viral B. and Tebbutt, Will},
  year = {2019},
  journal = {arXiv},
  eprint = {1907.07587},
  eprinttype = {arxiv},
  abstract = {Scientific computing is increasingly incorporating the advancements in machine learning and the ability to work with large amounts of data. At the same time, machine learning models are becoming increasingly sophisticated and exhibit many features often seen in scientific computing, stressing the capabilities of machine learning frameworks. Just as the disciplines of scientific computing and machine learning have shared common underlying infrastructure in the form of numerical linear algebra, we now have the opportunity to further share new computational infrastructure, and thus ideas, in the form of Differentiable Programming. We describe a Differentiable Programming ({$\partial$}P) system that is able to take gradients of Julia programs making Automatic Differentiation a first class language feature. Our system supports almost all language constructs (control flow, recursion, mutation, etc.) and compiles high-performance code without requiring any user intervention or refactoring to stage computations. This enables an expressive programming model for deep learning and, more importantly, it enables users to utilize the existing Julia ecosystem of scientific computing packages in deep learning models. We discuss support for advanced techniques such as mixed-mode, complex and checkpointed differentiation, and present how this leads to efficient code generation. We then showcase several examples of differentiating programs and mixing deep learning with existing Julia packages, including differentiable ray tracing, machine learning on simulated quantum hardware, training neural stochastic differential equation representations of financial models and more.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/WHDZXEBN/1907.07587v1.pdf}
}

@article{IsmelBritoFernandoHerreroCarron2004,
  title = {On the Evaluation of {{DisCSP Algorithms}}},
  author = {Ismel Brito, Fernando Herrero Carron, Pedro Meseguer},
  year = {2004},
  journal = {Edinburgh Mathematical Notes},
  volume = {32},
  number = {September 2015},
  pages = {xiii-xiv},
  issn = {0950-1843},
  doi = {10.1017/s0950184300800014},
  abstract = {It is not perhaps generally realised that the special bilinear substitution , used to reduce the integral},
  file = {/home/cameron/Zotero/storage/WXZEHK7S/On_the_Evaluation_of_DisCSP_Algorithms.pdf}
}

@article{jabercromby,
  title = {{{NASA RESEARCH AND TECHNOLOGY STUDIES}} ({{RATS}}) 2012: {{EVALUATION OF HUMAN AND ROBOTIC SYSTEMS FOR EXPLORATION OF NEAR-EARTH ASTEROIDS}}},
  author = {J Abercromby, Andrew F and Chappell, S P and Litaker, H L and Gernhardt, M L},
  doi = {10.1016/j.actaastro},
  abstract = {Introduction: Fundamental to the development of NASA's Capability Driven Framework [1] is identifying the exploration systems that are required for the range of destinations being considered and finding safe, affordable, and effective ways to develop and operate those systems. Design reference missions (DRMs) currently being considered by NASA for human exploration of near-Earth asteroids (NEAs) include stays in the proximity of the target NEA of between 14 and 56 days during which time the Earth-NEA transit vehicle, Deep Space Habitat (DSH), and multipurpose crew vehicle (MPCV, used for crew launch and re-entry) would remain between 500 meters and 2 km away from the NEA to minimize the possibility of collision. Exploration and sampling of the NEA surface would be conducted by crewmembers leaving the DSH in extravehicular activity (EVA) suits with appropriately sized EVA jetpacks to enable brief sor-ties to the NEA surface and/or by using a Multi-Mission Space Exploration Vehicle (MMSEV), which is a small pressurized spacecraft with rapid EVA capability enabling multi-day exploration sorties away from the DSH [2,3]. Methods: The RATS 2012 test focused on optimizing utilization of a four-person crew, DSH, MMSEV, EVA jetpacks, and Mission Control Center operating with 50 seconds each-way communication latency, during exploration within an immersive virtual reality physics-based simulation of the NEA Itokawa (Fig. 1). High-resolution imagery from the Japanese Space Agency's (JAXA) Hayabusa mission was integrated into the simulation and models of three-dimensional boulders of representative sizes, shapes, and distributions were added at specific locations based on inputs from a scientist with expert knowledge of Itokawa. Traverse plans were prepared based on the Hayabusa imagery and using assumptions for sampling and instrument deployment methods and durations based on data from complementary testing in reduced gravity environments [4]. Traverses were planned to include variation in terrain, illumination, and centripe-tal acceleration within each traverse while maintaining consistency among test conditions.},
  file = {/home/cameron/Zotero/storage/ULUL49IP/rats-2012.pdf}
}

@article{Jawin2019,
  title = {Lunar {{Science}} for {{Landed Missions Workshop Findings Report}}},
  author = {Jawin, Erica R. and Valencia, Sarah N. and Watkins, Ryan N. and Crowell, James M. and Neal, Clive R. and Schmidt, Gregory},
  year = {2019},
  journal = {Earth and Space Science},
  volume = {6},
  number = {1},
  pages = {2--40},
  issn = {23335084},
  doi = {10.1029/2018EA000490},
  abstract = {The Lunar Science for Landed Missions workshop was convened at the National Aeronautics and Space Administration Ames Research Center on 10\textendash 12 January, 2018. Interest in the workshop was broad, with 110 people participating in person and 70 people joining online. In addition, the workshop website (https://lunar-landing.arc.nasa.gov) includes video recordings of many of the presentations. This workshop defined a set of targets that near-term landed missions could visit for scientific exploration. The scope of such missions was aimed primarily, but not exclusively, at commercial exploration companies with interests in pursuing ventures on the surface of the Moon. Contributed and invited talks were presented that detailed many high priority landing site options across the surface of the Moon that would meet scientific goals in a wide variety of areas, including impact cratering processes and dating, volatiles, volcanism, magnetism, geophysics, and astrophysics. Representatives from the Japan Aerospace Exploration Agency and the European Space Agency also presented about international plans for lunar exploration and science. This report summarizes the set of landing sites and/or investigations that were presented at the workshop that would address high priority science and exploration questions. In addition to landing site discussions, technology developments were also specified that were considered as enhancing to the types of investigations presented. It is evident that the Moon is rich in scientific exploration targets that will inform us on the origin and evolution of the Earth-Moon system and the history of the inner Solar System, and also has enormous potential for enabling human exploration and for the development of a vibrant lunar commercial sector.},
  keywords = {exploration,landed missions,lunar science,Moon,technology development},
  file = {/home/cameron/Zotero/storage/QCYKW8KB/LunarLandedScience_Publication.pdf}
}

@article{Johnson2007,
  title = {Notes on {{Adjoint Methods}}},
  author = {Johnson, Steven G},
  year = {2007},
  journal = {October},
  pages = {1--7},
  file = {/home/cameron/Zotero/storage/62PWNSGM/m-api-23b48860-2566-e10b-6dfc-e83f6f6c8e21.pdf}
}

@article{Johnson2016,
  title = {Machine {{Learning}} and {{Decision Support}} in {{Critical Care}}},
  author = {Johnson, Alistair E.W. and Ghassemi, Mohammad M. and Nemati, Shamim and Niehaus, Katherine E. and Clifton, David and Clifford, Gari D.},
  year = {2016},
  journal = {Proceedings of the IEEE},
  volume = {104},
  number = {2},
  pages = {444--466},
  issn = {00189219},
  doi = {10.1109/JPROC.2015.2501978},
  abstract = {Clinical data management systems typically provide caregiver teams with useful information, derived from large, sometimes highly heterogeneous, data sources that are often changing dynamically. Over the last decade there has been a significant surge in interest in using these data sources, from simply reusing the standard clinical databases for event prediction or decision support, to including dynamic and patient-specific information into clinical monitoring and prediction problems. However, in most cases, commercial clinical databases have been designed to document clinical activity for reporting, liability, and billing reasons, rather than for developing new algorithms. With increasing excitement surrounding ``secondary use of medical records'' and ``Big Data'' analytics, it is important to understand the limitations of current databases and what needs to change in order to enter an era of ``precision medicine.'' This review article covers many of the issues involved in the collection and preprocessing of critical care data. The three challenges in critical care are considered: compartmentalization, corruption, and complexity. A range of applications addressing these issues are covered, including the modernization of static acuity scoring; online patient tracking; personalized prediction and risk assessment; artifact detection; state estimation; and incorporation of multimodal data sources such as genomic and free text data.},
  isbn = {0018-9219 VO - 104},
  pmid = {27765959},
  keywords = {Critical care,Feature extraction,Machine learning,Signal processing},
  file = {/home/cameron/Zotero/storage/C5WERKWJ/ml-in-health-care.pdf}
}

@article{johnson2016,
  title = {Machine {{Learning}} and {{Decision Support}} in {{Critical Care}}},
  author = {Johnson, Alistair E.W. and Ghassemi, Mohammad M. and Nemati, Shamim and Niehaus, Katherine E. and Clifton, David and Clifford, Gari D.},
  year = {2016},
  month = feb,
  journal = {Proceedings of the IEEE},
  volume = {104},
  number = {2},
  pages = {444--466},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {15582256},
  doi = {10.1109/JPROC.2015.2501978},
  abstract = {Clinical data management systems typically provide caregiver teams with useful information, derived from large, sometimes highly heterogeneous, data sources that are often changing dynamically. Over the last decade there has been a significant surge in interest in using these data sources, from simply reusing the standard clinical databases for event prediction or decision support, to including dynamic and patient-specific information into clinical monitoring and prediction problems. However, in most cases, commercial clinical databases have been designed to document clinical activity for reporting, liability, and billing reasons, rather than for developing new algorithms. With increasing excitement surrounding 'secondary use of medical records' and 'Big Data' analytics, it is important to understand the limitations of current databases and what needs to change in order to enter an era of 'precision medicine.' This review article covers many of the issues involved in the collection and preprocessing of critical care data. The three challenges in critical care are considered: compartmentalization, corruption, and complexity. A range of applications addressing these issues are covered, including the modernization of static acuity scoring; online patient tracking; personalized prediction and risk assessment; artifact detection; state estimation; and incorporation of multimodal data sources such as genomic and free text data.},
  pmid = {27765959},
  keywords = {Critical care,Feature extraction,Machine learning,Signal processing},
  file = {/home/cameron/Zotero/storage/228Z7FSF/ml-in-health-care.pdf}
}

@phdthesis{jones2016,
  title = {Using {{Planned View Trajectories}} to {{Build Good Models}} of {{Planetary Features}} under {{Transient Illumination}}},
  author = {Jones, Heather L.},
  year = {2016},
  number = {May},
  abstract = {This research addresses the modeling of substantially 3D planetary terrain features, such as skylights, canyons, craters, rocks, and mesas, by a surface robot. The sun lights planetary features with transient, directional illumination. For concave features like skylight pits, craters, and canyons, this can lead to dark shadows. For all terrain features, the ability to detect interest points and to match them between images is complicated by changing illumination, so seeing planetary features in the best light requires a coordinated dance with the sun as it arcs across the sky. The research develops a process for planned-view-trajectory model building that converts a coarse model of a terrain feature and knowledge about illumination change and mission parameters into a view trajectory planning problem, plans and executes a view trajectory, and builds a detailed model from the captured images. An understanding of how view and illumination angles affect model quality is reached through controlled lighting laboratory experiments. The planning of view trajectories, (what to image, from where, and at what time), is cast as an OPTWIND, a new vehicle routing problem formulated in the thesis work. Each part of the planned-view-trajectory model building process is examined in detail, with existing tools identified and tested to solve parts of the problem, where appropriate, and new solutions implemented otherwise. The research also demonstrates planned-view-trajectory model building. Contributions of the research include development, implementation, and demonstration of planned-view-trajectory model building, experimental determination of factors affecting model quality and formulation of view trajectory planning as a new vehicle routing problem. Datasets for planetary analog terrain and for imaging under directional lighting were also collected, totaling over 25,000 images.},
  school = {Carnegie Mellon University},
  file = {/home/cameron/Zotero/storage/QEU3TN2B/hljones_thesis_document_20160511_FINAL.pdf}
}

@article{Joshi2013,
  title = {Information Fusion Based Learning for Frugal Traffic State Sensing},
  author = {Joshi, Vikas and Rajamani, Nithya and Takayuki, K. and Prathapaneni, Naveen and Subramaniam, L. V.},
  year = {2013},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  pages = {2826--2832},
  issn = {10450823},
  abstract = {Traffic sensing is a key baseline input for sustainable cities to plan and administer demand-supply management through better road networks, public transportation, urban policies etc., Humans sense the environment frugally using a combination of complementary information signals from different sensors. For example, by viewing and/or hearing traffic one could identify the state of traffic on the road. In this paper, we demonstrate a fusion based learning approach to classify the traffic states using low cost audio and image data analysis using real world dataset. Road side collected traffic acoustic signals and traffic image snapshots obtained from fixed camera are used to classify the traffic condition into three broad classes viz., Jam, Medium and Free. The classification is done on \{10sec audio, image snapshot in that 10sec\} data tuple. We extract traffic relevant features from audio and image data to form a composite feature vector. In particular, we extract the audio features comprising MFCC (Mel-Frequency Cepstral Coefficients) classifier based features, honk events and energy peaks. A simple heuristic based image classifier is used, where vehicular density and number of corner points within the road segment are estimated and are used as features for traffic sensing. Finally the composite vector is tested for its ability to discriminate the traffic classes using Decision tree classifier, SVM classifier, Discriminant classifier and Logistic regression based classifier. Information fusion at multiple levels (audio, image, overall) shows consistently better performance than individual level decision making. Low cost sensor fusion based on complementary weak classifiers and noisy features still generates high quality results with an overall accuracy of 93 - 96\%.},
  isbn = {9781577356332},
  keywords = {Special Track on Artificial Intelligence and Compu},
  file = {/home/cameron/Zotero/storage/NVUCTFH8/416.pdf}
}

@article{Joslin1995,
  title = {Passive and {{Active Decision Postponement}} in {{Plan Generation}}},
  author = {Joslin, David and Pollack, Martha E},
  year = {1995},
  journal = {In: Proceedings of the 3rd European Conference on Planning},
  pages = {1--15},
  keywords = {constraint satisfaction,deci-,partial-order causal-link planning,plan generation,sion postponement},
  file = {/home/cameron/Zotero/storage/332FXAZ5/Joslin, Pollack - 1995 - Passive and Active Decision Postponement in Plan Generation.pdf}
}

@inproceedings{Kanelakos2020,
  title = {Artemis {{EVA Flight Operations}} - {{Preparing}} for {{Lunar EVA Training}} \& {{Execution}}},
  booktitle = {{{NASA EVA Exploration Workshop}}},
  author = {Kanelakos, Alex},
  year = {2020},
  pages = {47},
  address = {{Houston, TX}},
  file = {/home/cameron/Zotero/storage/28NLFNYZ/m-api-77b3015c-bf58-41db-3ad3-2dd636c46991.pdf}
}

@article{karaman2011,
  title = {Sampling-Based {{Algorithms}} for {{Optimal Motion Planning}}},
  author = {Karaman, Sertac and Frazzoli, Emilio},
  year = {2011},
  month = may,
  eprint = {1105.1186},
  eprinttype = {arxiv},
  abstract = {During the last decade, sampling-based path planning algorithms, such as Probabilistic RoadMaps (PRM) and Rapidly-exploring Random Trees (RRT), have been shown to work well in practice and possess theoretical guarantees such as probabilistic completeness. However, little effort has been devoted to the formal analysis of the quality of the solution returned by such algorithms, e.g., as a function of the number of samples. The purpose of this paper is to fill this gap, by rigorously analyzing the asymptotic behavior of the cost of the solution returned by stochastic sampling-based algorithms as the number of samples increases. A number of negative results are provided, characterizing existing algorithms, e.g., showing that, under mild technical conditions, the cost of the solution returned by broadly used sampling-based algorithms converges almost surely to a non-optimal value. The main contribution of the paper is the introduction of new algorithms, namely, PRM* and RRT*, which are provably asymptotically optimal, i.e., such that the cost of the returned solution converges almost surely to the optimum. Moreover, it is shown that the computational complexity of the new algorithms is within a constant factor of that of their probabilistically complete (but not asymptotically optimal) counterparts. The analysis in this paper hinges on novel connections between stochastic sampling-based path planning algorithms and the theory of random geometric graphs.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/Y5CBVLKR/KaramanFrazzoliSamplingBasedMotionPlanning.pdf}
}

@article{Karpas2015,
  title = {Temporal Landmarks: {{What}} Must Happen, and When},
  author = {Karpas, Erez and Wang, David and Williams, Brian C. and Haslum, Patrik},
  year = {2015},
  journal = {Proceedings International Conference on Automated Planning and Scheduling, ICAPS},
  volume = {2015-Janua},
  pages = {138--146},
  issn = {23340843},
  abstract = {Current temporal planners have a hard time solving large, real-world problems which involve dealing with metric time and concurrent actions. While landmarks have enabled classical planners to scale up to significantly larger problems, they have not yet brought as much benefit to temporal planning. We argue that the reason for this is that for landmarks to make an effective addition to planning with complex temporal interactions (such as required concurrency), they must incorporate information about the timing of conditions and events. We define temporal landmarks, which associate time intervals and time points, respectively, with state and action landmarks, thereby capturing both what must happen and when it must happen. We show how to derive temporal landmarks and constraints on their associated time points from planning problems, and how exploiting them, in a planner-independent way, can improve planner performance. Notably, the greatest gain is on problems which require concurrency, showing that the temporal information we add to landmarks complements the reasoning used by current temporal planners.},
  isbn = {9781577357315},
  keywords = {Main Technical Track},
  file = {/home/cameron/Zotero/storage/KMXPN2V8/lec-07-reading-4.pdf}
}

@article{Karpas2015a,
  title = {Temporal Landmarks: {{What}} Must Happen, and When},
  author = {Karpas, Erez and Wang, David and Williams, Brian C. and Haslum, Patrik},
  year = {2015},
  journal = {Proceedings International Conference on Automated Planning and Scheduling, ICAPS},
  volume = {2015-Janua},
  pages = {138--146},
  issn = {23340843},
  abstract = {Current temporal planners have a hard time solving large, real-world problems which involve dealing with metric time and concurrent actions. While landmarks have enabled classical planners to scale up to significantly larger problems, they have not yet brought as much benefit to temporal planning. We argue that the reason for this is that for landmarks to make an effective addition to planning with complex temporal interactions (such as required concurrency), they must incorporate information about the timing of conditions and events. We define temporal landmarks, which associate time intervals and time points, respectively, with state and action landmarks, thereby capturing both what must happen and when it must happen. We show how to derive temporal landmarks and constraints on their associated time points from planning problems, and how exploiting them, in a planner-independent way, can improve planner performance. Notably, the greatest gain is on problems which require concurrency, showing that the temporal information we add to landmarks complements the reasoning used by current temporal planners.},
  keywords = {Main Technical Track},
  file = {/home/cameron/Zotero/storage/ZKL88RD5/document.pdf}
}

@article{Katayama1994,
  title = {Attempt to Isolate Mast-Cell Precursors Based on the Differential Sensitivity to {{UV-B}} and {{X-irradiation}}},
  author = {Katayama, N. and Nohara, O. and Moriyama, H. and Fujimaki, H.},
  year = {1994},
  journal = {Toxic Substances Journal},
  volume = {13},
  number = {2},
  pages = {85--95},
  issn = {01993178},
  doi = {10.1609/AIMAG.V17I3.1230},
  abstract = {Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media atten- tion of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges in- volved in real-world applications of knowledge discovery, and current and future research direc- tions in the field.},
  arxiv = {aimag.v17i3.1230},
  isbn = {0262560976},
  pmid = {12948721},
  file = {/home/cameron/Zotero/storage/QTM37ZIL/1230-Article Text-1227-1-10-20080129.pdf}
}

@article{Katsikopoulos2022,
  title = {Decoding Human Behavior with Big Data ? {{Critical}} , Constructive Input from the Decision Sciences},
  author = {Katsikopoulos, Konstantinos V and Canellas, Marc C.},
  year = {2022},
  journal = {AI Magazine},
  number = {43},
  pages = {126--138},
  doi = {10.1002/aaai.12034},
  abstract = {Big data analytics employs algorithms to uncover people's preferences and values, and support their decision making. A central assumption of big data analytics is that it can explain and predict human behavior. We investigate this assumption, aiming to enhance the knowledge basis for developing algorithmic standards in big data analytics. First, we argue that big data analytics is by design atheoretical and does not provide process-based explanations of human behavior; thus, it is unfit to support deliberation that is transparent and explainable. Second, we review evidence from interdisciplinary decision science, showing that the accuracy of complex algorithms used in big data analytics for predicting human behavior is not consistently higher than that of simple rules of thumb. Rather, it is lower in situations such as predicting election outcomes, criminal profiling, and granting bail. Big data algorithms can be considered as candidate models for explaining, predicting, and supporting human decision making when they match, in transparency and accuracy, simple, process-based, domain-grounded theories of human behavior. Big data analytics can be inspired by behavioral and cognitive theory},
  file = {/home/cameron/Zotero/storage/YGBQD989/m-api-6e8b9422-0c50-8e53-3b52-4bfbeb4680fd.pdf}
}

@techreport{Kautz1992,
  title = {Planning as {{Satisfiability}}},
  author = {Kautz, Henry and Selman, Bart},
  year = {1992},
  pages = {12},
  address = {{Murray Hill, NJ}},
  institution = {{AT\&T Bell Laboratories}},
  abstract = {We develop a formal model of planning based on satisfiability rather than deduction.},
  file = {/home/cameron/Zotero/storage/2RFKRTEC/m-api-2c91a570-3801-4723-245b-cea2414d0ad1.pdf}
}

@article{Kautz1996a,
  title = {Pushing the Envelope: {{Planning}}, Propositional Logic, and Stochastic Search},
  author = {Kautz, Henry and Selman, Bart},
  year = {1996},
  journal = {Proceedings of the National Conference on Artificial Intelligence},
  volume = {2},
  pages = {1194--1201},
  abstract = {Planning is a notoriously hard combinatorial search problem. In many interesting domains, current planning algorithms fail to scale up gracefully. By combining a general, stochastic search algorithm and appropriate problem encodings based on propositional logic, we are able to solve hard planning problems many times faster than the best current planning systems. Although stochastic methods have been shown to be very effective on a wide range of scheduling problems, this is the first demonstration of its power on truly challenging classical planning instances. This work also provides a new perspective on representational issues in planning.},
  file = {/home/cameron/Zotero/storage/TN3X9RNV/Kautz, Selman - 1996 - Pushing the envelope Planning, propositional logic, and stochastic search.pdf}
}

@article{Kepner2008,
  title = {Performance Metrics and Software Architecture},
  author = {Kepner, Jeremy and Meuse, Theresa and Schrader, Glenn E.},
  year = {2008},
  journal = {High Performance Embedded Computing Handbook: A Systems Perspective},
  pages = {303--334},
  doi = {10.1201/9781420006667.ch15},
  abstract = {FIGURE 15-2 (Color figure follows p. 278.) Unprocessed (left) and processed (right) SAR data. The area that reflects a single pulse is large and an image of this raw data is very blurry (left). A SAR system provides multiple looks at the same area of the ground from multiple viewing angles. Combining these different viewing angles together produces a much sharper image (right). (From Bader et al., Designing scalable synthetic compact applications for benchmarking high productivity computing systems, CTWatch Quarterly 2(4B), 2006. With permission.) Handbook: A Systems Perspective FIGURE 15-3 System mode block diagram. SAR system mode consists of Stage 1 front-end sensor processing and Stage 2 back-end knowledge formation. In addition, there is significant IO to the storage system. (From Bader et al., Designing scalable synthetic compact applications for benchmarking high productivity computing systems, CTWatch Quarterly 2(4B), 2006. With permission.) FIGURE 15-4 (Color figure follows p. 278.) Compute Only Mode block diagram. Simulates a streaming sensor that moves data directly from front-end processing to back-end processing. (From Bader et al., Designing scalable synthetic compact applications for benchmarking high productivity computing systems. CTWatch Quarterly 2(4B), 2006. With permission.) Handbook: A Systems Perspective FIGURE 15-5 Algorithm flow. In Stage 1, the data are transformed in series of steps from a n mcs singleprecision complex valued array to a n mx s single-precision real valued array. At each step the processing is along either the rows or the columns.},
  isbn = {9781420006667},
  file = {/home/cameron/Zotero/storage/NPME875J/Kepner, Meuse, Schrader - 2008 - Performance metrics and software architecture.pdf}
}

@techreport{Keyser1974,
  title = {Apollo {{Experience Report}} - {{The Role}} of {{Flight Mission Rules}} in {{Mission Preparation}} and {{Conduct}}},
  author = {Keyser, Larry W},
  year = {1974},
  pages = {11},
  institution = {{NASA Johnson Space Center}},
  abstract = {This report describes the development of flight mission rules from the mission development phase through the detailed mission-planning phase and through the testing and training phase. The procedure for review of the rules and the coordination requirements for mission-rule development are presented. The application of the rules to real-time decisionmaking is outlined, and consideration is given to the benefit of training ground controllers and flightcrews in the methods of determining the best response to a nonnominal in-flight situation for which no action has been preplanned. The Flight Mission Rules document is discussed in terms of the purpose and objective thereof and in terms of the definition, the development, and the use of mission rules.},
  keywords = {Barcode: 0133650},
  file = {/home/cameron/Zotero/storage/X9HJISS5/m-api-67d7b740-3b92-b489-6076-1404afcbf679.pdf}
}

@article{Khonji2019,
  title = {Risk-{{Aware Reasoning}} for {{Autonomous Vehicles}}},
  author = {Khonji, Majid and Dias, Jorge and Seneviratne, Lakmal},
  year = {2019},
  eprint = {1910.02461},
  eprinttype = {arxiv},
  abstract = {A significant barrier to deploying autonomous vehicles (AVs) on a massive scale is safety assurance. Several technical challenges arise due to the uncertain environment in which AVs operate such as road and weather conditions, errors in perception and sensory data, and also model inaccuracy. In this paper, we propose a system architecture for risk-aware AVs capable of reasoning about uncertainty and deliberately bounding the risk of collision below a given threshold. We discuss key challenges in the area, highlight recent research developments, and propose future research directions in three subsystems. First, a perception subsystem that detects objects within a scene while quantifying the uncertainty that arises from different sensing and communication modalities. Second, an intention recognition subsystem that predicts the driving-style and the intention of agent vehicles (and pedestrians). Third, a planning subsystem that takes into account the uncertainty, from perception and intention recognition subsystems, and propagates all the way to control policies that explicitly bound the risk of collision. We believe that such a white-box approach is crucial for future adoption of AVs on a large scale.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/AHZER4WB/1910.02461.pdf}
}

@article{Kim2000,
  title = {Model-Based {{Planning}} for {{Coordinated Air Vehicle Missions}}},
  author = {Kim, Phil},
  year = {2000},
  pages = {1--86},
  abstract = {In the future, webs of unmanned air and space vehicles will act together to robustly perform elaborate missions in uncertain and sometimes hostile environments. To achieve this robustness we go beyond current embedded programming languages, introducing a model-based programming language that enables autonomous vehicles to select and adapt coordinated mission plans on the fly. First, we present a variant of the Reactive Model-based Programming Language (RMPL), that allows for the expression of complex concurrent activites, metric time constraints and multiple contingencies. Second, we introduce the Temporal Planning Network (TPN), a simple, compact encoding of all possible executions of an RMPL program, that supports a model of program interpretation as fast temporal planning. Finally, we introduce an RMPL interpreter, called Kirk, which uses graph search on TPNs to find temporally consistent executions of RMPL programs.},
  file = {/home/cameron/Zotero/storage/DVGLMLYR/phils_MEngthesis.pdf}
}

@article{Kim2001,
  title = {Executing Reactive, Model-Based Programs through Graph-Based Temporal Planning},
  author = {Kim, Phil and Williams, Brian C. and Abramson, Mark},
  year = {2001},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  pages = {487--493},
  issn = {10450823},
  abstract = {In the future, webs of unmanned air and space vehicles will act together to robustly perform elaborate missions in uncertain environments. We coordinate these systems by introducing a reactive model-based programming language (RMPL) that combines within a single unified representation the flexibility of embedded programming and reactive execution languages, and the deliberative reasoning power of temporal planners. The KIRK planning system takes as input a problem expressed as a RMPL program, and compiles it into a temporal plan network (TPN), similar to those used by temporal planners, but extended for symbolic constraints and decisions. This intermediate representation clarifies the relation between temporal planning and causal-link planning, and permits a single task model to be used for planning and execution. Such a unified model has been described as a holy grail for autonomous agents by the designers of the Remote Agent[Muscettola et al., 1998b].},
  file = {/home/cameron/Zotero/storage/MHV44Y7S/document.pdf}
}

@article{Kim2019a,
  title = {Bayesian Inference of Linear Temporal Logic Specifications for Contrastive Explanations},
  author = {Kim, Joseph and Muise, Christian and Shah, Ankit and Agarwal, Shubham and Shah, Julie},
  year = {2019},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  volume = {2019-Augus},
  pages = {5591--5598},
  issn = {10450823},
  doi = {10.24963/ijcai.2019/776},
  abstract = {Temporal logics are useful for providing concise descriptions of system behavior, and have been successfully used as a language for goal definitions in task planning. Prior works on inferring temporal logic specifications have focused on ``summarizing'' the input dataset - i.e., finding specifications that are satisfied by all plan traces belonging to the given set. In this paper, we examine the problem of inferring specifications that describe temporal differences between two sets of plan traces. We formalize the concept of providing such contrastive explanations, then present BayesLTL - a Bayesian probabilistic model for inferring contrastive explanations as linear temporal logic (LTL) specifications. We demonstrate the robustness and scalability of our model for inferring accurate specifications from noisy data and across various benchmark planning domains.},
  isbn = {9780999241141},
  keywords = {Humans and AI: Human-AI Collaboration,Planning and Scheduling: Activity and Plan Recogni,Planning and Scheduling: Search in Planning and Sc},
  file = {/home/cameron/Zotero/storage/9WTH6YHT/Kim et al. - 2019 - Bayesian inference of linear temporal logic specifications for contrastive explanations.pdf}
}

@article{Kirkpatrick1983,
  title = {Optimization by {{Simulated Annealing}}},
  author = {Kirkpatrick, S. and Gelatt Jr, C. D. and Vecchi, M. P.},
  year = {1983},
  journal = {Science},
  volume = {220},
  number = {4598},
  pages = {671--680},
  doi = {DOI: 10.1126/science.220.4598.671},
  abstract = {There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods.},
  file = {/home/cameron/Zotero/storage/QJ4P78ZR/Kirkpatrick, Gelatt Jr, Vecchi - 1983 - Optimization by Simulated Annealing.pdf}
}

@article{Kirtley2003,
  title = {Permanent {{Magnet}} `` {{Brushless DC}} '' {{Motors}}},
  author = {Kirtley, J. L.},
  year = {2003},
  journal = {Introduction to Power Systems - Class Notes Chapter 12},
  pages = {32},
  abstract = {This document is a brief introduction to the design evaluation of permanent magnet motors, with an eye toward servo and drive applications.},
  file = {/home/cameron/Zotero/storage/6XBBI5M9/MIT6_061S11_ch12.pdf}
}

@article{Kishimoto2009,
  title = {Scalable, Parallel Best-First Search for Optimal Sequential Planning},
  author = {Kishimoto, Akihiro and Fukunaga, Alex and Botea, Adi},
  year = {2009},
  journal = {ICAPS 2009 - Proceedings of the 19th International Conference on Automated Planning and Scheduling},
  pages = {201--208},
  abstract = {Large-scale, parallel clusters composed of commodity processors are increasingly available, enabling the use of vast processing capabilities and distributed RAM to solve hard search problems. We investigate parallel algorithms for optimal sequential planning, with an emphasis on exploiting distributed memory computing clusters. In particular, we focus on an approach which distributes and schedules work among processors based on a hash function of the search state. We use this approach to parallelize the A* algorithm in the optimal sequential version of the Fast Downward planner. The scaling behavior of the algorithm is evaluated experimentally on clusters using up to 128 processors, a significant increase compared to previous work in parallelizing planners. We show that this approach scales well, allowing us to effectively utilize the large amount of distributed memory to optimally solve problems which require hundreds of gigabytes of RAM to solve. We also show that this approach scales well for a single, shared-memory multicore machine. Copyright \textcopyright{} 2009, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
  isbn = {9781577354062},
  file = {/home/cameron/Zotero/storage/2ZL5X3TW/b3d77f640577b1b636080d6d39931457b88e.pdf}
}

@article{Klein2010,
  title = {Rapid {{Decision Making}} on the {{Fire Ground}}: {{The Original Study Plus}} a {{Postscript}}},
  author = {Klein, Gary and Calderwood, Roberta and {Clinton-Cirocco}, Anne},
  year = {2010},
  journal = {Journal of Cognitive Engineering and Decision Making},
  volume = {4},
  number = {3},
  pages = {186--209},
  issn = {1555-3434},
  doi = {10.1518/155534310X12844000801203},
  abstract = {[This is an edited version of the original, unpublished 1985 study that identified recognition-primed decision making, with a new commentary added.] The objective of this study was to examine the way in which decisions are made by highly proficient personnel, under conditions of extreme time pressure, and in environments where the consequences of the decisions could affect lives and property. The domain of fire- fighting was selected, and the research focused on the decisions made by fire ground commanders (FGCs). Interviews were conducted with 26 experienced FGCs (mean experience of 23 years). Each interview covered a critical incident that was nonroutine and that demanded expertise. A total of 156 decision points were probed in this way. In less than 12\% of them was there any evidence of simultaneous comparisons and relative evaluation of two or more options. In over 80\% of the decision points, the strat- egy was for the FGCs to use their experience to directly identify the situation as typical of a standard prototype and to identify a course of action as typical for that prototype. In this way, the FGCs handled decision points without any need to consider more than one option. A recognition-primed decision (RPD) model was synthesized from these data, which emphasized the use of recognition rather than calculation or analysis for rapid decision making},
  isbn = {doi:10.1177/154193128603000616},
  pmid = {10220165},
  file = {/home/cameron/Zotero/storage/WRAP58UR/Klein2010_Rapid_Decision_Making_on_the_Fire_Gr.pdf}
}

@book{Kochenderfer2022,
  title = {Algorithms for Decision Making},
  author = {Kochenderfer, Mykel J. and Wheeler, Tim A. and Wray, Kyle H.},
  year = {2022},
  journal = {The proceedings of the programmed learning conference},
  abstract = {This book provides a broad introduction to algorithms for optimal decision mak- ing under uncertainty. We cover a wide variety of topics related to decision making, introducing the underlying mathematical problem formulations and the algorithms for solving them. Figures, examples, and exercises are provided to convey the intuition behind the various approaches. This text is intended for advanced undergraduates and graduate students as well as professionals. The book requires some mathematical maturity and assumes prior exposure to multivariable calculus, linear algebra, and probability concepts. Some review material is provided in the appendix. Disciplines where the book would be especially useful include mathematics, statistics, computer science, aerospace, electrical engineering, and operations research. Fundamental to this textbook are the algorithms, which are all implemented in the Julia programming language. We have found the language to be ideal for specifying algorithms in human readable form. The priority in the design of the algorithmic implementations was interpretability rather than efficiency. Indus- trial applications, for example, may benefit from alternative implementations. Permission is granted, free of charge, to use the code snippets associated with this book, subject to the condition that the source of the code is acknowledged. We anticipate that others may want to contribute translations of these algorithms to other programming languages. As translations become available, we will link to them from the book's webpage.},
  isbn = {978-0-262-04701-2},
  keywords = {ALGORITME,FONS,INCOMPLETE,INSTRUCTIES,INSTRUCTIONAL TEXT},
  file = {/home/cameron/Zotero/storage/G7YT4H8V/m-api-73271af2-a633-50af-02a4-61d0c2503f31.pdf}
}

@article{Koutra2011,
  title = {Unifying Guilt-by-Association Approaches: {{Theorems}} and Fast Algorithms},
  author = {Koutra, Danai and Ke, Tai You and Kang, U. and Chau, Duen Horng and Pao, Hsing Kuo Kenneth and Faloutsos, Christos},
  year = {2011},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {6912 LNAI},
  number = {PART 2},
  pages = {245--260},
  issn = {03029743},
  doi = {10.1007/978-3-642-23783-6_16},
  abstract = {If several friends of Smith have committed petty thefts, what would you say about Smith? Most people would not be surprised if Smith is a hardened criminal. Guilt-by-association methods combine weak signals to derive stronger ones, and have been extensively used for anomaly detection and classification in numerous settings (e.g., accounting fraud, cyber-security, calling-card fraud). The focus of this paper is to compare and contrast several very successful, guilt-by-association methods: Random Walk with Restarts, Semi-Supervised Learning, and Belief Propagation (BP). Our main contributions are two-fold: (a) theoretically, we prove that all the methods result in a similar matrix inversion problem; (b) for practical applications, we developed FaBP, a fast algorithm that yields 2\texttimes{} speedup, equal or higher accuracy than BP, and is guaranteed to converge. We demonstrate these benefits using synthetic and real datasets, including YahooWeb, one of the largest graphs ever studied with BP. \textcopyright{} 2011 Springer-Verlag.},
  isbn = {9783642237829},
  keywords = {Belief Propagation,inference,probabilistic graphical models,Random Walk with Restart,Semi-Supervised Learning},
  file = {/home/cameron/Zotero/storage/RWEACFB5/Koutra et al. - 2011 - Unifying guilt-by-association approaches Theorems and fast algorithms.pdf}
}

@article{Koutra2016,
  title = {{{DeltaCon}} : {{Principled Massive-Graph Similarity Function}} with {{Attribution}}},
  author = {Koutra, Danai and Shah, Neil and Vogelstein, Joshua and Gallagher, Brian and Faloutsos, Christos},
  year = {2016},
  journal = {ACM Transactions on Knowledge Discovery from Data},
  volume = {10},
  number = {3},
  pages = {1--43},
  issn = {1556-4681},
  doi = {10.1145/2888412},
  file = {/home/cameron/Zotero/storage/4MNEBRKT/Koutra et al. - 2016 - Delta Con Principled Massive-Graph Similarity Function with Attribution.pdf}
}

@article{Kumar1991,
  title = {Scalability of Parallel Algorithms for the All-Pairs Shortest-Path Problem},
  author = {Kumar, Vipin and Singh, Vineet},
  year = {1991},
  journal = {Journal of Parallel and Distributed Computing},
  volume = {13},
  number = {2},
  pages = {124--138},
  issn = {07437315},
  doi = {10.1016/0743-7315(91)90083-L},
  abstract = {This paper uses the isoefficiency metric to analyze the scalability of several parallel algorithms for finding shortest paths between all pairs of nodes in a densely connected graph. Parallel algorithms analyzed in this paper either have been previously presented elsewhere or are small variations of them. Scalability is analyzed with respect to mesh, hypercube, and shared-memory architectures. We demonstrate that isoefficiency functions are a compact and useful predictor of performance. In fact, previous comparative predictions of some of the algorithms based on experimental results are shown to be incorrect whereas isoefficiency functions predict correctly. We find the classic trade-offs of hardware cost vs time and memory vs time to be represented here as trade-offs of hardware cost vs scalability and memory vs scalability. \textcopyright{} 1991.},
  file = {/home/cameron/Zotero/storage/A5YR5TDX/Kumar, Singh - 1991 - Scalability of parallel algorithms for the all-pairs shortest-path problem.pdf}
}

@article{Kuwata2008,
  title = {Motion Planning in Complex Environments Using Closed-Loop Prediction},
  author = {Kuwata, Yoshiaki and Teo, Justin and Karaman, Sertac and Fiore, Gaston and Frazzoli, Emilio and How, Jonathan P.},
  year = {2008},
  journal = {AIAA Guidance, Navigation and Control Conference and Exhibit},
  doi = {10.2514/6.2008-7166},
  abstract = {This paper describes the motion planning and control subsystems of Team MIT's entry in the 2007 DARPA Grand Challenge. The novelty is in the use of closed-loop prediction in the framework of Rapidly-exploring Random Tree (RRT). Unlike the standard RRT, an input to the controller is sampled, followed by the forward simulation using the vehicle model and the controller to compute the predicted trajectory. This enables the planner to generate smooth trajectories much more efficiently, while the randomization allows the planner to explore cluttered environment. The controller consists of a Proportional-Integral speed controller and a nonlinear pure-pursuit steering controller, which are used both in execution and in the simulation-based prediction. The main advantages of the forward simulation are that it can easily incorporate any nonlinear control law and nonlinear vehicle dynamics, and the resulting trajectory is dynamically feasible. By using a stabilizing controller, it can handle vehicles with unstable dynamics. Several results obtained using MIT's race vehicle demonstrate these features of the approach. \textcopyright{} 2008 by the American Institute of Aeronautics and Astronautics, Inc.},
  isbn = {9781563479458},
  file = {/home/cameron/Zotero/storage/ZRMFFANZ/KuwataGNC08.pdf}
}

@article{kuznetz,
  title = {What ?},
  author = {Kuznetz, L and Nguyan, D and Srinivasen, R and Clancey, W and Dowding, J},
  file = {/home/cameron/Zotero/storage/8B9UV7R3/BioAdvisoryAlgorithmPres.pdf}
}

@article{kuznetz2007,
  title = {Funtional {{Requirements}} and {{Specifications}} for {{LEGACI}} ({{Lunar}}/{{Mars Exploration Guidance Algorithm}} and {{Consumables Interrogator}}) and {{VIOLET}} ({{Voice Initiated Operator}} for {{Lunar}}/{{Mars Exploration Tracking}})},
  author = {Kuznetz, Lawrence},
  year = {2007},
  number = {December},
  file = {/home/cameron/Zotero/storage/I7AXSHIB/Bioadvisory_Alg_Functional_Spec_26April2010.pdf}
}

@techreport{kuznetz2007a,
  title = {Lkuznetz {{Funtional Requirements}} and {{Specifications}} for {{LEGACI}} ({{Lunar}}/{{Mars Exploration Guidance Algorithm}} and {{Consumables Interrogator}}) and {{VIOLET}} ({{Voice Initiated Operator}} for {{Lunar}}/{{Mars Exploration Tracking}})},
  author = {Kuznetz, Lawrence},
  year = {2007},
  file = {/home/cameron/Zotero/storage/XVS2S93S/Bioadvisory_Alg_Functional_Spec_26April2010.pdf}
}

@techreport{kuznetza,
  title = {{{LEGACI}} and {{VIOLET}} \ldots.{{A}} Smart Suit Bioadvisory Algorithm for the {{Moon}} and {{Mars}}},
  author = {Kuznetz, L and Nguyan, Usra-Jsc D},
  file = {/home/cameron/Zotero/storage/RN74FCCN/BioAdvisoryAlgorithmPres.pdf}
}

@article{Lacotte,
  title = {All {{Local Minima}} Are {{Global}} for {{Two-Layer ReLU Neural Networks}} : {{The Hidden Convex Optimization Landscape}}},
  author = {Lacotte, Jonathan},
  eprint = {2006.05900v1},
  eprinttype = {arxiv},
  pages = {1--23},
  abstract = {We are interested in two-layer ReLU neural networks from an optimization per- spective. We prove that the path-connected sublevel set, i.e., valleys, of a neural network which is Clarke stationary with respect to the training loss with weight de- cay regularization contains a specific, simpler and more structured neural network, which we call its minimal representation. We provide an explicit construction of a continuous path between the neural network and its minimal counterpart. Impor- tantly, we show that characterizing the optimality properties of a neural network can be reduced to characterizing those of its minimal representation. Thanks to the specific structure of minimal neural networks, we show that we can embed them into a convex optimization landscape. Leveraging convexity, we are able to (i) characterize the minimal size of the hidden layer so that the neural network optimization landscape has no spurious valleys and (ii) provide a polynomial-time algorithm for checking if a neural network is a global minimum of the training loss. Overall, we provide a rich framework for studying the landscape of the neural network training loss through our embedding to a convex optimization landscape.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/9ZM28R5B/2006.05900.pdf}
}

@article{Lamport1978,
  title = {Time, {{Clocks}}, and the {{Ordering}} of {{Events}} in a {{Distributed System}}},
  author = {Lamport, Leslie},
  year = {1978},
  journal = {Communications of the ACM},
  volume = {21},
  number = {7},
  pages = {558--565},
  issn = {15577317},
  doi = {10.1145/359545.359563},
  abstract = {The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become. \textcopyright{} 1978, ACM. All rights reserved.},
  keywords = {clock synchronization,computer networks,distributed systems,multiprocess systems},
  file = {/home/cameron/Zotero/storage/6JY3WGSG/Lamport - 1978 - Time, Clocks, and the Ordering of Events in a Distributed System.pdf}
}

@article{Lamport1984,
  title = {Using {{Time Instead}} of {{Timeout}} for {{Fault-Tolerant Distributed Systems}}},
  author = {Lamport, Leslie},
  year = {1984},
  journal = {ACM Transactions on Programming Languages and Systems (TOPLAS)},
  volume = {6},
  number = {2},
  pages = {254--280},
  issn = {15584593},
  doi = {10.1145/2993.2994},
  abstract = {A general method is described for implementing a distributed system with any desired degree of faulttolerance. Instead of relying upon explicit timeouts, processes execute a simple clock-driven algorithm. Reliable clock synchronization and a solution to the Byzantine Generals Problem are assumed. \textcopyright{} 1984, ACM. All rights reserved.},
  keywords = {Byzantine Generals Problem,Clocks,intractive consistency,timestamps,transaction commit},
  file = {/home/cameron/Zotero/storage/NQ2C8NQT/Lamport - 1984 - Using Time Instead of Timeout for Fault-Tolerant Distributed Systems.pdf}
}

@article{Lamport1998,
  title = {The {{Part-Time Parliment}}},
  author = {Lamport, Leslie and Van Renesse, Robbert and Schiper, Nicolas and Schneider, Fred B. and Ailijiang, Ailidani and Charapko, Aleksey and Demirbas, Murat and Pedone, Fernando and Cavin, David and Dolev, Danny and Dwork, Cynthia and Stockmeyer, Larry and Ko{\'n}czak, Jan and Santos, Nuno and Lamport, Leslie and {Leslie} and Dutta, Partha and Guerraoui, Rachid and Lamport, Leslie and Martin, J P and Alvisi, L and Keidar, Idit and Shraer, Alexander and Borran, F and Hutle, M and Santos, Nuno and Schiper, Andr{\'e} Andre and Chandra, Tushar Deepak and Toueg, Sam and Dwork, Cynthia and Lynch, Nancy A and Stockmeyer, Larry and Chandra, Tushar Deepak and Hadzilacos, Vassos and Toueg, Sam and Gafni, Eli and Dutta, Partha and Guerraoui, Rachid and Raynal, Michel and Lamport, Leslie and Boichat, Romain and Dutta, Partha and Fr{\o}lund, Svend and Guerraoui, Rachid and Zielinski, P and Keidar, Idit and Rajsbaum, Sergio and Guerraoui, Rachid and Schiper, Andr{\'e} Andre and Fischer, Michael J and Lynch, Nancy A and Paterson, Michael S and {Charron-Bost}, Bernadette and Schiper, Andr{\'e} Andre and Pease, Marshall and Shostak, Robert and Lamport, Leslie and Aspnes, James and Herlihy, Maurice and Elrad, Tzilla and Francez, Nissim and Van Renesse, Robbert and Altinbuken, Deniz and {Olfati-Saber}, Reza and Murray, ROSRM Richard M and Kuhn, Fabian and Oshman, Rotem and Moses, Yoram and Murray, ROSRM Richard M and Wang, Long and Xiao, Feng and Xie, Guangming and Wang, Long and Li, Zhongkui and Duan, Zhisheng and Chen, Guanrong and Huang, Lin and {Olfati-Saber}, Reza and Fax, J Alex and Murray, ROSRM Richard M and Sun, Yuan Gong and Wang, Long and Xie, Guangming and Spanos, Demetri P and {Olfati-Saber}, Reza and Murray, ROSRM Richard M and Lamport, Leslie and Shostak, Robert and Pease, Marshall and Vaidya, Nitin H and Tseng, Lewis and Liang, Guanfeng and Attiya, Chagit and Dolev, Danny and Gil, Joseph and Correia, Miguel and Veronese, Giuliana Santos and Neves, Nuno Ferreira and Verissimo, Paulo and Most\&\#x000E9, Achour and {faoui} and Moumen, Hamouma and Raynal, Michel and Rabin, Michael O and Jeanneau, D and Rieutord, T and Arantes, L and Sens, P and Qin, Jiahu and Ma, Qichao and Shi, Yang and Wang, Long and Keidar, Idit and Rajsbaum, Sergio and Lynch, Nancy A and Alagappan, Ramnatthan and Ganesan, Aishwarya and Lee, Eric and Albarghouthi, Aws and Chidambaram, Vijay and {Arpaci-Dusseau}, Andrea C and {Arpaci-Dusseau}, Remzi H and Sutra, Pierre and Shapiro, Marc and Moraru, Iulian and Andersen, David G and Kaminsky, Michael and {Yanhua} and Mao, Flavio and {P.} and Junqueria, Keith and {Marzullo} and Lorch, Jacob R and Adya, Atul and Bolosky, William J and Chaiken, Ronnie and Douceur, John R and Howell, Jon and Ongaro, Diego and Ousterhout, John K and Batra, Rahul and Katti, Amogh and Di Fatta, Giuseppe and Naughton, Thomas and Engelmann, Christian and Lilja, David J and {de Camargo}, Edson Tavares and Duarte, Elias P and Pedone, Fernando and Poke, Marius and Hoefler, Torsten and Glass, Colin W and Herault, Thomas and Bouteiller, Aurelien and Bosilca, George and Gamell, Marc and Teranishi, Keita and Parashar, Manish and Dongarra, Jack and Buntinas, Darius and Baumann, Robert C and Kaul, Himanshu and Anders, Mark and Hsu, Steven and Agarwal, Amit and Krishnamurthy, Ram and Borkar, Shekhar and Cappello, Franck and Geist, Al and Gropp, William and Kale, Sanjay and Kramer, Bill and Snir, Marc and Bergman, Keren and Borkar, Shekhar and Campbell, Dan and Carlson, William and Dally, William and Denneau, Monty and Franzon, Paul and Harrod, William and Hill, Kerry and Hiller, Jon and others and Dai, X and Wang, J M and Bensaou, B and Xu, X and Dou, W and Zhang, X and Chen, J and Yang, Y and Chang, X and Liu, J and Li, L and Goudarzi, H and Pedram, M and Khoshkholghi, M A and Derahman, M N and Abdullah, A and Subramaniam, S and Othman, M and Beloglazov, A and Buyya, R and Dabbagh, M and Hamdaoui, B and Guizani, M and Rayes, A and Mashayekhy, L and Nejad, M M and Grosu, D and Mostefaoui, A and Rennes, Universite De and Beaulieu, Campus De and Cedex, Rennes and Raynal, Michel and Rennes, Universite De and Beaulieu, Campus De and Cedex, Rennes and Gao, G and Keidar, Idit and Shraer, Alexander and Chandra, Tushar Deepak and Griesemer, Robert and Redstone, Joshua and Dutta, Partha and Guerraoui, Rachid and Keidar, Idit and Alistarh, Dan and Gilbert, Seth and Guerraoui, Rachid and Travers, Corentin},
  year = {1998},
  journal = {ACM Transactions on Computer Systems},
  volume = {16},
  number = {2},
  eprint = {1309.5671},
  eprinttype = {arxiv},
  pages = {373--386},
  issn = {2168-7161},
  doi = {10.1145/568425.568433},
  abstract = {As used in practice, traditional consensus algorithms require three message delays before any process can learn the chosen value. Fast Paxos is an extension of the classic Paxos algorithm that allows the value to be learned in two message delays. How and why the algorithm works are explained informally, and a TLA+ specification of the algorithm appears as an appendix.},
  archiveprefix = {arXiv},
  isbn = {0178-2770},
  keywords = {Adaptation models,aggressive VM consolidation,agreement problem,Algorithm design and analysis,Analytical models,approximate winner determination algorithm,Approximation algorithms,Approximation methods,approximation theory,asynchronous Byzantine consensus,asynchronous consensus protocols,asynchronous distributed system,Asynchronous message-passing system,asynchronous systems,atomic broadcast,auction-based setting,Bandwidth,bandwidth constraints,benchmark algorithm,benchmark algorithms,benchmark testing,broadcast abstraction,Broadcasting,Byzantine failures,Byzantine fault tolerance,Byzantine Generals' problem,Byzantine process,Byzantine settings,carbon dioxide emissions,CDC long-term energy consumption,CDC network scale,CDC network virtualization,CDC physical resource allocation,centralized management solution,Clocks,cloud computing,Cloud computing,Cloud Computing,cloud computing platforms,cloud computing systems,cloud data center,Cloud data center,cloud data centers,cloud datacenter,cloud environment,cloud platform expansion,cloud provider,cloud providers,cloud service provider,Clouds,Clustering algorithms,commit problem,common coin,communication predicates,communication steps,Communication system software,computation-efficient embedding algorithms,computational complexity,Computational modeling,computer centres,Computer crashes,consensus,Consensus,Consensus algorithms,Consensus in the Cloud: Paxos Systems Demystified,consensus problem,consensus.,Containers,contracts,Contracts,cooling,Cooling,cooling power consumption,Cost accounting,CPU,crash failure,crash failures,CSP long-term revenue,data center,Data center,Data centers,data clustering,Data Clustering,Data models,decision making,decision making complexity,Delay,Detection algorithms,Detectors,distributed agreement protocols,distributed algorithm,distributed algorithms,Distributed algorithms,Distributed computing,distributed processing,Distributed processing,distributed system,distributed systems,Distributed systems,dynamic consolidation,dynamic deployment,Dynamic networks,dynamic virtual machine consolidation,dynamic VM consolidation,electrical energy,energy conservation,energy consumption,Energy consumption,energy consumption model,energy consumption reduction,energy efficiency,Energy efficiency,Energy Efficiency,energy efficient resource management,energy nonproportionality,Energy-aware method,energy-aware resource allocation method,energy-efficient algorithms,energy-efficient electronic devices,energy-efficient embedding algorithms,energy-efficient operation,energy-efficient virtual machine scheduling,energy-performance tradeoff,EnReal,estimation theory,eventual synchrony,Eventual Synchrony,experiences,failure detection,failure detectors,Failure detectors,fault tolerance,Fault tolerance,fault tolerant computing,Fault tolerant systems,fault-tolerance,fault-tolerant distributed computing,file servers,FT middleware,global information technology infrastructure,globally distributed resources,Google,greedy algorithms,greedy approximation algorithms,green computing,green virtual cloud data center provisioning,guarantee-based SLA,heavy-load CDCs,Helium,Heuristic algorithms,hierarchical service level agreement-based resourc,hierarchical SLA-based resource management solutio,hierarchical SLA-driven resource management,hierarchical structure,high-operational costs,high-quality service,host overload detection,idle nodes,implementation,Indexes,indulgent consensus,integer programming,integer programming problem,k-Set agreement,Laboratories,large-scale computing,large-scale data centers,leader oracle,live migration,lower bounds,Markov chain model,Markov processes,mean intermigration time,Measurement,mechanism design,mechanism design approach,Memory management,message passing,MinCS,minimum communication virtual machine scheduling a,minimum energy virtual machine scheduling algorith,mixed integer programming,Monitoring,multisize sliding window workload estimation techn,multitenant data centers,Network topology,node heterogeneity,nonsynchronous period,NP-hard problem,operational cost,optimal offline algorithm,optimal resilience,optimistically terminating consensus,overloaded host management,partial synchronous system,partial synchrony,Pathology,Paxos,payment function,peak power-aware operation,performance degradation,performance evaluation,physical machine estimation,physical machine resource management,PlanetLab VMs,PM estimation,Ports (Computers),power aware computing,power consumption reduction,Power demand,Pricing,processor failures,processor scheduling,Proposals,protocols,Protocols,QoS goal,quality of service,Quality of service,quality of service constraints,quantitative analysis,RAM,randomized algorithm,Real time systems,reliability,replicated state machine,resource allocation,resource management,Resource management,resource provisioning,resource shortages,resource utilization,resource utilization optimization,round-based consensus algorithm,round-based model,Safety,scheduling,scientific information systems,scientific workflow,scientific workflow executions,server overloads,Servers,service level agreement,service-level agreement,signature-free algorithm,simplicity,SLA constraints,sleep mode,Software performance,Stability,statistical analysis,strategy-proof mechanism,Switches,synchronisation,Synchronization,Synchrony assumptions,system modeling.,Systems specification methodology,tenant service level agreements,Time-varying systems,Timing,tolerance,Topology,two-step consensus protocol,unknown nonstationary workloads,Upper bound,VDC acceptance ratio,VDC embedding algorithms,VDC virtual links,virtual data center embedding,virtual machine,virtual machine consolidation,virtual machine placement,virtual machine scheduling,virtual machines,Virtual machining,virtualisation,virtualization,VM,VM instances,VM MinES,VM provision,VM reallocation,Wiener filtering,Wiener Filtering,workload prediction,Workload Prediction},
  file = {/home/cameron/Zotero/storage/V94HKFED/lamport-paxos.pdf}
}

@article{Lattimore2018,
  title = {Refining the Confidence Level for Optimistic Bandit Strategies},
  author = {Lattimore, Tor},
  year = {2018},
  journal = {Journal of Machine Learning Research},
  volume = {19},
  pages = {1--32},
  issn = {15337928},
  abstract = {This paper introduces the first strategy for stochastic bandits with unit variance Gaussian noise that is simultaneously minimax optimal up to constant factors, asymptotically optimal, and never worse than the classical upper confidence bound strategy up to universal constant factors. Preliminary empirical evidence is also promising. Besides this, a conjecture on the optimal form of the regret is shown to be false and a finite-time lower bound on the regret of any strategy is presented that very nearly matches the finite-time upper bound of the newly proposed strategy.},
  keywords = {Regret minimisation,Sequential decision making,Stochastic bandits},
  file = {/home/cameron/Zotero/storage/MSIVDBXG/Lattimore - 2018 - Refining the confidence level for optimistic bandit strategies.pdf}
}

@article{Leaute2005,
  title = {Coordinating Agile Systems through the Model-Based Execution of Temporal Plans},
  author = {L{\'e}aut{\'e}, Thomas and Williams, Brian C.},
  year = {2005},
  journal = {Proceedings of the National Conference on Artificial Intelligence},
  volume = {1},
  pages = {114--120},
  abstract = {Agile autonomous systems arc emerging, such as unmanned aerial vehicles (UAVs), that must robustly perform tightly coordinated time-critical missions; for example, military surveillance or search-and-rescue scenarios. In the space domain, execution of temporally flexible plans has provided an enabler for achieving the desired coordination and robustness. We address the challenge of extending plan execution to under-actuated systems that are controlled indirectly through the setting of continuous state variables. Our solution is a novel model-based executive that takes as input a temporally flexible state plan, specifying intended state evolutions, and dynamically generates a near-optimal control sequence. To achieve optimality and safety, the executive plans into the future, framing planning as a disjunctive programming problem. To achieve robustness to disturbances and tractability, planning is folded within a receding horizon, continuous planning framework. Key to performance is a problem reduction method based on constraint pruning. We benchmark performance through a suite of UAV scenarios using a hardware-in-the-loop testbed. Copyright \textcopyright{} 2005, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.},
  file = {/home/cameron/Zotero/storage/MX3UR64H/lec-05-reading-1.pdf}
}

@article{Lecun2015,
  title = {Deep Learning},
  author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  journal = {Nature},
  volume = {521},
  number = {7553},
  eprint = {1312.6184v5},
  eprinttype = {arxiv},
  pages = {436--444},
  issn = {14764687},
  doi = {10.1038/nature14539},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  archiveprefix = {arXiv},
  isbn = {9780521835688},
  pmid = {10463930},
  file = {/home/cameron/Zotero/storage/TU6Y4E8H/DeepLearning_LeCun.pdf}
}

@article{lecun2015,
  title = {Deep Learning},
  author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  journal = {Nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {{Nature Publishing Group}},
  issn = {14764687},
  doi = {10.1038/nature14539},
  abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
  pmid = {26017442},
  file = {/home/cameron/Zotero/storage/DCYKFL6C/DeepLearning_LeCun.pdf}
}

@article{Leger2005,
  title = {Mars {{Exploration Rover}} Surface {{Operations}}: {{Driving Spirit}} at {{Gusev Crater}}},
  author = {Leger, P. Chris and {Trebi-Ollennu}, Ashitey and Wright, John R. and Maxwell, Scott A. and Bonitz, Robert G. and Biesiadecki, Jeffrey J. and Hartman, Frank R. and Cooper, Brian K. and Baumgartner, Eric T. and Maimone, Mark W.},
  year = {2005},
  journal = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
  volume = {2},
  pages = {1815--1822},
  issn = {1062922X},
  doi = {10.1109/icsmc.2005.1571411},
  abstract = {Spirit is one of two rovers that landed on Mars in January 2004 as part of NASA's Mars Exploration Rover mission. As of July 2005, Spirit has traveled over 4.5 kilometers across the Martian surface while investigating rocks and soils, digging trenches to examine subsurface materials, and climbing hills to reach outcrops of bedrock. Originally designed to last 90 sols (Martian days), Spirit has survived over 500 sols of operation and continues to explore. During the mission, we achieved increases in efficiency, accuracy, and traverse capability through increasingly complex command sequences, growing experience, and updates to the on-board and ground-based software. Safe and precise mobility on slopes and in the presence of obstacles has been a primary factor in development of new software and techniques. \textcopyright 2005 IEEE.},
  keywords = {Mars,MER,Mobility,Planetary robotics,Rovers},
  file = {/home/cameron/Zotero/storage/N4UT8IK2/leger_driving05.pdf}
}

@phdthesis{Levine2019,
  title = {Risk-Bounded {{Coordination}} of {{Human-Robot Teams}} through {{Concurrent Intent Recognition}} and {{Adaptation}}},
  author = {Levine, Steven James},
  year = {2019},
  abstract = {There is an ever-growing demand for humans and robots to work fluidly together in a number of important domains, such as home care, manufacturing, and medical robotics. In order to achieve this fluidity, robots must be able to (1) recognize their human teammate's intentions, and (2) automatically adapt to those intentions in an intelligent manner. This thesis makes progress in these areas by proposing a framework that solves these two problems (task-level intent recognition and robotic adaptation) concurrently and holistically, using a single model and set of algorithms for both. The result is a mixed-initiative human-robot interaction that achieves the team's goals. The robot is able to reason about the action requirements, timing constraints, and unexpected disturbances in order to adapt intelligently to the human. We extend this framework by additionally maintaining a probabilistic belief over the human's intentions. We develop a risk-aware executive that performs concurrent intent recognition and adaptation. Our executive continuously assesses the risk asso- ciated with plan execution, selects adaptations that are safe enough, asks uncertainty- reducing questions when appropriate, and provides a proactive early warning of likely failure. Finally, we present an extension to this work which enables the robot to save time by ignoring potentially many, vanishingly-unlikely scenarios. To achieve this behavior, we frame concurrent intent recognition and adaptation as a constraint satisfaction problem, and compactly represent their associated solutions and policies using compiled structures that are updated online as new observations arise. Through the use of these compiled structures, the robot efficiently reasons about which actions to perform, as well as when to perform them \textendash{} thereby ensuring decision making consistent with the team's goals.},
  school = {Massachusetts Institute of Technology},
  file = {/home/cameron/Zotero/storage/RTE7J6KU/sjlevine_PhDThesis.pdf}
}

@article{Li2008,
  title = {Generative Planning for Hybrid Systems Based on {{Flow Tubes}}},
  author = {Li, Hui X. and Williams, Brian C.},
  year = {2008},
  journal = {ICAPS 2008 - Proceedings of the 18th International Conference on Automated Planning and Scheduling},
  number = {Icaps},
  pages = {206--213},
  abstract = {When controlling an autonomous system, it is inefficient or sometimes impossible for the human operator to specify detailed commands. Instead, the field of AI autonomy has developed goal-directed systems, in which human operators specify a series of goals to be accomplished. Increasingly, the control of autonomous systems involves performing a mix of discrete and continuous actions. For example, a typical autonomous underwater vehicle (AUV) mission involves discrete actions, like get GPS and set sonar, and continuous actions, like descend and ascend, which involve continuous dynamics of the vehicle. Accordingly, we develop a hybrid planner that determines a series of discrete and continuous actions that achieve the mission goals. In this paper, we describe a novel approach to solving the generative planning problem for hybrid systems, involving both continuous and discrete actions. The planner, Kongming 1, incorporates two innovations. First, it employs a compact representation of all hybrid plans, called a Hybrid Flow Graph, which combines the strengths of a Planning Graph for discrete actions and Flow Tubes for continuous actions. Second, it encodes the Hybrid Flow Graph as a mixed logic linear/nonlinear program, which it solves using an off-theshelf solver. We empirically demonstrate that Kongming can efficiently plan for real-world scenarios that are based on science missions performed at the Monterey Bay Aquarium Research Institute (MBARI). Copyright \textcopyright{} 2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
  isbn = {9781577353867},
  file = {/home/cameron/Zotero/storage/FMD846KQ/ICAPS08-026.pdf}
}

@article{Liao2015,
  title = {Minimizing {{Movement}} for {{Target Coverage}} and {{Network Connectivity}} in {{Mobile Sensor Networks}}},
  author = {Liao, Zhuofan and Wang, Jianxin and Zhang, Shigeng and Cao, Jiannong and Min, Geyong},
  year = {2015},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  volume = {26},
  number = {7},
  pages = {1971--1983},
  publisher = {{IEEE}},
  issn = {10459219},
  doi = {10.1109/TPDS.2014.2333011},
  abstract = {Coverage of interest points and network connectivity are two main challenging and practically important issues of Wireless Sensor Networks (WSNs). Although many studies have exploited the mobility of sensors to improve the quality of coverage andconnectivity, little attention has been paid to the minimization of sensors' movement, which often consumes the majority of the limited energy of sensors and thus shortens the network lifetime significantly. To fill in this gap, this paper addresses the challenges of the Mobile Sensor Deployment (MSD) problem and investigates how to deploy mobile sensors with minimum movement to form a WSN that provides both target coverage and network connectivity. To this end, the MSD problem is decomposed into two sub-problems: the Target COVerage (TCOV) problem and the Network CONnectivity (NCON) problem. We then solve TCOV and NCON one by one and combine their solutions to address the MSD problem. The NP-hardness of TCOV is proved. For a special case of TCOV where targets disperse from each other farther than double of the coverage radius, an exact algorithm based on the Hungarian method is proposed to find the optimal solution. For general cases of TCOV, two heuristic algorithms, i.e., the Basic algorithm based on clique partition and the TV-Greedy algorithm based on Voronoi partition of the deployment region, are proposed to reduce the total movement distance ofsensors. For NCON, an efficient solution based on the Steiner minimum tree with constrained edge length is proposed. Thecombination of the solutions to TCOV and NCON, as demonstrated by extensive simulation experiments, offers a promising solutionto the original MSD problem that balances the load of different sensors and prolongs the network lifetime consequently.},
  keywords = {connectivity,energy consumption,mobile sensors,target coverage,Wireless sensor networks},
  file = {/home/cameron/Zotero/storage/FB9GR5TB/06846302.pdf}
}

@article{Lieberman2015,
  title = {Breaking {{Bias Updated}}: The {{SEEDS}} Model},
  author = {Lieberman, Matthew D and Rock, David and Halvorson, Heidi Grant and Cox, Christine},
  year = {2015},
  journal = {Neuroleadership Journal},
  volume = {6},
  number = {November},
  pages = {4--18},
  abstract = {Despite decades of effort and major investment dedicated to reducing bias in organizational settings, it persists. The central challenge in removing bias from decisions is that most biases operate unconsciously. While raising awareness can help people to realize that they might be biased, it does not enable them to recognize bias in their own thinking\textemdash we simply do not have conscious access to the operations of bias in the brain.In this paper, we propose an alternative solution to mitigating bias, derived from a brain-based perspective. We identify processes that can interrupt and redirect unconsciously biased thinking. We provide The SEEDS ModelTM for designing and guiding the use of such processes. The SEEDSModelTM simplifies the roughly 150 identified cognitive biases and recognizes five categories of bias, each category responsive to a different set of actions that will help mitigate them. To use The SEEDS ModelTM, we propose three steps: 1. Accept that we are biased by virtue of our biology; 2. Label the type of bias that might influence a particular decision, using The SEEDSModelTM;3. Mitigate using the right process.},
  file = {/home/cameron/Zotero/storage/QR8VVHMN/Lieberman et al. - 2015 - Breaking Bias Updated the SEEDS model.pdf}
}

@book{Lim2004,
  title = {Solving the Crane Scheduling Problem Using Intelligent Search Schemes (Extended Abstract)},
  author = {Lim, Andrew and Rodrigues, Brian and Xu, Zhou},
  year = {2004},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {3258},
  issn = {03029743},
  doi = {10.1007/978-3-540-30201-8_59},
  isbn = {3-540-23241-9},
  file = {/home/cameron/Zotero/storage/DVX5LW4V/2004_Book_PrinciplesAndPracticeOfConstra.pdf}
}

@article{Lim2019,
  title = {Designing and {{Developing Mission Elements}} in {{Support}} of {{Human Scientific Exploration}} of {{Mars}}},
  author = {Lim, Darlene S.S. and Abercromby, Andrew F.J. and Nawotniak, Shannon E Kobs and Lees, David S and Miller, Matthew J. and Brady, Allyson L. and Mirmalek, Zara and Sehlke, Alexander and Payler, Samuel J. and Stevens, Adam H. and Haberle, Christopher W and Beaton, Kara H. and Chappell, Steven P. and Hughes, Scott S. and Cockell, Charles S},
  year = {2019},
  volume = {19},
  number = {3},
  pages = {245--259},
  doi = {10.1089/ast.2018.1869},
  isbn = {2015082018},
  keywords = {245,259,analog,astrobiology 19,basalt,mars,operations,science,spaceflight},
  file = {/home/cameron/Zotero/storage/9W4KATNR/ast.2018.1869.pdf}
}

@article{Liu2018,
  title = {Accurate Real-Time Ball Trajectory Estimation with Onboard Stereo Camera System for Humanoid Ping-Pong Robot},
  author = {Liu, Yong and Liu, Liang},
  year = {2018},
  journal = {Robotics and Autonomous Systems},
  volume = {101},
  pages = {34--44},
  publisher = {{Elsevier B.V.}},
  issn = {09218890},
  doi = {10.1016/j.robot.2017.12.004},
  abstract = {In this paper, an accurate real-time ball trajectory estimation approach working on the onboard stereo camera system for the humanoid ping-pong robot has been presented. As the asynchronous observations from different cameras will great reduce the accuracy of the trajectory estimation, the proposed approach will main focus on increasing the estimation accuracy under those asynchronous observations via concerning the flying ball's motion consistency. The approximate polynomial trajectory model for the flying ball is built to optimize the best parameters from the asynchronous observations in each discrete temporal interval. The experiments show the proposed approach can performance much better than the method that ignores the asynchrony and can achieve the similar performance as the hardware-triggered synchronizing based method, which cannot be deployed in the real onboard vision system due to the limited bandwidth and real-time output requirement.},
  keywords = {Humanoid ping-pong robot,Onboard vision,Trajectory estimation},
  file = {/home/cameron/Zotero/storage/JQXGTEN6/Liu, Liu - 2018 - Accurate real-time ball trajectory estimation with onboard stereo camera system for humanoid ping-pong robot.pdf}
}

@inproceedings{Liu2019,
  title = {Trust-{{Aware Behavior Reflection}} for {{Robot Swarm Self-Healing}} {${_\ast}$}},
  booktitle = {{{AAMAS}}},
  author = {Liu, Rui and Jia, Fan and Luo, Wenhao and Chandarana, Meghan and Nam, Changjoo and Lewis, Michael and Sycara, Katia},
  year = {2019},
  pages = {122--130},
  address = {{Montr\'eal, Canada}},
  abstract = {The deployment ofrobot swarms is influenced by real-world factors, such as motor issues, sensor failure, and wind disturbances. These factors cause the appearance of faulty robots. In a decentralized swarm, sharing incorrect information from faulty robots will lead to undesired swarm behaviors, such as swarm disconnection and incorrect heading directions. We envision a system where a human operator is exerting supervisory control over a remote swarm by indicating changes in trust to the swarm via a "trust-signal". By cor- recting faulty behaviors, trust between the human and the swarm is maintained to facilitate human-swarm cooperation. In this research, a trust-aware behavior reflection method \textendash{} Trust-R \textendash{} is designed based on a weighted mean subsequence reduced algorithm (WMSR). By using Trust-R, detected faulty behaviors are automatically cor- rected by the swarm in a decentralized fashion by referring to the motion status of their trusted neighbors and isolating failed robots from the others. Based on real-world scenarios, three types of robot faults \textendash{} degraded performance caused by motor wear, abnormal motion caused by system uncertainty and motion deviation caused by an external disturbance such as wind \textendash{} were simulated to test the effectiveness of Trust-R. Results show that Trust-R is effective in correcting swarm behaviors for swarm self-healing.},
  keywords = {acm reference format,behavior reflection,Behavior Reflection,changjoo nam,fan jia,meghan chandarana,michael,rui liu,swarm self-healing,Swarm Self-Healing,trust,Trust,trust-r,Trust-R,wenhao luo,wmsr,WMSR},
  file = {/home/cameron/Zotero/storage/CN3PFZ4T/p122.pdf}
}

@article{Logenthiran2008,
  title = {Multi-Agent Coordination for {{DER}} in Microgrid},
  author = {Logenthiran, T. and Srinivasan, Dipti and Wong, David},
  year = {2008},
  journal = {2008 IEEE International Conference on Sustainable Energy Technologies, ICSET 2008},
  pages = {77--82},
  publisher = {{IEEE}},
  doi = {10.1109/ICSET.2008.4746976},
  abstract = {Multi-agent system (MAS) is one of the most exciting and the fastest growing domains in agent oriented technology which deals with modeling of autonomous decision making entities. This paper presents an application of MAS for distributed energy resource (DER) management in a MicroGrid. MicroGrid can be defined as low voltage distributed power networks comprising various distributed generators (DG), storage and controllable loads, which can be operated as interconnected or as islands from the main power grid. By representing each element in MicroGrid as an autonomous intelligent agent, multi agent modeling of a MicroGrid is designed and implemented. JADE framework is proposed for the modeling and reliability of the MicroGrid is confirmed with PowerWorld Simulator. Further, the FIPA contract net coordination between the agents is demonstrated through software simulation. As a result, this paper provides a MicroGrid modeling which has the necessary communication and coordination structure to create a scalable system. The optimized MicroGrid management and operations can be developed on it in future. \textcopyright{} 2008 IEEE.},
  isbn = {9781424418886},
  file = {/home/cameron/Zotero/storage/C57TAYLZ/04746976.pdf}
}

@article{Lopez2011,
  title = {A Framework for Building Mobile Single and Multi-Robot Applications},
  author = {L{\'o}pez, Joaquin and P{\'e}rez, Diego and Zalama, Eduardo},
  year = {2011},
  journal = {Robotics and Autonomous Systems},
  volume = {59},
  number = {3-4},
  pages = {151--162},
  publisher = {{Elsevier B.V.}},
  issn = {09218890},
  doi = {10.1016/j.robot.2011.01.004},
  abstract = {The complexity of robot software systems calls for the use of a well-conceived architecture together with programming tools to support it. One common feature of robot architectures is the modular decomposition of systems into simpler and largely independent components. These components implement primitive actions and report events about their state. The robot programming framework proposed here includes a tool (RoboGraph) to program and coordinate the activity (tasks) of these middleware modules. Project developers use the same task programming IDE (RoboGraph) on two different levels. The first is to program tasks that must be executed autonomously by one robot and the second is to program tasks that can include several robots and building elements. Tasks are described using a Signal Interpreted Petri Net (SIPN) editor and stored in an xml file. A dispatcher loads these files and executes the different Petri nets as needed. A monitor that shows the state of all the running nets is very useful for debugging and tracing purposes. The whole system has been used in several applications: A tour-guide robot (GuideBot), a multi-robot surveillance project (WatchBot) and a hospital food and laundry transportation system based on mobile robots. \textcopyright{} 2011 Elsevier B.V. All rights reserved.},
  keywords = {Mobile robot,Petri net,Robot control architecture,Robot programming framework},
  file = {/home/cameron/Zotero/storage/SUJN6R5G/1-s2.0-S092188901100011X-main.pdf}
}

@article{Love2013,
  title = {Delayed Voice Communication},
  author = {Love, Stanley G. and Reagan, Marcum L.},
  year = {2013},
  journal = {Acta Astronautica},
  volume = {91},
  pages = {89--95},
  publisher = {{Elsevier}},
  issn = {00945765},
  doi = {10.1016/j.actaastro.2013.05.003},
  abstract = {We present results from simulated deep-space exploration missions that investigated voice communication with significant time delays. The simulations identified many challenges: confusion of sequence, blocked calls, wasted crew time, impaired ability to provide relevant information to the other party, losing track of which messages have reached the other party, weakened rapport between crew and ground, slow response to rapidly changing situations, and reduced situational awareness. These challenges were met in part with additional training; greater attention and foresight; longer, less frequent transmissions; meticulous recordkeeping and timekeeping; and specific alerting and acknowledging calls. Several simulations used both delayed voice and text messaging. Text messaging provided a valuable record of transmissions and allowed messages to be targeted to subsets of the flight and ground crew, but it was a poor choice for high-workload operators such as vehicle drivers and spacewalkers. Even with the foregoing countermeasures, delayed voice communication is difficult. Additional aids such as automatic delay timers and voice-to-text transcription would help. Tests comparing delays of 50 and 300 s unexpectedly revealed that communicating with the shorter delay was just as challenging as with the longer one. \textcopyright{} 2013 Published by Elsevier Ltd. on behalf of IAA.},
  keywords = {Deep space exploration,Delayed voice communication,Human space flight,Space flight analog,Speed-of-light delay,Voice communication},
  file = {/home/cameron/Zotero/storage/U63T32AS/delayed-voice-communication.pdf}
}

@article{love2013,
  title = {Delayed Voice Communication},
  author = {Love, Stanley G. and Reagan, Marcum L.},
  year = {2013},
  journal = {Acta Astronautica},
  volume = {91},
  pages = {89--95},
  issn = {00945765},
  doi = {10.1016/j.actaastro.2013.05.003},
  abstract = {We present results from simulated deep-space exploration missions that investigated voice communication with significant time delays. The simulations identified many challenges: confusion of sequence, blocked calls, wasted crew time, impaired ability to provide relevant information to the other party, losing track of which messages have reached the other party, weakened rapport between crew and ground, slow response to rapidly changing situations, and reduced situational awareness. These challenges were met in part with additional training; greater attention and foresight; longer, less frequent transmissions; meticulous recordkeeping and timekeeping; and specific alerting and acknowledging calls. Several simulations used both delayed voice and text messaging. Text messaging provided a valuable record of transmissions and allowed messages to be targeted to subsets of the flight and ground crew, but it was a poor choice for high-workload operators such as vehicle drivers and spacewalkers. Even with the foregoing countermeasures, delayed voice communication is difficult. Additional aids such as automatic delay timers and voice-to-text transcription would help. Tests comparing delays of 50 and 300 s unexpectedly revealed that communicating with the shorter delay was just as challenging as with the longer one. \textcopyright{} 2013 Published by Elsevier Ltd. on behalf of IAA.},
  keywords = {Deep space exploration,Delayed voice communication,Human space flight,Space flight analog,Speed-of-light delay,Voice communication},
  file = {/home/cameron/Zotero/storage/EBGGFWVB/delayed-voice-communication.pdf}
}

@article{love2019,
  title = {The {{Future}} of {{Mars}} , on {{Earth Today}}},
  author = {Love, Stanley G},
  year = {2019},
  volume = {19},
  number = {3},
  pages = {243--244},
  doi = {10.1089/ast.2018.1863},
  keywords = {analog,communication latency,exploration,field work,mars,mission control},
  file = {/home/cameron/Zotero/storage/TFL5JZTJ/ast.2018.1863.pdf}
}

@article{Love2019a,
  title = {{{BASALT}}: {{The Future}} of {{Mars}}, on {{Earth Today}}},
  author = {Love, Stanley G.},
  year = {2019},
  month = mar,
  journal = {Astrobiology},
  volume = {19},
  number = {3},
  pages = {243--244},
  issn = {1531-1074},
  doi = {10.1089/ast.2018.1863},
  keywords = {analog,communication latency,exploration,field work,mars,mission control},
  file = {/home/cameron/Zotero/storage/68RFBD79/Love - 2019 - The Future of Mars , on Earth Today.pdf}
}

@article{Ma2019,
  title = {{{TrafficPredict}}: {{Trajectory Prediction}} for {{Heterogeneous Traffic-Agents}}},
  author = {Ma, Yuexin and Zhu, Xinge and Zhang, Sibo and Yang, Ruigang and Wang, Wenping and Manocha, Dinesh},
  year = {2019},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  number = {Kalman 1960},
  eprint = {1811.02146},
  eprinttype = {arxiv},
  pages = {6120--6127},
  issn = {2159-5399},
  doi = {10.1609/aaai.v33i01.33016120},
  abstract = {To safely and efficiently navigate in complex urban traffic, autonomous vehicles must make responsible predictions in relation to surrounding traffic-agents (vehicles, bicycles, pedestrians, etc.). A challenging and critical task is to explore the movement patterns of different traffic-agents and predict their future trajectories accurately to help the autonomous vehicle make reasonable navigation decision. To solve this problem, we propose a long short-term memory-based (LSTM-based) realtime traffic prediction algorithm, TrafficPredict. Our approach uses an instance layer to learn instances' movements and interactions and has a category layer to learn the similarities of instances belonging to the same type to refine the prediction. In order to evaluate its performance, we collected trajectory datasets in a large city consisting of varying conditions and traffic densities. The dataset includes many challenging scenarios where vehicles, bicycles, and pedestrians move among one another. We evaluate the performance of TrafficPredict on our new dataset and highlight its higher accuracy for trajectory prediction by comparing with prior prediction methods.},
  archiveprefix = {arXiv},
  keywords = {Multi-agent path finding,multi-agent systems,multi-robot systems,symmetry breaking},
  file = {/home/cameron/Zotero/storage/ILAZ4Z43/4569-Article Text-7608-1-10-20190707.pdf}
}

@article{Mackin2010,
  title = {Effective {{Presentation}} of {{Metabolic Rate Information}} for {{Lunar Extravehicular Activity}} ( {{EVA}} )},
  author = {Mackin, Michael A and Gonia, Philip and {Lombay-gonzalez}, Jos{\'e}},
  year = {2010},
  number = {August},
  file = {/home/cameron/Zotero/storage/HJHYIWUL/Mackin, Gonia, Lombay-gonzalez - 2010 - Effective Presentation of Metabolic Rate Information for Lunar Extravehicular Activity ( EVA ).pdf}
}

@article{Maki2005,
  title = {Operation and {{Performance}} of the {{Mars Exploration Rover Imaging System}} on the {{Martian Surface}}},
  author = {Maki, Justin N. and Litwin, Todd and Schwochert, Mark and Herkenhoff, Ken},
  year = {2005},
  journal = {Conference Proceedings - IEEE International Conference on Systems, Man and Cybernetics},
  volume = {1},
  pages = {930--936},
  issn = {1062922X},
  doi = {10.1109/icsmc.2005.1571265},
  abstract = {The Imaging System on the Mars Exploration Rovers has successfully operated on the surface of Mars for over one Earth year. The acquisition of hundreds of panoramas and tens of thousands of stereo pairs has enabled the rovers to explore Mars at a level of detail unprecedented in the history of space exploration. In addition to providing scientific value, the images also play a key role in the daily tactical operation of the rovers. The mobile nature of the MER surface mission requires extensive use of the imaging system for traverse planning, rover localization, remote sensing instrument targeting, and robotic arm placement. Each of these activity types requires a different set of data compression rates, surface coverage, and image acquisition strategies. An overview of the surface imaging activities is provided, along with a summary of the image data acquired to date. \textcopyright{} 2005 IEEE.},
  keywords = {Cameras,Imaging system,Mars,Operations,Rovers},
  file = {/home/cameron/Zotero/storage/WF9Q7S6V/01571265.pdf}
}

@article{Maldonado2015,
  title = {Continuous {{Bayesian}} Networks vs. Other Methods for Regression in Environmental Modelling},
  author = {Maldonado, A D and Ropero, R F and Aguilera, P A and Fern{\'a}ndez, A and Rum{\'i}, R and Salmer{\'o}n, A},
  year = {2015},
  journal = {Procedia Environmental Sciences},
  volume = {26},
  pages = {70--73},
  publisher = {{Elsevier B.V.}},
  issn = {1878-0296},
  doi = {10.1016/j.proenv.2015.05.027},
  abstract = {Several algorithms have been developed in the literature to solve regression problems. We propose a novel methodology based on Bayesian networks (BNs) to deal with regression problems in environmental research. To demonstrate its capabilities and strength, we compare a BN model with 3 other methods commonly used to solve regression tasks, in terms of their root mean squared error (RMSE). The errors were depicted on error maps, providing information about the reliability of the predictions in each observation. The results show that BNs are competitive with other popular methods.},
  keywords = {Bayesina networks,error map,Linear regression,model Tree,Multilayer perceptron,surface waters},
  file = {/home/cameron/Zotero/storage/WWVYVL7Z/m-api-1a35ff6e-bf6a-2273-7528-8de01144ff0d.pdf}
}

@article{maldonado2015,
  title = {Continuous {{Bayesian Networks}} vs. {{Other Methods}} for {{Regression}} in {{Environmental Modelling}}},
  author = {Maldonado, A.D. and Ropero, R.F. and Aguilera, P.A. and Fern{\'a}ndez, A. and Rum{\'i}, R. and Salmer{\'o}n, A.},
  year = {2015},
  journal = {Procedia Environmental Sciences},
  volume = {26},
  pages = {70--73},
  publisher = {{Elsevier BV}},
  issn = {18780296},
  doi = {10.1016/j.proenv.2015.05.027},
  abstract = {Several algorithms have been developed in the literature to solve regression problems. We propose a novel methodology based on Bayesian networks (BNs) to deal with regression problems in environmental research. To demonstrate its capabilities and strength, we compare a BN model with 3 other methods commonly used to solve regression tasks, in terms of their root mean squared error (RMSE). The errors were depicted on error maps, providing information about the reliability of the predictions in each observation. The results show that BNs are competitive with other popular methods.},
  file = {/home/cameron/Zotero/storage/9F34DUGM/1-s2.0-S1878029615001942-main.pdf}
}

@article{maldonado2015a,
  title = {Continuous {{Bayesian Networks}} vs. {{Other Methods}} for {{Regression}} in {{Environmental Modelling}}},
  author = {Maldonado, A.D. and Ropero, R.F. and Aguilera, P.A. and Fern{\'a}ndez, A. and Rum{\'i}, R. and Salmer{\'o}n, A.},
  year = {2015},
  journal = {Procedia Environmental Sciences},
  volume = {26},
  pages = {70--73},
  publisher = {{Elsevier BV}},
  issn = {18780296},
  doi = {10.1016/j.proenv.2015.05.027},
  abstract = {Several algorithms have been developed in the literature to solve regression problems. We propose a novel methodology based on Bayesian networks (BNs) to deal with regression problems in environmental research. To demonstrate its capabilities and strength, we compare a BN model with 3 other methods commonly used to solve regression tasks, in terms of their root mean squared error (RMSE). The errors were depicted on error maps, providing information about the reliability of the predictions in each observation. The results show that BNs are competitive with other popular methods.},
  file = {/home/cameron/Zotero/storage/CB778HZJ/1-s2.0-S1878029615001942-main.pdf}
}

@article{Maler1995,
  title = {On the Synthesis of Discrete Controllers for Timed Systems (an Extended Abstract)},
  author = {Maler, O. and Maler, O. and Pnueli, Amir and Pnueli, A. and Sifakis, J. and Sifakis, J.},
  year = {1995},
  journal = {Stacs},
  volume = {95},
  number = {6021},
  pages = {229--242},
  issn = {16113349},
  abstract = {This paper presents algorithms for the automatic synthesis of real-time controllers by finding a winning strategy for certain games defined by the timed-automata of Alur and Dill. In such games, the outcome depends on the players' actions as well as on their timing. We believe that these results will pave the way for the application of program synthesis techniques to the construction of real-time embedded systems from their specifications.},
  isbn = {3540590420},
  file = {/home/cameron/Zotero/storage/Y5ZQBSRJ/m-api-4943027d-6a31-8979-1772-ff8fcff42425.pdf}
}

@article{Marques-Silva2021,
  title = {Chapter 4: {{Conflict-driven}} Clause Learning {{SAT}} Solvers},
  author = {{Marques-Silva}, Joao and Lynce, Ines and Malik, Sharad},
  year = {2021},
  journal = {Frontiers in Artificial Intelligence and Applications},
  volume = {336},
  pages = {133--182},
  issn = {09226389},
  doi = {10.3233/FAIA200987},
  abstract = {One of the most important paradigm shifts in the use of SAT solvers for solving industrial problems has been the introduction of clause learning. Clause learning entails adding a new clause for each conflict during backtrack search. This new clause prevents the same conflict from occurring again during the search process. Moreover, sophisticated techniques such as the identification of unique implication points in a graph of implications, allow creating clauses that more precisely identify the assignments responsible for conflicts. Learned clauses often have a large number of literals. As a result, another paradigm shift has been the development of new data structures, namely lazy data structures, which are particularly effective at handling large clauses. These data structures are called lazy due to being in general unable to provide the actual status of a clause. Efficiency concerns and the use of lazy data structures motivated the introduction of dynamic heuristics that do not require knowing the precise status of clauses. This chapter describes the ingredients of conflict-driven clause learning SAT solvers, namely conflict analysis, lazy data structures, search restarts, conflict-driven heuristics and clause deletion strategies.},
  isbn = {9781643681603},
  file = {/home/cameron/Zotero/storage/KXNB9UUA/Marques-Silva, Lynce, Malik - 2021 - Chapter 4 Conflict-driven clause learning SAT solvers.pdf}
}

@techreport{MarquesSilva1996,
  title = {{{GRASP}}\textemdash{{A New Search Algorithm}} for {{Satisfiabilit}}},
  author = {{Marques-Silva}, Joao and Sakallah, Karem A.},
  year = {1996},
  journal = {Computer Science and Engineering Division Department of Electrical Engineering and Computer Science},
  volume = {138},
  number = {12},
  pages = {1282},
  institution = {{The University of Michigan}},
  doi = {10.1109/ICCAD.1996.569607},
  abstract = {This report introduces GRASP (Generic seaRch Algorithm for the Satisfiability Problem), an integrated algorithmic frame- work for SAT that unifies several previously proposed search-pruning techniques and facilitates identification of additional ones. GRASP is premised on the inevitability of conflicts during search and its most distinguishing feature is the augmentation of basic backtracking search with a powerful conflict analysis procedure. Analyzing conflicts to determine their causes enables GRASP to backtrack non-chronologically to earlier levels in the search tree, potentially pruning large portions of the search space. In addition, by ``recording'' the causes of conflicts, GRASP can recognize and preempt the occurrence of similar con- flicts later on in the search. Finally, straightforward bookkeeping of the causality chains leading up to conflicts allows GRASP to identify assignments that are necessary for a solution to be found. Experimental results obtained from a large number of benchmarks, including many from the field of test pattern generation, indicate that application of the proposed conflict analy- sis techniques to SAT algorithms can be extremely effective for a large number of representative classes of SAT instances.},
  file = {/home/cameron/Zotero/storage/D88BD7MN/Marques-Silva, Sakallah - 1996 - GRASPâ€”A New Search Algorithm for Satisfiabilit.pdf}
}

@article{MarquesSilva1999,
  title = {{{GRASP}}: {{A Search Algorithm}} for {{Propositional Satisfiability}}},
  author = {{Marques-Silva}, Joao and Sakallah, Karem A.},
  year = {1999},
  journal = {IEEE Transactions on Computers},
  volume = {48},
  number = {5},
  pages = {506--521},
  doi = {10.1109/12.769433},
  abstract = {This paper introduces GRASP (Generic seaRch Algorithm for the Satisfiability Problem), a new search algorithm for Propositional Satisfiability (SAT). GRASP incorporates several search-pruning techniques that proved to be quite powerful on a wide variety of SAT problems. Some of these techniques are specific to SAT, whereas others are similar in spirit to approaches in other fields of Artificial Intelligence. GRASP is premised on the inevitability of conflicts during the search and its most distinguishing feature is the augmentation of basic backtracking search with a powerful conflict analysis procedure. Analyzing conflicts to determine their causes enables GRASP to backtrack nonchronologically to earlier levels in the search tree, potentially pruning large portions of the search space. In addition, by \textordfeminine recording\textordmasculine{} the causes of conflicts, GRASP can recognize and preempt the occurrence of similar conflicts later on in the search. Finally, straightforward bookkeeping of the causality chains leading up to conflicts allows GRASP to identify assignments that are necessary for a solution to be found. Experimental results obtained from a large number of benchmarks indicate that application of the proposed conflict analysis techniques to SAT algorithms can be extremely effective for a large number of representative classes of SAT instances.},
  keywords = {conflict diagnosis,conflict-based equivalence,conflict-directed nonchronological backtracking,ÃSatisfiability,failure-driven assertions,search algorithms,unique implication points.},
  file = {/home/cameron/Zotero/storage/KW5U2FUX/m-api-6e247518-a22c-8293-573c-49080c4b7c1d.pdf}
}

@techreport{marquez,
  title = {Enabling {{Communication Between Astronauts}} and {{Ground Teams}} for {{Space Exploration Missions}}},
  author = {Marquez, Jessica J. and Hillenius, Steven},
  file = {/home/cameron/Zotero/storage/MXM6TKMW/Marquez-MissionLog-v3.pdf}
}

@inproceedings{marquez2017,
  title = {Increasing Crew Autonomy for Long Duration Exploration Missions: {{Self-scheduling}}},
  booktitle = {{{IEEE Aerospace Conference Proceedings}}},
  author = {Marquez, Jessica J. and Hillenius, Steven and Kanefsky, Bob and Zheng, Jimin and Deliz, Ivonne and Reagan, Marcum L.},
  year = {2017},
  month = jun,
  publisher = {{IEEE Computer Society}},
  issn = {1095323X},
  doi = {10.1109/AERO.2017.7943838},
  abstract = {Over the last three years, we have been investigating the operational concept of crew self-scheduling as a method of increasing crew autonomy for future exploration missions. Through Playbook, a planning and scheduling software tool, we have incrementally increased the ability for Earth analog mission crews to modify their schedules. Playbook allows the crew to add new activities from scratch, add new activities or groups of activities through a Task List, and reschedule or reassign flexible activities. The crew is also able to identify if plan modifications create violations, i.e., plan constraints not being met. This paper summarizes our observations with qualitative evidence from four NASA Extreme Environment Mission Operations (NEEMO) analog missions that supported self-scheduling as a feasible operational concept.},
  isbn = {978-1-5090-1613-6},
  file = {/home/cameron/Zotero/storage/RIJFD6QF/marquez-crew-autonomy-self-scheduling-neemo-v4.pdf}
}

@techreport{MarsEVA2022,
  title = {{{REFERENCE SURFACE ACTIVITIES FOR CREWED MARS MISSION SYSTEMS AND UTILIZATION}}},
  author = {{Exploration Systems Development Mission Directorate}},
  year = {2022},
  number = {January},
  pages = {54},
  address = {{Houston, TX}},
  institution = {{NASA}},
  abstract = {This document details constraints and considerations in planning daily activity timelines for crewed exploration on Mars. This work was done for the purpose of estimating how much time during each martian day (hereafter referred to as a sol) might be available for exploration activities, (also referred to as utilization activities in this document), after accounting for crew and equipment care. These results become part of the factors taken into consideration as specific goals and objectives are assembled into a mission plan. Daily},
  file = {/home/cameron/Zotero/storage/ZL4N8ND4/Aeronautics - 2022 - Exploration Systems Development Mission Directorate ( ESDMD ) REFERENCE SURFACE ACTIVITIES FOR CREWED.pdf}
}

@article{McBarron1994,
  title = {Past, Present, and Future: {{The U}}.{{S}}. {{EVA Program}}},
  author = {McBarron, James W.},
  year = {1994},
  journal = {Acta Astronautica},
  volume = {32},
  number = {1},
  pages = {5--14},
  issn = {00945765},
  doi = {10.1016/0094-5765(94)90143-0},
  abstract = {This paper provides an overview and summary of U.S. extravehicular activity accomplishments of the last 26 years, Space Shuttle missions having scheduled extravehicular activities to be performed over the next several years, extravehicular activities expected to be necessary to support Space Station Freedom assembly tasks and operations, and potential extravehicular activity roles of the NASA Space Exploration Initiative Program. \textcopyright{} 1994.},
  pmid = {11541019},
  file = {/home/cameron/Zotero/storage/YD9W6H5N/McBarron - 1994 - Past, present, and future The U.S. EVA Program.pdf}
}

@techreport{McDermott1998,
  title = {{{PDDL}} - {{The Planning Domain Definition Language}}},
  author = {McDermott, Drew and {AIPS'98 Planning Competation Committee}},
  year = {1998},
  pages = {26},
  abstract = {This manual describes the syntax of PDDL? the Planning Domain De?nition Language? the problem?speci?cation language for the AIPS??? planning competition? The language has roughly the the expressiveness of Pednault?s ADL ???? for propositions? and roughly the expressiveness of UMCP ??? for actions? Our hope is to encourage empirical evaluation of planner performance? and development of standard sets of problems all in comparable notations},
  file = {/home/cameron/Zotero/storage/MEPV5MMC/McDermott, AIPS'98 Planning Competation Committee - 1998 - PDDL - The Planning Domain Definition Language.pdf}
}

@article{McGeachie2004,
  title = {Utility {{Functions}} for {{Ceteris Paribus Preferences}}},
  author = {McGeachie, Michael},
  year = {2004},
  month = may,
  journal = {Computational Intell},
  volume = {20},
  number = {2},
  pages = {158--217},
  issn = {0824-7935, 1467-8640},
  doi = {10.1111/j.0824-7935.2004.00235.x},
  abstract = {Ceteris paribus preference statements concisely represent preferences over outcomes or goals in a way natural to human thinking. Many decision making methods require an efficient method for comparing the desirability of two arbitrary goals. We address this need by presenting an algorithm for converting a set of qualitative ceteris paribus preferences into a quantitative utility function. Our algorithm is complete for a finite universe of binary features. Constructing the utility function can, in the worst case, take time exponential in the number of features. Common forms of independence conditions reduce the computational burden. We present heuristics using utility independence and constraint based search to achieve efficient utility functions.},
  langid = {english},
  file = {/home/cameron/Zotero/storage/367KQR8F/McGeachie and Doyle - 2004 - Utility Functions for Ceteris Paribus Preferences.pdf}
}

@article{McGhan2016,
  title = {The Resilient Spacecraft Executive: {{An}} Architecture for Risk-Aware Operations in Uncertain Environments},
  author = {McGhan, Catharine L.R. and Murrayy, Richard M. and Vaquero, Tiago and Williams, Brian C. and Ingham, Michel D. and Ono, Masahiro and Estlin, Tara and Lanka, Ravi and Arslan, Oktay and Elaasar, Maged E.},
  year = {2016},
  journal = {AIAA Space and Astronautics Forum and Exposition, SPACE 2016},
  pages = {1--21},
  doi = {10.2514/6.2016-5541},
  abstract = {In this paper we discuss the latest results from the Resilient Space Systems project, a joint effort between Caltech, MIT, NASA Jet Propulsion Laboratory (JPL), and the Woods Hole Oceanographic Institution (WHOI). The goal of the project is to define a resilient, risk-aware software architecture for onboard, real-time autonomous operations that can robustly handle uncertainty in spacecraft behavior within hazardous and unconstrained environments, without unnecessarily increasing complexity. The architecture, called the Resilient Spacecraft Executive (RSE), has been designed to support three functions: (1) adapting to component failures to allow graceful degradation, (2) accommodating environments, science observations, and spacecraft capabilities that are not fully known in advance, and (3) making risk-aware decisions without waiting for slow ground-based reactions. In implementation, the bulk of the RSE effort has focused on the parts of the architecture used for goal-directed execution and control, including the deliberative, habitual, and reexive modules. We specify the capabilities and constraints needed for each module, and discuss how we have extended the current state-of-the-art algorithms so that they can supply the required functionality, such as risk-aware planning in the deliberative module that conforms to mission operator-supplied priorities and constraints. Furthermore, the RSE architecture is modular to enable extension and reconfiguration, as long as the embedded algorithmic components exhibit the required risk-aware behavior in the deliberative module and risk-bounded behavior in the habitual module. To that end, we discuss some feasible, useful RSE configurations and deployments for a Mars rover case and an autonomous underwater vehicle case. We also discuss additional capabilities that the architecture requires to support needed resiliency, such as onboard analysis and learning.},
  isbn = {9781624104275},
  file = {/home/cameron/Zotero/storage/RFM5C49L/McGhan et al. - 2016 - The resilient spacecraft executive An architecture for risk-aware operations in uncertain environments.pdf}
}

@book{Medlin2020,
  title = {Trustworthy {{Autonomy}} : {{A Roadmap}} to {{Assurance Part I}} : {{System Effectiveness}}},
  author = {Medlin, Rebecca},
  year = {2020},
  isbn = {2-01-900036-9},
  file = {/home/cameron/Zotero/storage/N7WM2CHY/Medlin, Leader - 2020 - Trustworthy Autonomy A Roadmap to Assurance Part I System Effectiveness.pdf}
}

@article{Mende2012,
  title = {Android {{Bluetooth}} Monitoring},
  author = {Mende, Hendrik},
  year = {2012},
  file = {/home/cameron/Zotero/storage/QBL52U9K/2012_Thesis_HendrikMende.pdf}
}

@article{metawei2012,
  title = {Load Balancing in Distributed Multi-Agent Computing Systems},
  author = {Metawei, Maha A. and Ghoneim, Salma A. and Haggag, Sahar M. and Nassar, Salwa M.},
  year = {2012},
  journal = {Ain Shams Engineering Journal},
  issn = {20904479},
  doi = {10.1016/j.asej.2012.03.001},
  abstract = {In this paper, a dynamic load-balancing scheme is developed for implementation in an agent-based distributed system. Load balancing is achieved via agent migration from heavily loaded nodes to lightly loaded ones. The credit based concept is used for the dual objective of: (1) the selection of candidate agents for migration, and (2) the selection of destination nodes. This represents an elaboration of previous research work aimed at the selection of agents only. Multiple linear regression is used to achieve our dual objective. A complexity analysis for the proposed system operation is performed, the complete operation is O(n 2) where n is the number of agents on the local host. The proposed system is implemented using JADE (Java Agent DEvelopment Framework), the multiple linear regression operation is performed using R Tool. The experimental results show a modified system operation in terms of reduced user's query response time, by implementing agent load balancing. \textcopyright{} 2012 Ain Shams University. Production and hosting by Elsevier B.V.},
  keywords = {Cluster computing,Distributed systems,Load balancing,Multi-agent computing,Multiple linear regression,Task migration},
  file = {/home/cameron/Zotero/storage/CXTI6SZK/upload.pdf}
}

@article{Miller2015,
  title = {Information Flow Model of Human Extravehicular Activity Operations},
  author = {Miller, Matthew J. and McGuire, Kerry M. and Feigh, Karen M.},
  year = {2015},
  journal = {IEEE Aerospace Conference Proceedings},
  volume = {2015-June},
  issn = {1095323X},
  doi = {10.1109/AERO.2015.7118942},
  abstract = {information flow model; ISS; EVA;},
  isbn = {9781479953790},
  file = {/home/cameron/Zotero/storage/NQMJVU59/information-flow-model-of-eva-ops.pdf}
}

@phdthesis{Miller2017,
  title = {Decision {{Support System Development For Human Extravehicular Activity}}},
  author = {Miller, Matthew J.},
  year = {2017},
  number = {December},
  abstract = {The design and development processes for decision support systems (DSS) can have important implications for yielding desirable and effective design solutions. This disserta- tion seeks to understand, and ultimately provide a meaningful pathway to generating useful decision support system designs for use in environments that do not yet exist (e.g. envi- sioned worlds). The contributions of this thesis are two fold. The first is domain specific and addresses the known deficiencies that will impact future human extravehicular opera- tions. The second is methodological and generalizable across many domains. Central to this effort is realizing that design requirements are the medium through which hypothe- sized system designs are built. This dissertation first demonstrates that cognitive systems engineering (CSE) methods can be applied to yield design insight in the form of high level design requirements amenable to traditional systems engineering processes (Chapter 3). Second, this dissertation demonstrates how a subset of those requirements, along side en- visioning and testing within a future work context, can yield prototype designs suitable for supporting future extravehicular activity (EVA) operations (Chapter 4 and 5). Finally, this dissertation evaluated the resultant prototypes against the requirements to demonstrate both validity of the requirements and the verification of the design (Chapter 6). The intent here is to first define what is required by the domain and hypothesis how new design solutions might be capable to promote desired capabilities specified by the requirements derived from the work in Chapter 3. As a result, this thesis contributes the underlying science needed to design a DSS within the EVA work domain for future mission operations.},
  school = {Georgia Institute of Technology},
  file = {/home/cameron/Zotero/storage/N39XGLR2/Miller2017_Decision_Support_System_Development (2).pdf}
}

@inproceedings{miller2017,
  title = {Next-Generation Human Extravehicular Spaceflight Operations Support Systems Development},
  booktitle = {Proceedings of the {{International Astronautical Congress}}, {{IAC}}},
  author = {Miller, M.J. and Pittman, C.W. and Feigh, K.M.},
  year = {2017},
  volume = {9},
  issn = {00741795},
  abstract = {\textcopyright{} 2017 by the International Astronautical Federation. All rights reserved. This paper presents the research, design, and development efforts aimed at constructing next generation software that supports human extravehicular activity (EVA), commonly known as a spacewalk. EVA operations today rely on an extensive team of Earth-based flight controllers who actively monitor and direct EVA progress while maintaining crew and vehicle safety. However, future deep-space mission destinations will impose round-trip communication delays. In the case of Mars, round-trip delays range from 8 to 40 minutes. As a result, astronauts will need to rely on local decision support systems (DSS) to make tactical decisions during execution without Earth-based support personnel. This paper first presents the content, structure and form of existing EVA support systems that were leveraged as source material for prototype development. Then detailed descriptions of the engineering specifications and design features are provided of the two prototypes, (Baseline and Advanced), that were built. Finally, the phases software development and corresponding architectures are discussed. As a result of this effort, this study was able to harness the rapid prototyping capabilities and broad platform support of modern web technologies to iterate on designs and successfully deploy a high fidelity system to controlled laboratory simulations and NASA analog research sites. The results of this work provide an empirically derived set of design solutions to guide the future of EVA operational support systems for future human spaceflight missions.},
  isbn = {978-1-5108-5537-3},
  keywords = {Decision support system,Extravehicular activity}
}

@article{Miller2017c,
  title = {Decision {{Support System Requirements Definition}} for {{Human Extravehicular Activity Based}} on {{Cognitive Work Analysis}}},
  author = {Miller, Matthew J. and McGuire, Kerry M. and Feigh, Karen M.},
  year = {2017},
  journal = {Journal of Cognitive Engineering and Decision Making},
  volume = {11},
  number = {2},
  pages = {136--165},
  issn = {21695032},
  doi = {10.1177/1555343416672112},
  abstract = {The design and adoption of decision support systems within complex work domains is a challenge for cognitive systems engineering (CSE) practitioners, particularly at the onset of project development. This article presents an example of applying CSE techniques to derive design requirements compatible with traditional systems engineering to guide decision support system development. Specifically, it demonstrates the requirements derivation process based on cognitive work analysis for a subset of human spaceflight operations known as extravehicular activity. The results are presented in two phases. First, a work domain analysis revealed a comprehensive set of work functions and constraints that exist in the extravehicular activity work domain. Second, a control task analysis was performed on a subset of the work functions identified by the work domain analysis to articulate the translation of subject matter states of knowledge to high-level decision support system requirements. This work emphasizes an increm...},
  isbn = {0016-7878},
  keywords = {cognitive work analysis,decision support,envisioning,requirements definition},
  file = {/home/cameron/Zotero/storage/QFEBY5EW/Miller2016_Decision_Support_System_Requirement.pdf}
}

@article{Miller2017d,
  title = {Decision {{Support System Requirements Definition}} for {{Human Extravehicular Activity Based}} on {{Cognitive Work Analysis}}},
  author = {Miller, Matthew James and McGuire, Kerry M. and Feigh, Karen M.},
  year = {2017},
  month = jun,
  journal = {Journal of Cognitive Engineering and Decision Making},
  volume = {11},
  number = {2},
  pages = {136--165},
  publisher = {{SAGE Publications Inc.}},
  issn = {21695032},
  doi = {10.1177/1555343416672112},
  abstract = {The design and adoption of decision support systems within complex work domains is a challenge for cognitive systems engineering (CSE) practitioners, particularly at the onset of project development. This article presents an example of applying CSE techniques to derive design requirements compatible with traditional systems engineering to guide decision support system development. Specifically, it demonstrates the requirements derivation process based on cognitive work analysis for a subset of human spaceflight operations known as extravehicular activity. The results are presented in two phases. First, a work domain analysis revealed a comprehensive set of work functions and constraints that exist in the extravehicular activity work domain. Second, a control task analysis was performed on a subset of the work functions identified by the work domain analysis to articulate the translation of subject matter states of knowledge to high-level decision support system requirements. This work emphasizes an incremental requirements specification process as a critical component of CSE analyses to better situate CSE perspectives within the early phases of traditional systems engineering design.},
  keywords = {cognitive work analysis,decision support,envisioning,requirements definition},
  file = {/home/cameron/Zotero/storage/BAB3IVV5/Miller2016_Decision_Support_System_Requirement.pdf}
}

@article{Miller2017iac,
  title = {Next-{{Generation Human Extravehicular Spaceflight Operations Support Systems Development}}},
  author = {Miller, Matthew J. and Pittman, Cameron W. and Feigh, Karen M.},
  year = {2017},
  journal = {68th International Astronautical Congress},
  number = {October},
  pages = {0--17},
  issn = {00741795},
  isbn = {978-1-62410-215-8},
  keywords = {decision support system,extravehicular activity},
  file = {/home/cameron/Zotero/storage/GVGJTUZX/Miller2017_Next_Generation_Human_ExtravehicularSpaceflight_Operations (1).pdf}
}

@article{Miller2018,
  title = {{{SCIENTIFIC HYBRID REALITY ENVIRONMENTS}} ({{SHyRE}}): {{BRINGING FIELD WORK INTO THE LABORATORY}}},
  author = {Miller, Matthew J. and Graff, Trevor and Young, Kelsey and Coan, David and Whelley, Patrick L. and Richardson, J and Knudson, C. and Bleacher, Jacob E. and Garry, W. Brent and Delgado, Frank and Noyes, Matthew and Valle, P. and Buffington, Jesse A and Abercromby, Andrew F.J.},
  year = {2018},
  volume = {77058},
  pages = {2},
  file = {/home/cameron/Zotero/storage/CCE4ZCPU/shyre-abstract.pdf}
}

@article{Miller2019,
  title = {Future {{Needs}} for {{Science-Driven Geospatial}} and {{Temporal Extravehicular Activity Planning}} and {{Execution}}},
  author = {Marquez, Jessica J. and Miller, Matthew J. and Cohen, Tamar and Deliz, Ivonne and Lees, David S. and Zheng, Jimin and Lee, Yeon J. and Kanefsky, Bob and Norheim, Johannes and Deans, Matthew and Hillenius, Steven},
  year = {2019},
  month = mar,
  journal = {Astrobiology},
  volume = {19},
  number = {3},
  pages = {440--461},
  issn = {1531-1074},
  doi = {10.1089/ast.2018.1838},
  file = {/home/cameron/Zotero/storage/DNY6WD9X/Miller et al. - 2019 - Future Needs for Science-Driven Geospatial and Temporal Extravehicular Activity Planning and Execution 1.pdf}
}

@book{Miller2019b,
  title = {Addressing the Envisioned World Problem: {{A}} Case Study in Human Spaceflight Operations},
  author = {Miller, Matthew J. and Feigh, Karen M.},
  year = {2019},
  journal = {Design Science},
  volume = {5},
  publisher = {{Cambridge University Press}},
  issn = {20534701},
  doi = {10.1017/dsj.2019.2},
  abstract = {The construction of future technological systems in work domains that do not yet exist, known as the envisioned world problem, is an increasingly important topic for designers, particularly given the rapid rate of technological advancement in the modern era. This paper first discusses the theoretical underpinnings of using cognitive work analysis (CWA) for developing a decision support system (DSS) situated within the envisioned world problem and recasts the problem as pathway-dependent processes. Using this pathway-dependent framework, each stage of the envisioning process is described to reveal how human factors experts can link existing work domains to envisioned instances. Finally, a case study example of the envisioning process that incorporates CWA modelling is demonstrated as it pertains to the advancement of the human spaceflight domain. As a result, this paper provides a unified treatment of the envisioned world problem with an end-to-end example of one approach to designing future technologies for future work domains.},
  isbn = {0000000202817},
  keywords = {cognitive work analysis,decision support system,envisioned world problem,extravehicular activity,human spaceflight},
  file = {/home/cameron/Zotero/storage/2XVIVTIP/Miller_2019_Addressing_the_envisioned_world_pr.pdf}
}

@article{Miller2019c,
  title = {Explanation in Artificial Intelligence: {{Insights}} from the Social Sciences},
  author = {Miller, Tim},
  year = {2019},
  journal = {Artificial Intelligence},
  volume = {267},
  eprint = {1706.07269},
  eprinttype = {arxiv},
  pages = {1--38},
  issn = {00043702},
  doi = {10.1016/j.artint.2018.07.007},
  abstract = {There has been a recent resurgence in the area of explainable artificial intelligence as researchers and practitioners seek to provide more transparency to their algorithms. Much of this research is focused on explicitly explaining decisions or actions to a human observer, and it should not be controversial to say that looking at how humans explain to each other can serve as a useful starting point for explanation in artificial intelligence. However, it is fair to say that most work in explainable artificial intelligence uses only the researchers' intuition of what constitutes a `good' explanation. There exist vast and valuable bodies of research in philosophy, psychology, and cognitive science of how people define, generate, select, evaluate, and present explanations, which argues that people employ certain cognitive biases and social expectations to the explanation process. This paper argues that the field of explainable artificial intelligence can build on this existing research, and reviews relevant papers from philosophy, cognitive psychology/science, and social psychology, which study these topics. It draws out some important findings, and discusses ways that these can be infused with work on explainable artificial intelligence.},
  archiveprefix = {arXiv},
  keywords = {Explainability,Explainable AI,Explanation,Interpretability,Transparency},
  file = {/home/cameron/Zotero/storage/EEUE8QI6/Miller - 2019 - Explanation in artificial intelligence Insights from the social sciences.pdf}
}

@article{Miller2019c,
  title = {A {{Flexible Telecommunication Architecture}} for {{Human Planetary Exploration Based}} on the {{BASALT Science-Driven Mars Analog}}},
  author = {Miller, Matthew J. and {Santiago-materese}, Delia and Seibert, Marc A and Lim, Darlene S.S.},
  year = {2019},
  volume = {19},
  number = {3},
  pages = {478--496},
  doi = {10.1089/ast.2018.1906},
  file = {/home/cameron/Zotero/storage/TPKY9CCV/ast.2018.1906.pdf}
}

@inproceedings{Miller2019spoc,
  title = {Scientific {{Physical}} and {{Operations Characterization}} ({{SPOC}}) - {{Combining Cognitive}} and {{Biomechanical Analyses}} to {{Inform Planetary Science Exploration}}},
  booktitle = {International {{Astronautical Congress}}},
  author = {Miller, Matthew J. and Pittman, Cameron W. and Beaton, Kara H. and Stirling, Leia and McGrath, Timothy and Nguyen, Golda and Graff, Trevor and Abercromby, Andrew F.J.},
  year = {2019},
  address = {{Washington DC. USA}},
  abstract = {Future human planetary spaceflight missions to destinations such as the Moon or Mars aim to pro- mote human scientific exploration, yet outside of the Apollo program, the spaceflight community at large has limited experience performing scientific exploration. An open area of research is designing future human missions to enable exploration in support of scientific goals. One approach to overcome this lim- ited spaceflight experience is to learn from present-day expert scientists performing terrestrial scientific fieldwork. By studying and understanding the intrinsic work demands, constraints, and behaviors that field scientists must overcome and perform to achieve their scientific fieldwork objectives in present-day terrestrial settings, their scientific needs can be better understood within the context of proposed future missions and technologies. This paper presents the research formulation, prototype data capture sys- tems, and preliminary testing results of the Scientific Physical and Operation Characterization (SPOC) project, which aims to better understand the constraints that shape the process of scientific exploration in present-day terrestrial fieldwork as a means to inform future spaceflight planetary exploration concepts of operations. This research employs two complementary methodologies, cognitive work analysis and wearable sensor measurement, to unpack the cognitive and biomechanical constraints that shape scien- tists' actions in field settings. This paper first describes the methodology associated with simultaneously assessing the cognitive and physical dimensions of field work to illustrate the utility and application of this combined approach. A detailed description is then provided of the data capture system and data processing pipeline used to study science field teams and streamline post hoc analysis. Three field-ready 1 data collection configurations were designed with combinations of commercial off the shelf audio, video, remote sensing equipment, and wearable inertial measurement units (IMUs), depending on the role of the personnel in the field team (i.e. the scientist, the cognitive researcher, the physical sensors researcher). Finally, this paper presents the cognitive and physical task characterizations of scientists' activities that resulted from incremental proof-of-concept data capture sessions with scientists performing field activities in both laboratory-based and natural environmental settings. By modeling a more complete representa- tion of scientific fieldwork, these testing efforts will contribute to the formulation of necessary scientific capabilities and needs that promote scientific exploration, including informing future mission hardware and software development and mission concept of operations.},
  file = {/home/cameron/Zotero/storage/Q3ABRA6W/IAC-19,B3,7,x51477.brief.pdf}
}

@techreport{MillerApollo2017,
  title = {Operational {{Assessment}} of {{Apollo Lunar Surface Extravehicular Activity}}},
  author = {Miller, Matthew J. and Claybrook, Austin. and Greenlund, S J and Marquez, Jessica J. and Feigh, Karen M.},
  year = {2017},
  journal = {NASA STI Program},
  number = {203999},
  pages = {156},
  doi = {10.2514/6.2017-5115},
  abstract = {Quantifying the operational variability of extravehicular activity (EVA) execution is critical to help design and build future support systems to enable astronauts to monitor and manage operations in deep-space, where ground support operators will no longer be able to react instantly and manage execution deviations due to the significant communication latency. This study quantifies the operational variability exhibited during Apollo 14-17 lunar surface EVA operations to better understand the challenges and natural tendencies of timeline execution and life support system performance involved in surface operations. Each EVA (11 in total) is individually summarized as well as aggregated to provide descriptive trends exhibited through- out the Apollo missions. This work extends previous EVA task analyses by calcu- lating deviations between planned and as-performed timelines as well as examining metabolic rate and consumables usage throughout the execution of each EVA. The intent of this work is to convey the natural variability of EVA operations and to pro- vide operational context for coping with the variability inherent to EVA execution as a means to support future concepts of operations.},
  file = {/home/cameron/Zotero/storage/LR89JTC8/20170007261.pdf}
}

@techreport{MMV2001,
  title = {Dynamic {{Control Of Plans With Temporal Uncertainty}}},
  author = {Morris, Paul H. and Muscettola, Nicola and Vidal, Thierry},
  year = {2001},
  abstract = {Certain planning systems that deal with quantitative time constraints have used an underlying Simple Temporal Problem solver to ensure temporal consistency of plans. However, many applications involve processes of uncertain duration whose timing cannot be controlled by the execution agent. These cases require more complex notions of temporal feasibility. In previous work, various ``controllability'' properties such as Weak, Strong, and Dynamic Controllability have been defined. The most interesting and useful Controllability property, the Dynamic one, has ironically proved to be the most difficult to analyze. In this paper, we resolve the complexity issue for Dynamic Controllability. Unexpectedly, the problem turns out to be tractable. We also show how to efficiently execute networks whose status has been verified.},
  file = {/home/cameron/Zotero/storage/U3JE4SLW/m-api-38b6df6a-bc2b-6102-7d65-bec4e2669c51.pdf}
}

@article{Moffitt2007,
  title = {On the Partial Observability of Temporal Uncertainty},
  author = {Moffitt, Michael D.},
  year = {2007},
  journal = {Proceedings of the National Conference on Artificial Intelligence},
  volume = {2},
  pages = {1031--1037},
  abstract = {We explore a means to both model and reason about partial observability within the scope of constraint-based temporal reasoning. Prior studies of uncertainty in Temporal CSPs have required the realization of all exogenous processes to be made entirely visible to the agent. We relax this assumption and propose an extension to the Simple Temporal Problem with Uncertainty (STPU), one in which the executing agent is made aware of the occurrence of only a subset of uncontrollable events. We argue that such a formalism is needed to encode those complex environments whose external phenomena share a common, hidden source of temporal causality. After characterizing the levels of controllability in the resulting Partially Observable STPU and various special cases, we generalize a known family of reduction rules to account for this relaxation, introducing the properties of extended contingency and sufficient observability. We demonstrate that these modifications enable a polynomial filtering algorithm capable of determining a local form of dynamic controllability; however, we also show that there do remain some instances whose global controllability cannot yet be correctly identified by existing inference rules, leaving the true computational complexity of dynamic controllability an open problem for future research. Copyright \textcopyright 2007, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.},
  isbn = {1577353234},
  keywords = {Reasoning about Plans and Processes and Actions,Technical Papers},
  file = {/home/cameron/Zotero/storage/SRALUYSL/AAAI07-164.pdf}
}

@article{Morris2005,
  title = {Temporal Dynamic Controllability Revisited},
  author = {Morris, Paul and Muscettola, Nicola},
  year = {2005},
  journal = {Proceedings of the National Conference on Artificial Intelligence},
  volume = {3},
  pages = {1193--1198},
  abstract = {An important issue for temporal planners is the ability to handle temporal uncertainty. We revisit the question of how to determine whether a given set of temporal requirements are feasible in the light of uncertain durations of some processes. In particular, we consider how best to determine whether a network is Dynamically Controllable, i.e., whether a dynamic strategy exists for executing the network that is guaranteed to satisfy the requirements. Previous work has shown the existence of a pseudo-polynomial algorithm for testing Dynamic Controllability. Here, we simplify the previous framework, and present a strongly polynomial algorithm with a termination criterion based on the structure of the network.},
  file = {/home/cameron/Zotero/storage/6HW9JU9Q/0927 (Morris, P) (1).pdf}
}

@article{Morris2006,
  title = {A Structural Characterization of Temporal Dynamic Controllability},
  author = {Morris, Paul},
  year = {2006},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {4204 LNCS},
  pages = {375--389},
  issn = {16113349},
  doi = {10.1007/11889205_28},
  abstract = {An important issue for temporal planners is the ability to handle temporal uncertainty. Recent papers have addressed the question of how to tell whether a temporal network is Dynamically Controllable, i.e., whether the temporal requirements are feasible in the light of uncertain durations of some processes. Previous work has presented an O(N5) algorithm for testing this property. Here, we introduce a new analysis of temporal cycles that leads to an O(N \textsuperscript{4}) algorithm. \textcopyright{} Springer-Verlag Berlin Heidelberg 2006.},
  isbn = {3540462678},
  file = {/home/cameron/Zotero/storage/B2ZTGR8J/20060021467.pdf}
}

@article{Morris2013,
  title = {Embedding {{Temporal Constraints For Coordinated Execution}} in {{Habitat Automation}}},
  author = {Morris, Paul H. and Schwabacher, Mark and Dalal, Michael and Fry, Charles},
  year = {2013},
  file = {/home/cameron/Zotero/storage/ED56UWH5/8275 (Morris, P).pdf}
}

@article{Morris2014,
  title = {Dynamic Controllability and Dispatchability Relationships},
  author = {Morris, Paul},
  year = {2014},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {8451 LNCS},
  number = {Dc},
  pages = {464--479},
  issn = {16113349},
  doi = {10.1007/978-3-319-07046-9_33},
  abstract = {An important issue for temporal planners is the ability to handle temporal uncertainty. Recent papers have addressed the question of how to tell whether a temporal network is Dynamically Controllable, i.e., whether the temporal requirements are feasible in the light of uncertain durations of some processes. We present a fast algorithm for Dynamic Controllability. We also note a correspondence between the reduction steps in the algorithm and the operations involved in converting the projections to dispatchable form. This has implications for the complexity for sparse networks. \textcopyright{} 2014 Springer International Publishing.},
  isbn = {9783319070452},
  file = {/home/cameron/Zotero/storage/A2VL4D2I/20140008604.pdf}
}

@article{Morris2016,
  title = {The Mathematics of Dispatchability Revisited},
  author = {Morris, Paul H.},
  year = {2016},
  journal = {Proceedings International Conference on Automated Planning and Scheduling, ICAPS},
  volume = {2016-Janua},
  number = {Icaps},
  pages = {244--252},
  issn = {23340843},
  abstract = {Dispatchability is an important property for the efficient execution of temporal plans where the temporal constraints are represented as a Simple Temporal Network (STN). It has been shown that every STN may be reformulated as a dispatchable STN, and dispatchability ensures that the temporal constraints need only be satisfied locally during execution. Recently it has also been shown that Simple Temporal Networks with Uncertainty, augmented with wait edges, are Dynamically Controllable provided every projection is dispatchable. Thus, the dispatchability property has both theoretical and practical interest. One thing that hampers further work in this area is the underdeveloped theory. The existing definitions are expressed in terms of algorithms, and are less suitable for mathematical proofs. In this paper, we develop a new formal theory of dispatchability in terms of execution sequences. We exploit this to prove a characterization of dispatchability involving the structural properties of the STN graph. This facilitates the potential application of the theory to uncertainty reasoning.},
  keywords = {Technical Papers: Main Track},
  file = {/home/cameron/Zotero/storage/8GQ6YBKK/2016-morris-dispatchability.pdf}
}

@inproceedings{Morris2019,
  title = {Dynamic {{Controllability}} with {{Single}} and {{Multiple Indirect Observations}}},
  booktitle = {{{ICAPS}} 2019 - {{Proceedings}} of the 29th {{International Conference}} on {{Automated Planning}} and {{Scheduling}}},
  author = {Morris, Paul H. and {Bit-monnot}, Arthur},
  year = {2019},
  pages = {9},
  address = {{Berkeley, CA}},
  abstract = {A recent paper introduced a transformation-based approach for determining dynamic controllability of Simple Tempo- ral Networks with Uncertainty (STNUs) extended to have variably-delayed observations of uncontrolled timepoints. Al- though the approach correctly determines dynamic control- lability, it does not always provide the most flexible possi- ble dynamic strategy. We show how to refine the approach in a way that improves the flexibility, and further extend it to a class of Partially Observable STNUs where the hidden timepoints can be indirectly observed via a chain of contin- gent links. We show how to construct a labeled distance graph for these problems, leading to a complete solution. This ap- proach handles ``single-headed'' chained contingent links. For ``multi-headed'' problems, we prove a theorem characteriz- ing their dynamic controllability in isolation. This provides a check on more general networks (and more general methods). We also consider potential extensions of the single-headed approach to multi-headed problems and point out some diffi- culties that arise.},
  file = {/home/cameron/Zotero/storage/RL9WNHZN/69859 (Morris, P).pdf}
}

@article{Moschitti,
  title = {Machine {{Learning}}},
  author = {Moschitti, Alessandro},
  journal = {Communications},
  file = {/home/cameron/Zotero/storage/LUIQWVPF/Machine Learning Cheatsheet.pdf}
}

@inproceedings{Moskewicz2001,
  title = {Chaff: {{Engineering}} an {{Efficient SAT Solver}}},
  booktitle = {Proceedings of the 38th {{ACM}}/{{IEEE Design Automation Conference}}},
  author = {Moskewicz, Matthew W. and Madigan, Conor F. and Zhao, Ying and Zhang, Lintao and Malik, Sharad},
  year = {2001},
  pages = {530--535},
  abstract = {Boolean Satisfiability is probably the most studied of combinatorial optimization/search problems. Significant effort has been devoted to trying to provide practical solutions to this problem for problem instances encountered in a range of applications in Electronic Design Automation (EDA), as well as in Artificial Intelligence (AI). This study has culminated in the development of several SAT packages, both proprietary and in the public domain (e.g. GRASP, SATO) which find significant use in both research and industry. Most existing complete solvers are variants of the Davis-Putnam (DP) search algorithm. In this paper we describe the development of a new complete solver, Chaff, which achieves significant performance gains through careful engineering of all aspects of the search \textendash{} especially a particularly efficient implementation of Boolean constraint propagation (BCP) and a novel low overhead decision strategy. Chaff has been able to obtain one to two orders of magnitude performance improvement on difficult SAT benchmarks in comparison with other solvers (DP or otherwise), including GRASP and SATO.},
  isbn = {1-58113-297-2},
  keywords = {boolean satisfiability,design verification},
  file = {/home/cameron/Zotero/storage/NM7KJ22J/m-api-557c9b60-70e0-8272-3894-7074de3d09a5.pdf}
}

@techreport{Muise2016,
  title = {Plan {{Dispatchability}}: {{A Survey}}},
  author = {Muise, Christian and McIlraith, Sheila A. and Beck, Christopher J.},
  year = {2016},
  pages = {52},
  address = {{Toronto, Ontario, Canada}},
  institution = {{University of Toronto}},
  abstract = {In this paper we focus on dispatching plans that are in the form of a Simple Temporal Network (STN) and its variants. A simple temporal network, introduced in the 1980's by Dechter et al. [Dechter et al., 1989], is a form of solution that primarily deals with the ordering of events, and the timing restrictions between them. Durative actions are represented by using a pair of events, and a number of temporal formalisms can be expressed by a combination of temporal constraints in an STN (e.g., many of the rules of Allen's Interval Algebra [Allen, 1984]). Executing the events in an STN is referred to as dispatching the network, and a network that can be dispatched properly is called consistent. The problem of checking the consistency of a STN is referred to as a the Simple Temporal Problem, and remains as an integral part of planning systems today (e.g., checking the continued consistency of a network in temporal planners).},
  file = {/home/cameron/Zotero/storage/4HQ7CJRP/Muise, McIlraith, Beck - 2016 - Plan Dispatchability A Survey.pdf}
}

@article{Muller2018,
  title = {Value Driven Landmarks for Oversubscription Planning},
  author = {Muller, Daniel and Karpas, Erez},
  year = {2018},
  journal = {Proceedings International Conference on Automated Planning and Scheduling, ICAPS},
  volume = {2018-June},
  number = {Icaps},
  pages = {171--179},
  issn = {23340843},
  abstract = {Oversubscription planning is the problem of choosing an action sequence which reaches a state with a high utility, given a budget for total action cost. Most previous work on oversubscription planning was restricted to only non-negative utility functions and 0-binary utility functions. While this restriction allows using techniques similar to partial satisfaction planning, it limits the expressivity of the formalism. In this paper, we address oversubscription planning with general additive utility functions over a finite-domain representation. We introduce the notions of net utility of an action, and of a gross positive action. Using these notions, we prove several properties about the structure of an optimal plan, which are then compiled into a classical planning problem. The landmarks of this classical planning problem are value driven landmarks of the original oversubscription problem, that is, they must occur in any action sequence which improves utility. An empirical evaluation demonstrates that these landmarks are more informative than previous state-of-the-art methods for landmark discovery for oversubscription planning, and lead to better planning performance.},
  keywords = {Main Track},
  file = {/home/cameron/Zotero/storage/VBMQH5V8/Muller, Karpas - 2018 - Value driven landmarks for oversubscription planning.pdf}
}

@article{Mulling2013,
  title = {Learning to Select and Generalize Striking Movements in Robot Table Tennis {{Cited}} by Me {{DMP}} Reactive\_motion\_gen...},
  author = {M{\"u}lling, K and Kober, J and Kroemer, O and Peters, J},
  year = {2013},
  journal = {The International Journal of Robotics  \ldots},
  pages = {1--24},
  abstract = {Learning to select and generalize striking movements in robot table tennisPING-PONG PLAYERHUMANOID ROBOTPRIMITIVESPREDICTIONIMITATIONTASKBALLAbstract Learning new motor tasks from physical interactions is an important goal for both robotics and machine learning. However, when moving beyond basic skills, most monolithic machine learning approaches fail to scale. For more complex ...},
  keywords = {AAAI Technical Report FS-12-07},
  file = {/home/cameron/Zotero/storage/C6PFMN4Q/Muelling2012AAAI.pdf}
}

@inproceedings{Muqri2015,
  title = {A Taste of Python - {{Discrete}} and Fast Fourier Transforms},
  booktitle = {{{ASEE Annual Conference}} and {{Exposition}}, {{Conference Proceedings}}},
  author = {Muqri, Mohammad Rafiq and Wilson, Eric John and Shakib, Javad},
  year = {2015},
  volume = {122nd ASEE},
  issn = {21535965},
  abstract = {This paper is an attempt to present the development and application of a practical teaching module introducing Python programming techniques to electronics, computer, and bioengineering students at an undergraduate level before they encounter digital signal processing and its applications in junior or senior level courses. The Fourier transform takes a signal in time domain, switches it into the frequency domain, and vice versa. Fourier Transforms are extensively used in engineering and science in a vast and wide variety of fields including concentrations in acoustics, digital signal processing, image processing, geophysical processing, wavelet theory, and optics and astronomy. The Discrete Fourier Transform (DFT) is an essential digital signal processing tool that is highly desirable if the integral form of the Fourier Transform cannot be expressed as a mathematical equation. The key to spectral analysis is to choose a window length that suits the signal to be analyzed, since the length of the window used for DFT calculations has a substantial impact on the information the DFT can provide. The operation count of the DFT algorithm is time-intensive, and as such a number of Fast Fourier Transform methods have been developed to adequately perform DFT efficiently. This paper will explain how this learning and teaching module was instrumental in progressive learning for students by presenting Python programming and the general theory of the Fourier Transform in order to demonstrate how the DFT and FFT algorithms are derived and computed through leverage of the Python data structures. This paper thereby serves as an innovative way to expose technology students to this difficult topic and gives them a fresh taste of Python programming while having fun learning the Discrete and Fast Fourier Transforms. \&copy; American Society for Engineering Education, 2015.},
  file = {/home/cameron/Zotero/storage/DYIS9Q5P/a-taste-of-python-discrete-and-fast-fourier-transforms.pdf}
}

@article{Murphy2019,
  title = {Personal and Organizational Mindsets at Work},
  author = {Murphy, Mary C. and Reeves, Stephanie L.},
  year = {2019},
  journal = {Research in Organizational Behavior},
  volume = {39},
  number = {xxxx},
  pages = {100121},
  publisher = {{Elsevier Ltd}},
  issn = {01913085},
  doi = {10.1016/j.riob.2020.100121},
  abstract = {Decades of research have shown that people's mindsets beliefs\textemdash their beliefs about the fixedness or malleability of talent, ability, and intelligence\textemdash can powerfully influence their motivation, engagement, and performance. This article explores the role of mindsets in organizational contexts. We start by describing the evolution of mindset theory and research and review why mindsets matter for people's workplace outcomes. We discuss some of the most common growth mindset misconceptions\textemdash termed ``false growth mindset''\textemdash that emerged as the fixed and growth mindset became popularized and (mis)applied in educational settings. We review literature on the situations that move people between their fixed and growth mindsets. Finally, we review new research on organizational mindsets and how organizations' mindset culture\textemdash communicated through its norms, policies, practices, and leadership messages\textemdash influences people's motivation and behavior in the workplace. We outline open theoretical and methodological questions as well as promising future directions for a forward-looking research agenda on mindsets at work. We suggest that extending mindset research\textemdash at the personal and organizational levels\textemdash to workplace contexts may shed new light on classic organizational behavior questions such as how to create more positive, innovative, and ethical organizational cultures; how to increase employee engagement; and how to reduce group-based disparities and inequalities in organizations.},
  keywords = {Behavior,Diversity,Implicit theories,Lay theories,Motivation,Organizational behavior,Organizational culture,Organizational mindset},
  file = {/home/cameron/Zotero/storage/L64I88FX/Murphy, Reeves - 2019 - Personal and organizational mindsets at work.pdf}
}

@article{Muscettola1998,
  title = {Reformulating {{Temporal Plans For Efficient Execution}}},
  author = {Muscettola, Nicola and Morris, Paul},
  year = {1998},
  journal = {Principles of Knowledge Representation and Reasoning},
  pages = {9},
  file = {/home/cameron/Zotero/storage/A25JEZN9/Muscettola, Morris - 1998 - Reformulating Temporal Plans For E cient Execution.pdf}
}

@article{Muscettola1998a,
  title = {Remote Agent: {{To}} Boldly Go Where No {{AI}} System Has Gone Before},
  author = {Muscettola, Nicola and Nayak, P. Pandurang and Pell, Barney and Williams, Brian C.},
  year = {1998},
  journal = {Artificial Intelligence},
  volume = {103},
  number = {1-2},
  pages = {5--47},
  issn = {00043702},
  doi = {10.1016/s0004-3702(98)00068-x},
  abstract = {Renewed motives for space exploration have inspired NASA to work toward the goal of establishing a virtual presence in space, through heterogeneous fleets of robotic explorers. Information technology, and Artificial Intelligence in particular, will play a central role in this endeavor by endowing these explorers with a form of computational intelligence that we call remote agents. In this paper we describe the Remote Agent, a specific autonomous agent architecture based on the principles of model-based programming, on-board deduction and search, and goal-directed closed-loop commanding, that takes a significant step toward enabling this future. This architecture addresses the unique characteristics of the spacecraft domain that require highly reliable autonomous operations over long periods of time with tight deadlines, resource constraints, and concurrent activity among tightly coupled subsystems. The Remote Agent integrates constraint-based temporal planning and scheduling, robust multi-threaded execution, and model-based mode identification and reconfiguration. The demonstration of the integrated system as an on-board controller for Deep Space One, NASA's first New Millennium mission, is scheduled for a period of a week in mid 1999. The development of the Remote Agent also provided the opportunity to reassess some of AI's conventional wisdom about the challenges of implementing embedded systems, tractable reasoning, and knowledge representation. We discuss these issues, and our often contrary experiences, throughout the paper. \textcopyright{} 1998 Published by Elsevier Science B.V.},
  keywords = {Architectures,Autonomous agents,Constraint-based planning,Diagnosis,Execution,Model-based reasoning,Reactive systems,Recovery,Scheduling},
  file = {/home/cameron/Zotero/storage/PB54J77V/Muscettola et al. - 1998 - Remote agent To boldly go where no AI system has gone before.pdf}
}

@article{Myronenko2010,
  title = {Point Set Registration: {{Coherent}} Point Drifts},
  author = {Myronenko, Andriy and Song, Xubo},
  year = {2010},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {32},
  number = {12},
  eprint = {0905.2635},
  eprinttype = {arxiv},
  pages = {2262--2275},
  publisher = {{IEEE}},
  issn = {01628828},
  doi = {10.1109/TPAMI.2010.46},
  abstract = {Point set registration is a key component in many computer vision tasks. The goal of point set registration is to assign correspondences between two sets of points and to recover the transformation that maps one point set to the other. Multiple factors, including an unknown nonrigid spatial transformation, large dimensionality of point set, noise, and outliers, make the point set registration a challenging problem. We introduce a probabilistic method, called the Coherent Point Drift (CPD) algorithm, for both rigid and nonrigid point set registration. We consider the alignment of two point sets as a probability density estimation problem. We fit the Gaussian mixture model (GMM) centroids (representing the first point set) to the data (the second point set) by maximizing the likelihood. We force the GMM centroids to move coherently as a group to preserve the topological structure of the point sets. In the rigid case, we impose the coherence constraint by reparameterization of GMM centroid locations with rigid parameters and derive a closed form solution of the maximization step of the EM algorithm in arbitrary dimensions. In the nonrigid case, we impose the coherence constraint by regularizing the displacement field and using the variational calculus to derive the optimal transformation. We also introduce a fast algorithm that reduces the method computation complexity to linear. We test the CPD algorithm for both rigid and nonrigid transformations in the presence of noise, outliers, and missing points, where CPD shows accurate results and outperforms current state-of-the-art methods. \textcopyright{} 2010 IEEE.},
  archiveprefix = {arXiv},
  keywords = {alignment,coherence,Coherent Point Drift (CPD),correspondence,EM algorithm,Gaussian mixture model (GMM),matching,nonrigid,point sets,Registration,regularization,rigid},
  file = {/home/cameron/Zotero/storage/LBTMQ96Z/Point_Set_Registration_Coherent_Point_Drift.pdf}
}

@misc{NASA2019,
  title = {Forward to the {{Moon}}: {{NASA}}'s {{Strategic Plan}} for {{Human Exploration}}},
  author = {{NASA}},
  year = {2019},
  pages = {20},
  file = {/home/cameron/Zotero/storage/Z2DSHXKL/m-api-53439b1f-c9bb-41c1-6253-78129d0275c1.pdf}
}

@book{NASAOfficeofInspectorGeneral2017,
  title = {{{NASA}}'s {{Management}} and {{Development}} of {{Spacesuits}}},
  author = {{NASA Office of Inspector General}},
  year = {2017},
  abstract = {IG-17-018 (A-16-014-00) Beginning with the Gemini 4 mission in June 1965, NASA astronauts have ventured outside their spacecraft hundreds of times wearing specialized suits that protect them from the harsh environments of space and provide the oxygen and temperature control necessary to preserve life. The spacesuits NASA astronauts currently use on the International Space Station (ISS or Station) \textendash{} known as Extravehicular Mobility Units (EMU) \textendash{} were developed more than 40 years ago and have far outlasted their original 15-year design life. While maintaining the existing fleet of EMUs for use on the ISS, the Agency has also spent almost \$200 million on three spacesuit development efforts to enable human exploration in deep space, including missions to Mars: the Constellation Space Suit System (\$135.6 million), Advanced Space Suit Project (\$51.6 million), and Orion Crew Survival System (\$12 million). A key part of these development efforts will be testing the next-generation spacesuit technologies on the ISS prior to its scheduled retirement in 2024.},
  isbn = {80-04-24918-3},
  file = {/home/cameron/Zotero/storage/CHXP66HW/IG-17-018.pdf}
}

@article{Nawotniak2019,
  title = {Tactical {{Scientific Decision-Making}} during {{Crewed Astrobiology Mars Missions}}},
  author = {Stevens, A. H. and Kobs Nawotniak, S. E. and Garry, W. Brent and Payler, Samuel J. and Brady, Allyson L. and Miller, Matthew J. and Beaton, Kara H. and Cockell, Charles S and Lim, Darlene S.S.},
  year = {2019},
  journal = {Astrobiology},
  volume = {19},
  number = {3},
  pages = {369--386},
  issn = {15311074},
  doi = {10.1089/ast.2018.1837},
  abstract = {The limitations placed upon human explorers on the surface of Mars will necessitate a methodology for scientific exploration that is different from standard approaches to terrestrial fieldwork and prior crewed exploration of the Moon. In particular, the data transmission limitations and communication latency between Earth and Mars create a unique situation for surface crew in contact with a terrestrial science team. The BASALT research program simulated a series of extravehicular activities (EVAs) in Mars analog terrains under various Mars-relevant bandwidth and latency conditions to investigate how best to approach this problem. Here we discuss tactical decision-making under these conditions, that is, how the crew on Mars interacts with a team of scientists and support personnel on Earth to collect samples of maximum scientific interest. We describe the strategies, protocols, and tools tested in BASALT EVAs and give recommendations on how best to conduct human exploration of Mars with support from Earth-based scientists. We find that even with scientists supporting them, the crew performing the exploration must be trained in the appropriate scientific disciplines in order to provide the terrestrial scientists with enough information to make decisions, but that with appropriate planning and structure, and tools such as a "dynamic leaderboard," terrestrial scientists can add scientific value to an EVA, even under Mars communication latency.},
  pmid = {30840503},
  keywords = {Decision-making,EVA,Mars,Science operations,Tactical},
  file = {/home/cameron/Zotero/storage/3WRES6IB/ast.2018.1837.pdf}
}

@article{Nicolin2020,
  title = {Agimus: {{A}} New Framework for Mapping Manipulation Motion Plans to Sequences of Hierarchical Task-Based Controllers},
  author = {Nicolin, A. and Mirabel, J. and Boria, S. and Stasse, O. and Lamiraux, F.},
  year = {2020},
  journal = {Proceedings of the 2020 IEEE/SICE International Symposium on System Integration, SII 2020},
  pages = {1022--1027},
  doi = {10.1109/SII46433.2020.9026288},
  abstract = {In this paper we present the integration of a manipulation motion planner involving multiple contacts with an automated generator of controllers to execute the manipulation tasks. The novelty of the method is not only to produce a configuration space trajectory but also automatically formulate the controllers that perform the tasks while keeping balance on a humanoid robot. We demonstrate this approach fully integrated on a real Talos humanoid robot while using controllers formulated as a sequence of hierarchical Stack-of-Tasks.},
  isbn = {9781728166674},
  file = {/home/cameron/Zotero/storage/367E4NRK/Nicolin et al. - 2020 - Agimus A new framework for mapping manipulation motion plans to sequences of hierarchical task-based controllers.pdf}
}

@article{Nilsson2013,
  title = {Incremental Dynamic Controllability Revisited},
  author = {Nilsson, Mikael and Kvarnstr{\"o}m, Jonas and Doherty, Patrick},
  year = {2013},
  journal = {ICAPS 2013 - Proceedings of the 23rd International Conference on Automated Planning and Scheduling},
  pages = {337--341},
  abstract = {Simple Temporal Networks with Uncertainty (STNUs) allow the representation of temporal problems where some durations are determined by nature, as is often the case for actions in planning. As such networks are generated it is essential to verify that they are dynamically controllable - executable regardless of the outcomes of uncontrollable durations - and to convert them to a dispatchable form. The previously published FastIDC algorithm achieves this incrementally and can therefore be used efficiently during plan construction. In this paper we show that FastIDC is not sound when new constraints are added, sometimes labeling networks as dynamically controllable when they are not. We analyze the algorithm, pinpoint the cause, and show how the algorithm can be modified to correctly detect uncontrollable networks. Copyright \textcopyright{} 2013, Association for the Advancement of Artificial Intelligence. All rights reserved.},
  isbn = {9781577356097},
  file = {/home/cameron/Zotero/storage/I2TV49T7/6028-30116-1-PB.pdf}
}

@article{Nilsson2014,
  title = {Incremental Dynamic Controllability in Cubic Worst-Case Time},
  author = {Nilsson, Mikael and Kvarnstrom, Jonas and Doherty, Patrick},
  year = {2014},
  journal = {Proceedings of the International Workshop on Temporal Representation and Reasoning},
  pages = {17--26},
  doi = {10.1109/TIME.2014.13},
  abstract = {It is generally hard to predict the exact duration of an action. Uncertainty in durations is often modeled in temporal planning by the use of upper bounds on durations, with the assumption that if an action happens to be executed more quickly, the plan will still succeed. However, this assumption is often false: If we finish cooking too early, the dinner will be cold before everyone is ready to eat. Simple Temporal Problems with Uncertainty (STPUs) allow us to model such situations. An STPU-based planner must verify that the plans it generates are executable, captured by the property of dynamic controllability. The Efficient IDC (EIDC) algorithm can do this incrementally during planning, with an amortized complexity per step of O(n3) but a worst-case complexity per step of O(n4). In this paper we show that the worst-case run-time of EIDC does occur, leading to repeated reprocessing of nodes in the STPU while verifying the dynamic controllability property. We present a new version of the algorithm, EIDC2, which through optimal ordering of nodes avoids the need for reprocessing. This gives EIDC2 a strictly lower worst-case run-time, making it the fastest known algorithm for incrementally verifying dynamic controllability of STPUs.},
  isbn = {9781479942275},
  keywords = {Dynamic Controllability,Incremental Algorithm,Planning,Temporal Networks},
  file = {/home/cameron/Zotero/storage/5TUYMMZ7/TIME-2014-Incremental-Dynamic-Controllability.pdf}
}

@article{nilsson2016,
  title = {Efficient Processing of Simple Temporal Networks with Uncertainty: Algorithms for Dynamic Controllability Verification},
  author = {Nilsson, Mikael and Kvarnstr{\"o}m, Jonas and Doherty, Patrick},
  year = {2016},
  journal = {Acta Informatica},
  volume = {53},
  number = {6-8},
  issn = {14320525},
  doi = {10.1007/s00236-015-0248-8},
  abstract = {Temporal formalisms are essential for reasoning about actions that are carried out over time. The exact durations of such actions are generally hard to predict. In temporal planning, the resulting uncertainty is often worked around by only considering upper bounds on durations, with the assumption that when an action happens to be executed more quickly, the plan will still succeed. However, this assumption is often false: if we finish cooking too early, the dinner will be cold before everyone is ready to eat. Using simple temporal networks with uncertainty (STNU), a planner can correctly take both lower and upper duration bounds into account. It must then verify that the plans it generates are executable regardless of the actual outcomes of the uncertain durations. This is captured by the property of dynamic controllability (DC), which should be verified incrementally during plan generation. Recently a new incremental algorithm for verifying dynamic controllability was proposed: EfficientIDC, which can verify if an STNU that is DC remains DC after the addition or tightening of a constraint (corresponding to a new action being added to a plan). The algorithm was shown to have a worst case complexity of O(n4) for each addition or tightening. This can be amortized over the construction of a whole STNU for an amortized complexity in O(n3). In this paper we improve the EfficientIDC algorithm in a way that prevents it from having to reprocess nodes. This improvement leads to a lower worst case complexity in O(n3).}
}

@inproceedings{Nissim2010,
  title = {A {{General}}, {{Fully Distributed Multi-Agent Planning Algorithm}}},
  booktitle = {Proceedings of the {{International Joint Conference}} on {{Autonomous Agents}} and {{Multiagent Systems}}, {{AAMAS}}},
  author = {Nissim, Raz and Brafman, Ronen I. and Domshlak, Carmel and Ixoo, H Suhvhqw D and Pxowl, Glvwulexwhg and Sodqqlqj, Djhqw and Ri, Frqvlvwhqf and Frruglqdwlrq, Wkhvh and Vroyh, Srlqwv and Glvwulexwhg, W K H and Wkh, Frqglwlrqv and Lv, Dqvzhu and Vshdnlqj, Srvlwlyh and Wkh, Zkhq and Doo, Vhqglqj and Wr, Lqirupdwlrq and Uho, D Q G and Rq, L Q J and Fhqwudo, Vrph and Ri, Vroxwlrq and Sureohpv, Sodqqlqj and Kdyh, Wkdw and Vwuxfwxuh, D Qdwxudo and Zlwk, Hdfk and Grpdlq, D Kxjh and Wkh, Y H Q and Vrsklvwlfdwhg, Prvw},
  year = {2010},
  number = {Aamas},
  pages = {1323--1330},
  issn = {15582914},
  abstract = {We present a fully distributed multi-agent planning algorithm. Our methodology uses distributed constraint satisfaction to coordinate between agents, and local planning to ensure the consistency of these coordination points. To solve the distributed CSP efficiently, we must modify existing methods to take advantage of the structure of the underlying planning problem. In multi-agent planning domains with limited agent interaction, our algorithm empirically shows scalability beyond state of the art centralized solvers. Our work also provides a novel, real-world setting for testing and evaluating distributed constraint satisfaction algorithms in structured domains and illustrates how existing techniques can be altered to address such structure. Copyright ?? 2010, International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.},
  isbn = {978-1-61738-771-5},
  keywords = {0xowl,Distributed Constraint Satisfaction,Distributed Problem Solving,jhqw 3odqqlqj,lvwulexwhg,lvwulexwhg 3ureohp 6roylqj,Multi-Agent Planning,rqvwudlqw 6dwlvidfwlrq 6lqjoh,Single-Agent Planning},
  file = {/home/cameron/Zotero/storage/D66TDVEE/Nissim et al. - 2010 - A General , Fully Distributed Multi-Agent Planning Algorithm.pdf;/home/cameron/Zotero/storage/DAWQV4V2/Nissim, Brafman, Domshlak - 2010 - A General, Fully Distributed Multi-Agent Planning Algorithm.pdf}
}

@article{Nissim2014,
  title = {Distributed {{Heuristic Forward Search}} for {{Multi-agent Planning}}},
  author = {Nissim, R. and Brafman, R.},
  year = {2014},
  journal = {Journal of Artificial Intelligence Research},
  volume = {51},
  eprint = {1306.5858},
  eprinttype = {arxiv},
  pages = {293--332},
  issn = {1076-9757},
  doi = {10.1613/jair.4295},
  abstract = {This paper deals with the problem of classical planning for multiple cooperative agents who have private information about their local state and capabilities they do not want to reveal. Two main approaches have recently been proposed to solve this type of problem -- one is based on reduction to distributed constraint satisfaction, and the other on partial-order planning techniques. In classical single-agent planning, constraint-based and partial-order planning techniques are currently dominated by heuristic forward search. The question arises whether it is possible to formulate a distributed heuristic forward search algorithm for privacy-preserving classical multi-agent planning. Our work provides a positive answer to this question in the form of a general approach to distributed state-space search in which each agent performs only the part of the state expansion relevant to it. The resulting algorithms are simple and efficient -- outperforming previous algorithms by orders of magnitude -- while offering similar flexibility to that of forward-search algorithms for single-agent planning. Furthermore, one particular variant of our general approach yields a distributed version of the A* algorithm that is the first cost-optimal distributed algorithm for privacy-preserving planning.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/2U7ZDK2F/heuristic_forward.pdf}
}

@article{None2020,
  title = {3 {{Convex}} Sets and Their Representation 3.1},
  author = {{None}},
  year = {2020},
  volume = {2},
  file = {/home/cameron/Zotero/storage/D9HYI4PX/None - 2020 - 3 Convex sets and their representation 3.1.1}
}

@techreport{Office2012,
  title = {Significant {{Incidents}} and {{Close Calls}} in {{Human Spaceflight}}},
  author = {Office, S\&MA Flight Safety},
  year = {2012},
  pages = {2},
  institution = {{National Aeronautics and Space Administration}},
  file = {/home/cameron/Zotero/storage/2IV5GGXC/Significant_Incidents.pdf}
}

@article{Olfati-Saber2007,
  title = {Distributed {{Kalman}} Filtering for Sensor Networks},
  author = {{Olfati-Saber}, R.},
  year = {2007},
  journal = {Proceedings of the IEEE Conference on Decision and Control},
  pages = {5492--5498},
  issn = {01912216},
  doi = {10.1109/CDC.2007.4434303},
  abstract = {In this paper, we introduce three novel distributed Kalman filtering (DKF) algorithms for sensor networks. The first algorithm is a modification of a previous DKF algorithm presented by the author in CDC-ECC '05. The previous algorithm was only applicable to sensors with identical observation matrices which meant the process had to be observable by every sensor. The modified DKF algorithm uses two identical consensus filters for fusion of the sensor data and covariance information and is applicable to sensor networks with different observation matrices. This enables the sensor network to act as a collective observer for the processes occurring in an environment. Then, we introduce a continuous-time distributed Kalman filter that uses local aggregation of the sensor data but attempts to reach a consensus on estimates with other nodes in the network. This peer-to-peer distributed estimation method gives rise to two iterative distributed Kalman filtering algorithms with different consensus strategies on estimates. Communication complexity and packet-loss issues are discussed. The performance and effectiveness of these distributed Kalman filtering algorithms are compared and demonstrated on a target tracking task. \textcopyright{} 2007 IEEE.},
  isbn = {1424414989},
  keywords = {Consensus filtering,Distributed Kalman filtering,Sensor fusion,Sensor networks},
  file = {/home/cameron/Zotero/storage/FM8LVP6P/04434303.pdf}
}

@article{Ongaro2019,
  title = {In Search of an Understandable Consensus Algorithm},
  author = {Ongaro, Diego and Ousterhout, John},
  year = {2019},
  journal = {Proceedings of the 2014 USENIX Annual Technical Conference, USENIX ATC 2014},
  pages = {305--319},
  abstract = {Raft is a consensus algorithm for managing a replicated log. It produces a result equivalent to (multi-)Paxos, and it is as efficient as Paxos, but its structure is different from Paxos; this makes Raft more understandable than Paxos and also provides a better foundation for building practical systems. In order to enhance understandability, Raft separates the key elements of consensus, such as leader election, log replication, and safety, and it enforces a stronger degree of coherency to reduce the number of states that must be considered. Results from a user study demonstrate that Raft is easier for students to learn than Paxos. Raft also includes a new mechanism for changing the cluster membership, which uses overlapping majorities to guarantee safety.},
  isbn = {9781931971102},
  file = {/home/cameron/Zotero/storage/7QH3P447/raft.pdf}
}

@article{Ono2008,
  title = {Iterative Risk Allocation: {{A}} New Approach to Robust {{Model Predictive Control}} with a Joint Chance Constraint},
  author = {Ono, Masahiro and Williams, Brian C.},
  year = {2008},
  journal = {Proceedings of the IEEE Conference on Decision and Control},
  number = {6},
  pages = {3427--3432},
  issn = {01912216},
  doi = {10.1109/CDC.2008.4739221},
  abstract = {This paper proposes a novel two-stage optimization method for robust Model Predictive Control (RMPC) with Gaussian disturbance and state estimation error. Since the disturbance is unbounded, it is impossible to achieve zero probability of violating constraints. Our goal is to optimize the expected value of an objective function while limiting the probability of violating any constraints over the planning horizon (joint chance constraint). Prior arts include ellipsoidal relaxation approach[1] and Particle Control[2], but the former yields very conservative result and the latter is computationally intensive. Our approach divide the optimization problem into two stages; the upper-stage that optimizes risk allocation, and the lower-stage that optimizes control sequence with tightened constraints. The lower-stage is a regular convex optimization, such as Linear Programming or Quadratic Programming. The upper-stage is also convex, but the objective function is not always differentiable. We developed a fast descent algorithm for the upper-stage called Iterative Risk Allocation (IRA), which yield much smaller suboptimality than ellipsoidal relaxation method while achieving a substantial speedup compared to and Particle Control. \textcopyright{} 2008 IEEE.},
  isbn = {9781424431243},
  keywords = {Predictive control for linear systems,Robust control,Stochastic optimal control},
  file = {/home/cameron/Zotero/storage/6GCV468A/lec-06-reading-1.pdf;/home/cameron/Zotero/storage/GDKYABYP/CDC_ono_revised.pdf}
}

@article{Ono2013,
  title = {Probabilistic Planning for Continuous Dynamic Systems under Bounded Risk},
  author = {Ono, Masahiro and Williams, Brian C. and Blackmore, Lars},
  year = {2013},
  journal = {Journal of Artificial Intelligence Research},
  volume = {46},
  pages = {511--577},
  issn = {10769757},
  doi = {10.1613/jair.3893},
  abstract = {This paper presents a model-based planner called the Probabilistic Sulu Planner or the p-Sulu Planner, which controls stochastic systems in a goal directed manner within user-specified risk bounds. The objective of the p-Sulu Planner is to allow users to command continuous, stochastic systems, such as unmanned aerial and space vehicles, in a manner that is both intuitive and safe. To this end, we first develop a new plan representation called a chance-constrained qualitative state plan (CCQSP), through which users can specify the desired evolution of the plant state as well as the acceptable level of risk. An example of a CCQSP statement is "go to A through B within 30 minutes, with less than 0.001\% probability of failure." We then develop the p-Sulu Planner, which can tractably solve a CCQSP planning problem. In order to enable CCQSP planning, we develop the following two capabilities in this paper: 1) risk-sensitive planning with risk bounds, and 2) goal-directed planning in a continuous domain with temporal constraints. The first capability is to ensures that the probability of failure is bounded. The second capability is essential for the planner to solve problems with a continuous state space such as vehicle path planning. We demonstrate the capabilities of the p-Sulu Planner by simulations on two real-world scenarios: the path planning and scheduling of a personal aerial vehicle as well as the space rendezvous of an autonomous cargo spacecraft. \textcopyright{} 2013 AI Access Foundation. All rights reserved.},
  file = {/home/cameron/Zotero/storage/W674DSBV/lec-06-reading-2.pdf}
}

@incollection{Oppenheim1983,
  title = {Signals and {{Systems Part I}}},
  booktitle = {Signals and {{Systems}}},
  author = {Oppenheim, Alan V.},
  year = {1983},
  volume = {1},
  pages = {1--9},
  publisher = {{MIT}},
  file = {/home/cameron/Zotero/storage/Z5T84ZKU/m-api-032380da-c802-7329-2b3c-d26fc25ad52d.pdf}
}

@article{Orabona2019,
  title = {A {{Modern Introduction}} to {{Online Learning}}},
  author = {Orabona, Francesco},
  year = {2019},
  eprint = {1912.13213},
  eprinttype = {arxiv},
  abstract = {In this monograph, I introduce the basic concepts of Online Learning through a modern view of Online Convex Optimization. Here, online learning refers to the framework of regret minimization under worst-case assumptions. I present first-order and second-order algorithms for online learning with convex losses, in Euclidean and non-Euclidean settings. All the algorithms are clearly presented as instantiation of Online Mirror Descent or Follow-The-Regularized-Leader and their variants. Particular attention is given to the issue of tuning the parameters of the algorithms and learning in unbounded domains, through adaptive and parameter-free online learning algorithms. Non-convex losses are dealt through convex surrogate losses and through randomization. The bandit setting is also briefly discussed, touching on the problem of adversarial and stochastic multi-armed bandits. These notes do not require prior knowledge of convex analysis and all the required mathematical tools are rigorously explained. Moreover, all the proofs have been carefully chosen to be as simple and as short as possible.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/S6GYY98K/1912.13213.pdf}
}

@article{Paakko2001,
  title = {Bayesian {{Networks}} for {{Advanced FDIR}}},
  author = {Paakko, M and Myllym{\"a}ki, P and Holsti, N and Tirri, H},
  year = {2001},
  journal = {Estec.Esa.Nl},
  number = {May 2014},
  file = {/home/cameron/Zotero/storage/2P938UZP/Bayesian_Networks_for_Advanced_FDIR (1).pdf}
}

@article{PandurangNayak1997,
  title = {Fast Context Switching in Real-Time Propositional Reasoning},
  author = {Pandurang Nayak, P. and Williams, Brian C.},
  year = {1997},
  journal = {Proceedings of the National Conference on Artificial Intelligence},
  pages = {50--56},
  abstract = {The trend to increasingly capable and affordable control processors has generated an explosion of embedded real-time gadgets that serve almost every function imaginable. The daunting task of programming these gadgets is greatly alleviated with real-time deductive engines that perform all execution and monitoring functions from a single core model. Fast response times are achieved using an incremental propositional deductive database (an LTMS). Ideally the cost of an LTMS's incremental update should be linear in the number of labels that change between successive contexts. Unfortunately an LTMS can expend a significant percentage of its time working on labels that remain constant between contexts. This is caused by the LTMS's conservative approach: a context switch first removes all consequences of deleted clauses, whether or not those consequences hold in the new context. This paper presents a more aggressive incremental TMS, called the ITMS, that avoids processing a significant number of these consequences that are unchanged. Our empirical evaluation for spacecraft control shows that the overhead of processing unchanged consequences can be reduced by a factor of seven.},
  file = {/home/cameron/Zotero/storage/6L5HY2MX/Pandurang Nayak, Williams - 1997 - Fast context switching in real-time propositional reasoning.pdf}
}

@book{Papadimitriou1982,
  title = {Combinatorial Optimization:{{Algorithms}} and Complexity},
  author = {Papadimitriou, Christos H. and Steiglitz, Kenneth},
  year = {1982},
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {32},
  issn = {0096-3518},
  doi = {10.1109/tassp.1984.1164450},
  file = {/home/cameron/Zotero/storage/EGQM3VZE/Combinatorial_Optimization_Algorithms_and_Complexi.pdf}
}

@article{Patterson1999,
  title = {Voice {{Loops}} as {{Coordination Aids}} in {{Space Shuttle Mission Control}}},
  author = {Patterson, Emily S. and {Watts-Perotti}, Jennifer and Woods, David D.},
  year = {1999},
  journal = {Computer Supported Cooperative Work},
  volume = {8},
  number = {4},
  pages = {353--371},
  issn = {09259724},
  doi = {10.1023/A:1008722214282},
  abstract = {Voice loops, an auditory groupware technology, are essential coordination support tools for experienced practitioners in domains such as air traffic management, aircraft carrier operations and space shuttle mission control. They support synchronous communication on multiple channels among groups of people who are spatially distributed. In this paper, we suggest reasons for why the voice loop system is a successful medium for supporting coordination in space shuttle mission control based on over 130 hours of direct observation. Voice loops allow practitioners to listen in on relevant communications without disrupting their own activities or the activities of others. In addition, the voice loop system is structured around the mission control organization, and therefore directly supports the demands of the domain. By understanding how voice loops meet the particular demands of the mission control environment, insight can be gained for the design of groupware tools to support cooperative activity in other event-driven domains.},
  pmid = {12269347},
  keywords = {Attention,Broadcasting,Common ground,Coordination,Ethnographic study,Mission control,Mutual awareness,Overhearing,Voice loops},
  file = {/home/cameron/Zotero/storage/L6TNTLU2/Patterson, Watts-Perotti, Woods - 1999 - Voice Loops as Coordination Aids in Space Shuttle Mission Control.pdf}
}

@article{Patterson2001,
  title = {Shift Changes, Updates, and the on-Call Architecture in Space Shuttle Mission Control},
  author = {Patterson, Emily S. and Woods, David D.},
  year = {2001},
  journal = {Computer Supported Cooperative Work},
  volume = {10},
  number = {3-4},
  pages = {27},
  issn = {09259724},
  doi = {10.1023/A:1012705926828},
  abstract = {In domains such as nuclear power, industrial process control, and space shuttle mission control, there is increased interest in reducing personnel during nominal operations. An essential element in maintaining safe operations in high risk environments with this 'on-call' organizational architecture is to understand how to bring called-in practitioners up to speed quickly during escalating situations. Targeted field observations were conducted to investigate what it means to update a supervisory controller on the status of a continuous, anomaly-driven process in a complex, distributed environment. Sixteen shift changes, or handovers, at the NASA Johnson Space Center were observed during the STS-76 Space Shuttle mission. The findings from this observational study highlight the importance of prior knowledge in the updates and demonstrate how missing updates can leave flight controllers vulnerable to being unprepared. Implications for mitigating risk in the transition to 'on-call' architectures are discussed.},
  isbn = {0925-9724},
  pmid = {12269342},
  keywords = {anomaly,Anomaly,common ground,Common ground,decision,Decision,ethnography,Ethnography,event,Event,knowledge,Knowledge,mutual awareness,Mutual awareness,observation,Observation,plan,Plan,shift change,Shift change,update,Update},
  file = {/home/cameron/Zotero/storage/7LU2UM54/Patterson, Woods - 2001 - Shift changes, updates, and the on-call architecture in space shuttle mission control.pdf}
}

@article{Payler2019b,
  title = {Developing {{Intra-EVA Science Support Team Practices}} for a {{Human Mission}} to {{Mars}}},
  author = {Payler, Samuel J. and Mirmalek, Z and Hughes, Scott S. and Kobs Nawotniak, S. E. and Brady, Allyson L. and Stevens, Adam H. and Cockell, Charles S and Lim, Darlene S.S.},
  year = {2019},
  journal = {Astrobiology},
  volume = {19},
  number = {3},
  pages = {387--400},
  issn = {15311074},
  doi = {10.1089/ast.2018.1846},
  abstract = {During the BASALT research program, real (nonsimulated) geological and biological science was accomplished through a series of extravehicular activities (EVAs) under simulated Mars mission conditions. These EVAs were supported by a Mission Support Center (MSC) that included an on-site, colocated Science Support Team (SST). The SST was composed of scientists from a variety of disciplines and operations researchers who provided scientific and technical expertise to the crew while each EVA was being conducted (intra-EVA). SST management and organization developed under operational conditions that included Mars-like communication latencies, bandwidth constraints, and EVA plans that were infused with Mars analog field science objectives. This paper focuses on the SST workspace considerations such as science team roles, physical layout, communication interactions, operational techniques, and work support technology. Over the course of BASALT field deployments to Idaho and Hawai'i, the SST team made several changes of note to increase both productivity and efficiency. For example, new roles were added for more effective management of technical discussions, and the layout of the SST workspace evolved multiple times during the deployments. SST members' reflexive adjustments resulted in a layout that prioritized face-To-face discussions over face-To-data displays, highlighting the importance of interpersonal communication during SST decision-making. In tandem with these workspace adjustments, a range of operational techniques were developed to help the SST manage discussions and information flow under time pressure.},
  pmid = {30840508},
  keywords = {Analog,EVA,Mars,Science operations,Workspace.},
  file = {/home/cameron/Zotero/storage/7EKJAHTY/ast.2018.1846.pdf}
}

@article{Pearl1982,
  title = {Reverend {{Bayes}} on {{Inference Engines}}: A {{Distributed Hierarchical Approach}}.},
  author = {Pearl, Judea},
  year = {1982},
  pages = {133--136},
  isbn = {0865760438},
  file = {/home/cameron/Zotero/storage/KN6V6BJZ/Pearl - 1982 - Reverend Bayes on Inference Engines a Distributed Hierarchical Approach.pdf}
}

@article{Pessolano2019,
  title = {Forensic {{Analysis}} of the {{Nintendo 3DS NAND}}},
  author = {Pessolano, Gus and Read, Huw O.L. and Sutherland, Iain and Xynos, Konstantinos},
  year = {2019},
  journal = {Digital Investigation},
  volume = {29},
  pages = {S61-S70},
  publisher = {{Elsevier Ltd}},
  issn = {17422876},
  doi = {10.1016/j.diin.2019.04.015},
  abstract = {Games consoles present a particular challenge to the forensics investigator due to the nature of the hardware and the inaccessibility of the file system. Many protection measures are put in place to make it deliberately difficult to access raw data in order to protect intellectual property, enhance digital rights management of software and, ultimately, to protect against piracy. History has shown that many such protections on game consoles are circumvented with exploits leading to jailbreaking/rooting and allowing unauthorized software to be launched on the games system. This paper details methods that enable the investigator to extract system activity, deleted images, Internet history items, relevant friends list information, the console's serial number and plaintext WiFi access point passwords. This is all possible with the use of publicly available, open-source security circumvention techniques that perform a non-invasive physical dump of the internal NAND storage of the Nintendo 3DS handheld device. It will also be shown that forensic integrity is maintained and a detailed analysis is possible without altering original evidence.},
  keywords = {Dump,Games console,NAND,Nintendo 3DS,Physical extraction,Piracy},
  file = {/home/cameron/Zotero/storage/T29RUX4Q/Pessolano et al. - 2019 - Forensic Analysis of the Nintendo 3DS NAND.pdf}
}

@article{Phinney2015,
  title = {Science {{Training History}} of the {{Apollo Astronauts}}},
  author = {Phinney, William C},
  year = {2015},
  abstract = {Following President Kennedy\&rsquo;s initiation of Project Apollo, NASA underwent substantial changes in personnel, organization, and programs and faced a major question: what to do on the Moon after landing. Once a decision that science activities, particularly geoscience, should be pursued, considerable debate ensued over how to accomplish this. Questions arose over instruments and tools required, samples and photos to be returned, landing site selection, and crew composition. Answers to these questions required major efforts for planning traverses on the Moon and training the astronauts in the extensive procedures necessary in low gravity to use tools, set up instruments, take adequate photos, collect and document samples, and provide proper descriptions. In addition to astronauts on the surface, an astronaut in lunar orbit managed additional instruments, photography and verbal descriptions. Training for these activities averaged nearly one hundred hours per month for over a year for each crew. There were many problems as the training progressed: adjusting groups and backgrounds of the training personnel for the best combination of personalities and skills, overcoming logistical troubles, revising awkward procedures, determining optimum means of communications between all involved groups, and devising contingency procedures for real-time problems. By the last mission these problems were overcome.},
  keywords = {â˜…,Astronauts Training of,Moon Exploration,Project Apollo (U.S.)},
  file = {/home/cameron/Zotero/storage/AWJ9LLEF/Phinney_NASA-SP-2015-626.pdf}
}

@article{Piotrowski2016,
  title = {Heuristic {{Planning}} for {{PDDL}}+ Domains},
  author = {Piotrowski, Wiktor and Fox, Maria and Long, Derek and Magazzeni, Daniele and Mercorio, Fabio},
  year = {2016},
  journal = {AAAI Workshop - Technical Report},
  volume = {WS-16-01 -},
  pages = {615--623},
  abstract = {Planning with hybrid domains modelled in PDDL+ has been gaining research interest in the Automated Planning community in recent years. Hybrid domain models capture a more accurate representation of real world problems that involve continuous processes than is possible using discrete systems. However, solving problems represented as PDDL+ domains is very challenging due to the construction of complex system dynamics, including non-linear processes and events. In this paper we introduce DiNo, a new planner capable of tackling complex problems with non-linear system dynamics governing the continuous evolution of states. DiNo is based on the discretise-and-validate approach and uses the novel Staged Relaxed Planning Graph+ (SRPG+) heuristic, which is introduced in this paper. Although several planners have been developed to work with subsets of PDDL+ features, or restricted forms of processes, DiNo is currently the only heuristic planner capable of handling non-linear system dynamics combined with the full PDDL+ feature set.},
  isbn = {9781577357599},
  keywords = {Planning for Hybrid Systems: Technical Report WS-1},
  file = {/home/cameron/Zotero/storage/DY2DRIP7/Piotrowski et al. - 2016 - Heuristic Planning for PDDL domains.pdf}
}

@article{pittman2009,
  title = {Blazar {{Microvariations Cameron Pittman Senior Honors Thesis Advisor}} : {{Dr}} . {{Robert Knop Department}} of {{Physics}} and {{Astronomy Spring}} 2009},
  author = {Pittman, Cameron W. and Knop, Robert},
  year = {2009},
  file = {/home/cameron/Zotero/storage/9G8CMSJG/PittmanThesis.pdf}
}

@article{Pittman2013,
  title = {Teaching with Portals: {{The}} Intersection of Video Games and Physics Education},
  author = {Pittman, Cameron W.},
  year = {2013},
  journal = {LEARNing Landscapes},
  volume = {6},
  number = {2},
  pages = {341--360},
  abstract = {The author, a high school physics teacher, describes the process of teaching with the commercial video game Portal 2. He gives his story from inception, through setbacks, to eventually teaching a semester of laboratories using the Portal 2 Puzzle Maker, a tool which allows for the easy conception and construction of levels. He describes how his students used the Puzzle Maker as a laboratory tool to build and analyze virtual experiments that followed real-world laws of physics. Finally, he concludes with a discussion on the current and future status of video games in education.},
  file = {/home/cameron/Zotero/storage/56VT8LHW/620-Article Text-389-1-10-20170227.pdf}
}

@article{Portree1997,
  title = {Walking to {{Olympus}}: {{An EVA Chronology}}},
  author = {Portree, David S F and Trevi{\~n}o, Robert C},
  year = {1997},
  number = {October},
  pages = {146},
  publisher = {{NASA History Office}},
  address = {{Washington DC. USA}},
  file = {/home/cameron/Zotero/storage/MHHIYV6F/m-api-77f004c7-d3fb-179b-1777-b22c20890a9d.pdf}
}

@article{Pralet2014,
  title = {Time-Dependent Simple Temporal Networks: {{Properties}} and Algorithms},
  author = {Pralet, C{\'e}dric and Verfaillie, G{\'e}rard},
  year = {2014},
  journal = {Proceedings International Conference on Automated Planning and Scheduling, ICAPS},
  volume = {2014-Janua},
  number = {January},
  pages = {536--539},
  issn = {23340843},
  doi = {10.1051/ro/2013033},
  abstract = {Simple Temporal Networks (STNs) allow minimum and maximum distance constraints between time-points to be represented. They are often used when tackling planning and scheduling problems that involve temporal aspects. This paper is a summary of the journal article Time-dependent Simple Temporal Networks: Properties and Algorithms published in RAIRO - Operations Research. This journal article introduces an extension of STN called Time-dependent STN (TSTN), which covers temporal constraints for which the temporal distance required between two time-points is not necessarily constant. Such constraints are useful to model time-dependent scheduling problems, in which the duration of an activity may depend on its starting time. The paper introduces the TSTN framework, its properties, resolution techniques, as well as examples of applications.},
  keywords = {Journal Presentation Track},
  file = {/home/cameron/Zotero/storage/JSTD2UHX/8667-37050-1-PB.pdf}
}

@book{Program2004,
  title = {R Oving {{Vehicles}} for {{Lunar}} and {{Planetary Exploration Roving Vehicles}} For},
  author = {Program, Technical Information},
  year = {2004},
  number = {January},
  isbn = {2-00-301107-2},
  file = {/home/cameron/Zotero/storage/NZSBPKRR/20040045218.pdf}
}

@article{rabiner1989,
  title = {A {{Tutorial}} on {{Hidden Markov Models}} and {{Selected Applications}} in {{Speech Recognition}}},
  author = {Rabiner, Lawrence R.},
  year = {1989},
  journal = {Proceedings of the IEEE},
  volume = {77},
  number = {2},
  pages = {257--286},
  issn = {0001-4966},
  doi = {10.1121/1.398355},
  file = {/home/cameron/Zotero/storage/S8ET9LQE/rabiner.pdf}
}

@article{Rackauckas2017,
  title = {{{DifferentialEquations}}.Jl \textendash{} {{A Performant}} and {{Feature-Rich Ecosystem}} for {{Solving Differential Equations}} in {{Julia}}},
  author = {Rackauckas, Christopher and Nie, Qing},
  year = {2017},
  journal = {Journal of Open Research Software},
  volume = {5},
  number = {May},
  issn = {2049-9647},
  doi = {10.5334/jors.151},
  abstract = {DifferentialEquations.jl is a package for solving differential equations in Julia. It covers discrete equations (function maps, discrete stochastic (Gillespie/Markov) simulations), ordinary differential equations, stochastic differential equations, algebraic differential equations, delay differential equations, hybrid differential equations, jump diffusions, and (stochastic) partial differential equations. Through extensive use of multiple dispatch, metaprogramming, plot recipes, foreign function interfaces (FFI), and call-overloading, DifferentialEquations.jl offers a unified user interface to solve and analyze various forms of differential equations while not sacrificing features or performance. Many modern features are integrated into the solvers, such as allowing arbitrary user-defined number systems for high-precision and arithmetic with physical units, built-in multithreading and parallelism, and symbolic calculation of Jacobians. Integrated into the package is an algorithm testing and benchmarking suite to both ensure accuracy and serve as an easy way for researchers to develop and distribute their own methods. Together, these features build a highly extendable suite which is feature-rich and highly performant.},
  keywords = {and dms1161621,and nsf grants dms1562176,equations,funding statement,high-precision,julia,metaprogramming,multiple dispatch,multithreading,ordinary differential equations,p50gm76516 and r01gm107264,partial differential,stochastic differential equations,supported by nih grants,the,this material is based,this work was partially,upon work supported by},
  file = {/home/cameron/Zotero/storage/6K8QEJ8T/151-1934-1-PB.pdf}
}

@article{Rackauckas2020,
  title = {Generalized Physics-Informed Learning through Language-Wide Differentiable Programming},
  author = {Rackauckas, Chris and Edelman, Alan and Fischer, Keno and Innes, Mike and Saba, Elliot and Shah, Viral B. and Tebbutt, Will},
  year = {2020},
  journal = {CEUR Workshop Proceedings},
  volume = {2587},
  issn = {16130073},
  abstract = {Scientific computing is increasingly incorporating the advancements in machine learning to allow for data-driven physics-informed modeling approaches. However, re-targeting existing scientific computing workloads to machine learning frameworks is both costly and limiting, as scientific simulations tend to use the full feature set of a general purpose programming language. In this manuscript we develop an infrastructure for incorporating deep learning into existing scientific computing code through Differentiable Programming ({$\partial$}P). We describe a {$\partial$}P system that is able to take gradients of full Julia programs, making Automatic Differentiation a first class language feature and compatibility with deep learning pervasive. Our system utilizes the one-language nature of Julia package development to augment the existing package ecosystem with deep learning, supporting almost all language constructs (control flow, recursion, mutation, etc.) while generating high-performance code without requiring any user intervention or refactoring to stage computations. We showcase several examples of physics-informed learning which directly utilizes this extension to existing simulation code: neural surrogate models, machine learning on simulated quantum hardware, and data-driven stochastic dynamical model discovery with neural stochastic differential equations.},
  file = {/home/cameron/Zotero/storage/LAYPS3EC/article_8.pdf}
}

@article{Rackauckas2020a,
  title = {Universal Differential Equations for Scientific Machine Learning},
  author = {Rackauckas, Christopher and Ma, Yingbo and Martensen, Julius and Warner, Collin and Zubov, Kirill and Supekar, Rohit and Skinner, Dominic and Ramadhan, Ali},
  year = {2020},
  journal = {arXiv},
  eprint = {2001.04385},
  eprinttype = {arxiv},
  abstract = {In the context of science, the well-known adage ``a picture is worth a thousand words'' might well be ``a model is worth a thousand datasets.'' Scientific models, such as Newtonian physics or biological gene regulatory networks, are human-driven simplifications of complex phenomena that serve as surrogates for the countless experiments that validated the models. Recently, machine learning has been able to overcome the inaccuracies of approximate modeling by directly learning the entire set of nonlinear interactions from data. However, without any predetermined structure from the scientific basis behind the problem, machine learning approaches are flexible but data-expensive, requiring large databases of homogeneous labeled training data. A central challenge is reconciling data that is at odds with simplified models without requiring ``big data''. In this work we develop a new methodology, universal differential equations (UDEs), which augments scientific models with machine-learnable structures for scientifically-based learning. We show how UDEs can be utilized to discover previously unknown governing equations, accurately extrapolate beyond the original data, and accelerate model simulation, all in a time and data-efficient manner. This advance is coupled with open-source software that allows for training UDEs which incorporate physical constraints, delayed interactions, implicitly-defined events, and intrinsic stochasticity in the model. Our examples show how a diverse set of computationally-difficult modeling issues across scientific disciplines, from automatically discovering biological mechanisms to accelerating climate simulations by 15,000x, can be handled by training UDEs.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/F6NBARES/2001.04385.pdf}
}

@techreport{rader,
  title = {Human-in-the-{{Loop Operations}} over {{Time Delay}}: {{NASA Analog Missions Lessons Learned}}},
  author = {Rader, Steven N and Reagan, Marcum L and Janoiko, Barbara and Johnson, James E},
  abstract = {Teams at NASA have conducted studies of time-delayed communications as it effects human exploration. In October 2012, the Advanced Exploration Systems (AES) Analog Missions project conducted a Technical Interchange Meeting (TIM) with the primary stakeholders to share information and experiences of studying time delay, to build a coherent picture of how studies are covering the problem domain, and to determine possible forward plans (including how to best communicate study results and lessons learned, how to inform future studies and mission plans, and how to drive potential development efforts). This initial meeting's participants included personnel from multiple NASA centers (HQ, JSC, KSC, ARC, and JPL), academia, and ESA. It included all of the known studies, analog missions, and tests of time delayed communications dating back to the Apollo missions including NASA Extreme Environment Mission Operations (NEEMO), Desert Research and Technology Studies (DRATS/RATS), International Space Station Test-bed for Analog Research (ISTAR), Pavilion Lake Research Project (PLRP), Mars 520, JPL Mars Orbiters/Rovers, Advanced Mission Operations (AMO), Devon Island analog missions, and Apollo experiences. Additionally, the meeting attempted to capture all of the various functional perspectives via presentations by disciplines including mission operations (flight director and mission planning), communications, crew, Capcom, Extra-Vehicular Activity (EVA), Behavioral Health and Performance (BHP), Medical/Surgeon, Science, Education and Public Outreach (EPO), and data management. The paper summarizes the descriptions and results from each of the activities discussed at the TIM and includes several recommendations captured in the meeting for dealing with time delay in human exploration along with recommendations for future development and studies to address this issue.},
  file = {/home/cameron/Zotero/storage/IBRS47FE/humans-in-the-loop.pdf}
}

@article{Rader2012,
  title = {Human-in-the-Loop Operations over Time Delay: {{NASA Analog Mission Lessons Learned}}},
  author = {Rader, Steven N. and Reagan, Marcum L. and Janoiko, Barbara and Johnson, James E.},
  year = {2012},
  journal = {American Institute of Aeronautics and Astronautics},
  pages = {1--33},
  doi = {doi:10.2514/6.2013-3520},
  abstract = {Teams at NASA have conducted studies of time-delayed communications as it effects human exploration.},
  isbn = {9781624102158},
  file = {/home/cameron/Zotero/storage/JYCWNC79/humans-in-the-loop.pdf}
}

@article{Radford2021,
  title = {Learning {{Transferable Visual Models From Natural Language Supervision}}},
  author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
  year = {2021},
  eprint = {2103.00020},
  eprinttype = {arxiv},
  abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/JLV963RR/Learning Transferable Visual Models from Natural Language Supervision- Radford et al. 2021.pdf}
}

@article{Rahaman2019,
  title = {On the Spectral Bias of Neural Networks},
  author = {Rahaman, Nasim and Baratin, Aristide and Arpit, Devansh and Draxlcr, Felix and Lin, Min and Hamprecht, Fred A. and Bengio, Yoshua and Courville, Aaron},
  year = {2019},
  journal = {36th International Conference on Machine Learning, ICML 2019},
  volume = {2019-June},
  number = {1},
  eprint = {1806.08734},
  eprinttype = {arxiv},
  pages = {9230--9239},
  abstract = {Neural networks are known to be a class of highly expressive functions able to fit even random input-output mappings with 100\% accuracy. In this work we present properties of neural networks that complement this aspect of expressivity. By using tools from Fourier analysis, we highlight a learning bias of deep networks towards low frequency functions - i.e. functions that vary globally without local fluctuations - which manifests itself as a frequency-dependent learning speed. Intuitively, this property is in line with the observation that over-parameterized networks prioritize learning simple patterns that generalize across data samples. We also investigate the role of the shape of the data manifold by presenting empirical and theoretical evidence that, somewhat counter-intuitively, learning higher frequencies gets easier with increasing manifold complexity.},
  archiveprefix = {arXiv},
  isbn = {9781510886988},
  file = {/home/cameron/Zotero/storage/2LYUJVKK/1806.08734.pdf}
}

@article{Raiman,
  title = {Semantic {{Relaxation}} : {{Interactive Plan Resolution Through Recommender Systems}}},
  author = {Raiman, Jonathan and Yu, Peng and Williams, Brian},
  abstract = {In the past, methods for incorporating preferences and se-mantics into decision aids and plan relaxations relied on static objective functions, or were specifically tuned to a domain and did not generalize or took significant time to construct. Moreover, these approaches either do not pro-vide an ability to incorporate feedback, or only incorporate feedback as hard constraints without any inference. Our approach uses online feedback from users and greatly automates the modeling process by incorporating web data in categorical and textual form into the model. This al-gorithm makes decision aids more robust in large domains by discovering a hierarchy over the variables, and increases flexibility by learning simultaneously from textual and cat-egorical data. Furthermore, semantics provide an efficient way of searching through assignments. In addition, we in-tegrate our semantic relaxation algorithm with a temporal relaxation algorithm to obtain feedback on preferences over both semantic and temporal relations, making the decision aid applicable to more real world scenarios. Empirical re-sults demonstrate that our method aligns well with human judgment when making recommendations.},
  keywords = {Biologically-inspired approaches and methods,Hierarchical Model Learning,Language Modeling,Machine Learning,Planning Keywords Semantic Relaxation,Single and multi-agent planning and schedul-ing},
  file = {/home/cameron/Zotero/storage/UE3VCQYX/Raiman, Yu, Williams - Unknown - Semantic Relaxation Interactive Plan Resolution Through Recommender Systems.pdf}
}

@article{Raissi2017,
  title = {Physics Informed Deep Learning ({{Part I}}): {{Data-driven}} Solutions of Nonlinear Partial Differential Equations},
  author = {Raissi, Maziar and Perdikaris, Paris and Karniadakis, George Em},
  year = {2017},
  journal = {arXiv},
  number = {Part I},
  eprint = {1711.10561v1},
  eprinttype = {arxiv},
  pages = {1--22},
  abstract = {We introduce physics informed neural networks - neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations. In this two part treatise, we present our developments in the context of solving two main classes of problems: data-driven solution and data-driven discovery of partial differential equations. Depending on the nature and arrangement of the available data, we devise two distinct classes of algorithms, namely continuous time and discrete time models. The resulting neural networks form a new class of data-efficient universal function approximators that naturally encode any underlying physical laws as prior information. In this first part, we demonstrate how these networks can be used to infer solutions to partial differential equations, and obtain physics-informed surrogate models that are fully differentiable with respect to all input coordinates and free parameters.},
  archiveprefix = {arXiv},
  keywords = {Data-driven scientific computing,Machine learning,Nonlinear dynamics,Predictive modeling,Runge-Kutta methods},
  file = {/home/cameron/Zotero/storage/HHNJFBMI/1711.10561.pdf}
}

@article{Rakthanmanon2013,
  title = {Addressing {{Big Data Time Series}}},
  author = {Rakthanmanon, Thanawin and Campana, Bilson and Mueen, Abdullah and Batista, Gustavo and Westover, Brandon and Zhu, Qiang and Zakaria, Jesin and Keogh, Eamonn},
  year = {2013},
  journal = {ACM Transactions on Knowledge Discovery from Data},
  volume = {7},
  number = {3},
  pages = {1--31},
  issn = {15564681},
  doi = {10.1145/2513092.2500489},
  file = {/home/cameron/Zotero/storage/QYHAIC7V/a10-rakthanmanon.pdf}
}

@article{Rapp2011,
  title = {A Ping-Pong Ball Catching and Juggling Robot: {{A}} Real-Time Framework for Vision Guided Acting of an Industrial Robot Arm},
  author = {Rapp, Holger H.},
  year = {2011},
  journal = {ICARA 2011 - Proceedings of the 5th International Conference on Automation, Robotics and Applications},
  pages = {430--435},
  publisher = {{IEEE}},
  doi = {10.1109/ICARA.2011.6144922},
  abstract = {This paper investigates the possibilities of real-time control of standard industrial robot arms by the means of a ping-pong ball juggling system that is able to catch a ball thrown by a human and to keep the ball airborne for more than 30 min. The sensors are two industrial cameras operating at 60 Hz. We discuss the image processing and the controlling algorithms used. Emphasis is placed on comparing the widespread naive linear model and a novel physically correct model of the flight trajectory of the ball. While both models are sufficient for the juggling task, it is shown that only the non-linear model is able to predict the impact parameters sufficiently early and precise enough to solve the initial catching task. The performance of the complete systems is also documented in a supplementary video [8]. \textcopyright{} 2011 IEEE.},
  isbn = {9781457703287},
  file = {/home/cameron/Zotero/storage/PD2B7MCD/Rapp - 2011 - A ping-pong ball catching and juggling robot A real-time framework for vision guided acting of an industrial robot arm.pdf}
}

@article{Reagan2012,
  title = {{{NASA}}'s {{Analog Missions}}: {{Driving Exploration Through Innovative Testing}}},
  author = {Reagan, Marcum L. and Janoiko, Barbara and Johnson, James and Chappell, Steven P. and Abercromby, Andrew F.J.},
  year = {2012},
  journal = {AIAA SPACE 2012 Conference \& Exposition},
  number = {September},
  pages = {1--18},
  doi = {10.2514/6.2012-5238},
  isbn = {978-1-60086-940-2},
  file = {/home/cameron/Zotero/storage/PF6J2NKP/nasa_analog_missions_2012.pdf}
}

@inproceedings{reagan2012,
  title = {{{NASA}}'s Analog Missions: {{Driving}} Exploration through Innovative Testing},
  booktitle = {{{AIAA SPACE Conference}} and {{Exposition}} 2012},
  author = {Reagan, Marcum L. and Janoiko, Barbara A. and Johnson, James E. and Chappell, Steven P. and Abercromby, Andrew F.},
  year = {2012},
  doi = {10.2514/6.2012-5238},
  abstract = {Human exploration beyond low-Earth orbit (LEO) will require a unique collection of advanced, innovative technologies and the precise execution of complex and challenging operational concepts. One tool we in the Analog Missions Project at the National Aeronautics and Space Administration (NASA) utilize to validate exploration system architecture concepts and conduct technology demonstrations, while gaining a deeper understanding of system-wide technical and operational challenges, is our analog missions. Analog missions are multi-disciplinary activities that test multiple features of future spaceflight missions in an integrated fashion to gain a deeper understanding of system-level interactions and integrated operations. These missions frequently occur in remote and extreme environments that are representative in one or more ways to that of future spaceflight destinations. They allow us to test robotics, vehicle prototypes, habitats, communications systems, in-situ resource utilization, and human performance as it relates to these technologies. And they allow us to validate architectural concepts, conduct technology demonstrations, and gain a deeper understanding of system-wide technical and operational challenges needed to support crewed missions beyond LEO. As NASA develops a capability driven architecture for transporting crew to a variety of space environments, including the moon, near-Earth asteroids (NEA), Mars, and other destinations, it will use its analog missions to gather requirements and develop the technologies that are necessary to ensure successful human exploration beyond LEO. Currently, there are four analog mission platforms: Research and Technology Studies (RATS), NASA's Extreme Environment Mission Operations (NEEMO), In-Situ Resource Utilization (ISRU), and International Space Station (ISS) Test bed for Analog Research (ISTAR).},
  isbn = {978-1-60086-940-2},
  file = {/home/cameron/Zotero/storage/V2DIM3RP/nasa_analog_missions_2012.pdf}
}

@article{Real2019,
  title = {Stochastic Modeling for Hysteretic Bit\textendash Rock Interaction of a Drill String under Torsional Vibrations},
  author = {Real, Fabio and Batou, Anas and Ritto, Thiago and Desceliers, Christophe},
  year = {2019},
  journal = {Journal of Vibration and Control},
  number = {X},
  eprint = {1706.09911v1},
  eprinttype = {arxiv},
  pages = {107754631982824},
  doi = {10.1177/ToBeAssigned},
  abstract = {We propose V2CNet, a new deep learning framework to automatically translate the demonstration videos to commands that can be directly used in robotic applications. Our V2CNet has two branches and aims at understanding the demonstration video in a fine-grained manner. The first branch has the encoder-decoder architecture to encode the visual features and sequentially generate the output words as a command, while the second branch uses a Temporal Convolutional Network (TCN) to learn the fine-grained actions. By jointly training both branches, the network is able to model the sequential information of the command, while effectively encodes the fine-grained actions. The experimental results on our new large-scale dataset show that V2CNet outperforms recent state-of-the-art methods by a substantial margin, while its output can be applied in real robotic applications. The source code and trained models will be made available.},
  archiveprefix = {arXiv},
  keywords = {[PHYS.MECA.VIBR]Physics [physics]/Mechanics [physi,bit-rock stochastic interaction model,drillstring dynamics,experimental identification,hysteretic friction},
  file = {/home/cameron/Zotero/storage/A3MS2JTP/Real et al. - 2019 - Stochastic modeling for hysteretic bitâ€“rock interaction of a drill string under torsional vibrations.pdf}
}

@article{Reeves2019,
  title = {Executing {{Multi-Goal Mission Plans}} for {{Coordinated Mobile Robots}}},
  author = {Reeves, Marlyse and {Fern{\'a}ndez-Gonz{\'a}lez}, Enrique and Williams, Brian},
  year = {2019},
  journal = {Icaps},
  abstract = {This paper presents a centralized executive for robotic mission plans with multiple temporally extended goals and coordinated agents. Common approaches to online motion planning and execution execute sequential goals independently, and do not consider the full plan when planning for the next goal. This often results in suboptimal or infeasible plans. Some hybrid and temporal planners are capable of generating motion plans for multiple vehicles with multiple goals over long horizons. However, the dynamics considered are often too simple to be executed by real systems. We present an executive planner that plans local trajectories using sufficiently accurate dynamics while considering the rest of the global plan constraints in the far future. We achieve this by repeatedly solving a single, multi-fidelity optimization problem, where we combine higher fidelity discrete-time dynamics to generate the local trajectory and lower fidelity continuous-time dynamics to capture the full remaining plan, guiding the local trajectory. We evaluate our multi-goal executive planner against a naive, myopic executive and demonstrate the scala-bility on a set of expressive real-world scenarios.},
  file = {/home/cameron/Zotero/storage/C3WLVMGV/Reeves, FernÃ¡ndez-GonzÃ¡lez, Williams - 2019 - Executing Multi-Goal Mission Plans for Coordinated Mobile Robots.pdf}
}

@article{Registries2020a,
  title = {Nine {{Best Practices}} for {{Research Software Registries}} and {{Repositories}}: {{A Concise Guide}}},
  author = {Registries, Task Force on Best Practices for Software and {:} and Monteil, Alain and {Gonzalez-Beltran}, Alejandra and Ioannidis, Alexandros and Allen, Alice and Lee, Allen and Bandrowski, Anita and Wilson, Bruce E. and Mecum, Bryce and Du, Cai Fan and Robinson, Carly and Garijo, Daniel and Katz, Daniel S. and Long, David and Milliken, Genevieve and M{\'e}nager, Herv{\'e} and Hausman, Jessica and Spaaks, Jurriaan H. and Fenlon, Katrina and Vanderbilt, Kristin and Hwang, Lorraine and Davis, Lynn and Fenner, Martin and Crusoe, Michael R. and Hucka, Michael and Wu, Mingfang and Hong, Neil Chue and Teuben, Peter and Stall, Shelley and Druskat, Stephan and Carnevale, Ted and Morrell, Thomas},
  year = {2020},
  number = {December},
  eprint = {2012.13117},
  eprinttype = {arxiv},
  pages = {1--18},
  abstract = {Scientific software registries and repositories serve various roles in their respective disciplines. These resources improve software discoverability and research transparency, provide information for software citations, and foster preservation of computational methods that might otherwise be lost over time, thereby supporting research reproducibility and replicability. However, developing these resources takes effort, and few guidelines are available to help prospective creators of registries and repositories. To address this need, we present a set of nine best practices that can help managers define the scope, practices, and rules that govern individual registries and repositories. These best practices were distilled from the experiences of the creators of existing resources, convened by a Task Force of the FORCE11 Software Citation Implementation Working Group during the years 2019-2020. We believe that putting in place specific policies such as those presented here will help scientific software registries and repositories better serve their users and their disciplines.},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/NGW5YXYC/2012.13117v1.pdf}
}

@inproceedings{Rintanen2010,
  title = {Heuristics for Planning with {{SAT}}},
  booktitle = {Lecture {{Notes}} in {{Computer Science}} (Including Subseries {{Lecture Notes}} in {{Artificial Intelligence}} and {{Lecture Notes}} in {{Bioinformatics}})},
  author = {Rintanen, Jussi},
  year = {2010},
  volume = {6308 LNCS},
  pages = {414--428},
  address = {{St. Andrews, Scotland, UK}},
  issn = {16113349},
  doi = {10.1007/978-3-642-15396-9_34},
  abstract = {Generic SAT solvers have been very successful in solving hard combinatorial problems in various application areas, including AI planning. There is potential for improved performance by making the SAT solving process more application-specific. In this paper we propose a variable selection strategy for AI planning. The strategy is based on generic principles about properties of plans, and its performance with standard planning benchmarks often substantially improves on generic variable selection heuristics used in SAT solving, such as the VSIDS strategy. These improvements lift the efficiency of SAT based planning to the same level as best planners that use other search methods. \textcopyright{} 2010 Springer-Verlag.},
  isbn = {3-642-15395-X},
  file = {/home/cameron/Zotero/storage/3RPPUVYP/Rintanen - 2010 - Heuristics for planning with SAT.pdf}
}

@article{Rintanen2011,
  title = {Planning with Specialized {{SAT}} Solvers},
  author = {Rintanen, Jussi},
  year = {2011},
  journal = {Proceedings of the National Conference on Artificial Intelligence},
  volume = {2},
  pages = {1563--1566},
  abstract = {Logic, and declarative representation of knowledge in general, have long been a preferred framework for problem solving in AI. However, specific subareas of AI have been eager to abandon general-purpose knowledge representation in favor of methods that seem to address their computational core problems better. In planning, for example, state-space search has in the last several years been preferred to logic-based methods such as SAT. In our recent work, we have demonstrated that the observed performance differences between SAT and specialized state-space search methods largely go back to the difference between a blind (or at least planning-agnostic) and a planning-specific search method. If SAT search methods are given even simple heuristics which make the search goal-directed, the efficiency differences disappear.},
  isbn = {9781577355090},
  file = {/home/cameron/Zotero/storage/MZPZCZKX/Rintanen - 2011 - Planning with specialized SAT solvers.pdf}
}

@article{Rintanen2012,
  title = {Planning as Satisfiability: {{Heuristics}}},
  author = {Rintanen, Jussi},
  year = {2012},
  journal = {Artificial Intelligence},
  volume = {193},
  pages = {45--86},
  issn = {00043702},
  doi = {10.1016/j.artint.2012.08.001},
  abstract = {Reduction to SAT is a very successful approach to solving hard combinatorial problems in Artificial Intelligence and computer science in general. Most commonly, problem instances reduced to SAT are solved with a general-purpose SAT solver. Although there is the obvious possibility of improving the SAT solving process with application-specific heuristics, this has rarely been done successfully. In this work we propose a planning-specific variable selection strategy for SAT solving. The strategy is based on generic principles about properties of plans, and its performance with standard planning benchmarks often substantially improves on generic variable selection heuristics, such as VSIDS, and often lifts it to the same level with other search methods such as explicit state-space search with heuristic search algorithms. \textcopyright{} 2012 Elsevier B.V.},
  keywords = {Heuristics,Planning,SAT},
  file = {/home/cameron/Zotero/storage/Y8FR324R/Rintanen - 2012 - Planning as satisfiability Heuristics.pdf}
}

@techreport{rizk,
  title = {{{OCAMS}}: {{The OSIRIS-REx Camera Suite}}},
  author = {Rizk, B and Drouet D'aubigny, C and Golish, D and Fellows, C and Merrill, C and Smith, P and Walker, M S and Hendershot, J E and Hancock, J and Bailey, S H and Dellagiustina, D and Lauretta, D and Tanner, R and Williams, M and Harshman, K and Fitzgibbon, M and Verts, W and Chen, J and Connors, T and Hamara, D and Dowd, A and Lowman, A and Dubin, M and Burt, R and Whiteley, M and Watson, M and Mcmahon, T and Ward, M and Booher, D and Read, M and Williams, Brian C. and Hunten, M and Little, E and Saltzman, T and Alfred, D and O'dougherty, S and Walthall, M and Kenagy, K and Peterson, S and Crowther, B and Perry, M L and See, C and Selznick, S and Sauve, C and Beiser, M and Black, W and Pfisterer, R N and Lancaster, A and Oliver, S and Oquest, C and Crowley, D and Morgan, C and Castle, C and Dominguez, R and Sullivan, M},
  abstract = {The requirements-driven OSIRIS-REx Camera Suite (OCAMS) acquires images essential to collecting a sample from the surface of Bennu. During proximity operations, these images document the presence of satellites and plumes, record spin state, enable an accurate digital terrain model of the asteroid's shape, and identify any surface hazards. They confirm the presence of sampleable regolith on the surface, observe the sampling event itself, and image the sample head in order to verify its readiness to be stowed. They document Bennu's history as an example of early solar system material, as a microgravity body with a planetesimal size-scale, and as a carbonaceous object. OCAMS is fitted with three cameras. The MapCam records point-source color images on approach to the asteroid in order to connect Bennu's ground-based point-source observational record to later higher-resolution surface spectral imaging. The SamCam documents the sample site before, during, and after it is disturbed by the sample mechanism. The PolyCam, using its focus mechanism, observes the sample site at sub-centimeter},
  file = {/home/cameron/Zotero/storage/5AZFY69W/1704.04531.pdf}
}

@techreport{RMPL2002,
  title = {The {{Reactive Model-based Programming Language}}},
  author = {Ingham, Mitch and Ragno, Robert and Wehowsky, Andreas and Williams, Brian},
  year = {2002},
  institution = {{MIT Space Systems and Artificial Intelligence Laboratories}},
  file = {/home/cameron/Zotero/storage/DELQLTMW/Ingham et al. - Unknown - The Reactive Model-based Programming Language.pdf}
}

@article{Rodriguez2022,
  title = {{{LyaNet}}: {{A Lyapunov Framework}} for {{Training Neural ODEs}}},
  author = {Rodriguez, Ivan Dario Jimenez and Ames, Aaron D. and Yue, Yisong},
  year = {2022},
  number = {1},
  eprint = {2202.02526},
  eprinttype = {arxiv},
  abstract = {We propose a method for training ordinary differential equations by using a control-theoretic Lyapunov condition for stability. Our approach, called LyaNet, is based on a novel Lyapunov loss formulation that encourages the inference dynamics to converge quickly to the correct prediction. Theoretically, we show that minimizing Lyapunov loss guarantees exponential convergence to the correct solution and enables a novel robustness guarantee. We also provide practical algorithms, including one that avoids the cost of backpropagating through a solver or using the adjoint method. Relative to standard Neural ODE training, we empirically find that LyaNet can offer improved prediction performance, faster convergence of inference dynamics, and improved adversarial robustness. Our code available at https://github.com/ivandariojr/LyapunovLearning .},
  archiveprefix = {arXiv},
  file = {/home/cameron/Zotero/storage/DU7RKHNX/Rodriguez, Ames, Yue - 2022 - LyaNet A Lyapunov Framework for Training Neural ODEs.pdf}
}

@article{ruppel2018,
  title = {P m a a c r i c u},
  author = {Ruppel, By Halley and Funk, Marjorie and Whittemore, Robin},
  year = {2018},
  volume = {27},
  number = {1},
  pages = {22--23},
  file = {/home/cameron/Zotero/storage/MX6SMJDD/alarm-fatigue-crit-care.pdf}
}

@article{ruppel2018a,
  title = {Measurement of Physiological Monitor Alarm Accuracy and Clinical Relevance in Intensive Care Units},
  author = {Ruppel, Halley and Funk, Marjorie and Whittemore, Robin},
  year = {2018},
  month = jan,
  journal = {American Journal of Critical Care},
  volume = {27},
  number = {1},
  pages = {11--21},
  publisher = {{American Association of Critical Care Nurses}},
  issn = {10623264},
  doi = {10.4037/ajcc2018385},
  abstract = {Background Alarm fatigue threatens patient safety by delaying or reducing clinician response to alarms, which can lead to missed critical events. Interventions to reduce alarms without jeopardizing patient safety target either inaccurate or clinically irrelevant alarms, so assessment of alarm accuracy and clinical relevance may enhance the rigor of alarm intervention studies done in clinical units. Objectives To (1) examine approaches used to measure accuracy and/or clinical relevance of physiological monitor alarms in intensive care units and (2) compare the proportions of inaccurate and clinically irrelevant alarms. Methods An integrative review was used to systematically search the literature and synthesize resulting articles. Results Twelve studies explicitly measuring alarm accuracy and/or clinical relevance on a clinical unit were identified. In the most rigorous studies, alarms were annotated retrospectively by obtaining alarm data and parameter waveforms rather than being annotated in real time. More than half of arrhythmia alarms in recent studies were inaccurate. However, contextual data were needed to determine alarms' clinical relevance. Proportions of clinically irrelevant alarms were high, but definitions of clinically irrelevant alarms often included inaccurate alarms. Conclusions Future studies testing interventions on clinical units should include alarm accuracy and/or clinical relevance as outcome measures. Arrhythmia alarm accuracy should improve with advances in technology. Clinical interventions should focus on reducing clinically irrelevant alarms, with careful consideration of how clinical relevance is defined and measured.},
  pmid = {29292271},
  file = {/home/cameron/Zotero/storage/YF4E2ZWZ/alarm-fatigue-crit-care.pdf}
}

@book{Russell2003,
  title = {Artificial {{Intelligence A Modern Approach}}},
  author = {Russell, Stuart J and Norvig, Peter},
  year = {2003},
  journal = {Pearson},
  eprint = {gr-qc/9809069v1},
  eprinttype = {arxiv},
  issn = {0269-8889},
  doi = {10.1017/S0269888900007724},
  abstract = {The long-anticipated revision of this \#1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed},
  archiveprefix = {arXiv},
  isbn = {978-0-13-604259-4},
  pmid = {20949757},
  file = {/home/cameron/Zotero/storage/UJ4GXNKX/document.pdf}
}

@article{Ruthotto2020,
  title = {Deep {{Neural Networks Motivated}} by {{Partial Differential Equations}}},
  author = {Ruthotto, Lars and Haber, Eldad},
  year = {2020},
  journal = {Journal of Mathematical Imaging and Vision},
  volume = {62},
  number = {3},
  eprint = {1804.04272},
  eprinttype = {arxiv},
  pages = {352--364},
  issn = {15737683},
  doi = {10.1007/s10851-019-00903-1},
  abstract = {Partial differential equations (PDEs) are indispensable for modeling many physical phenomena and also commonly used for solving image processing tasks. In the latter area, PDE-based approaches interpret image data as discretizations of multivariate functions and the output of image processing algorithms as solutions to certain PDEs. Posing image processing problems in the infinite-dimensional setting provides powerful tools for their analysis and solution. For the last few decades, the reinterpretation of classical image processing problems through the PDE lens has been creating multiple celebrated approaches that benefit a vast area of tasks including image segmentation, denoising, registration, and reconstruction. In this paper, we establish a new PDE interpretation of a class of deep convolutional neural networks (CNN) that are commonly used to learn from speech, image, and video data. Our interpretation includes convolution residual neural networks (ResNet), which are among the most promising approaches for tasks such as image classification having improved the state-of-the-art performance in prestigious benchmark challenges. Despite their recent successes, deep ResNets still face some critical challenges associated with their design, immense computational costs and memory requirements, and lack of understanding of their reasoning. Guided by well-established PDE theory, we derive three new ResNet architectures that fall into two new classes: parabolic and hyperbolic CNNs. We demonstrate how PDE theory can provide new insights and algorithms for deep learning and demonstrate the competitiveness of three new CNN architectures using numerical experiments.},
  archiveprefix = {arXiv},
  keywords = {Deep neural networks,Image classification,Machine learning,Partial differential equations,PDE-constrained optimization},
  file = {/home/cameron/Zotero/storage/46Y69IF3/Ruthotto, Haber - 2020 - Deep Neural Networks Motivated by Partial Differential Equations.pdf}
}

@article{sanders2015,
  title = {Final Review of Analog Field Campaigns for in {{Situ Resource Utilization}} Technology and Capability Maturation},
  author = {Sanders, Gerald B. and Larson, William E.},
  year = {2015},
  month = may,
  journal = {Advances in Space Research},
  volume = {55},
  number = {10},
  pages = {2381--2404},
  publisher = {{Elsevier Ltd}},
  issn = {18791948},
  doi = {10.1016/j.asr.2014.12.024},
  abstract = {A key aspect of enabling an affordable and sustainable program of human exploration beyond low Earth orbit is the ability to locate, extract, and harness the resources found in space to reduce what needs to be launched from Earth's deep gravity well and to minimize the risk of dependence on Earth for survival. Known as In Situ Resource Utilization or ISRU, the ability to convert space resources into useful and mission critical products has been shown in numerous studies to be mission and architecture enhancing or enabling. However at the time of the release of the US Vision for Space Exploration in 2004, only concept feasibility hardware for ISRU technologies and capabilities had been built and tested in the laboratory; no ISRU hardware had ever flown in a mission to the Moon or Mars. As a result, an ISRU development project was established with phased development of multiple generations of hardware and systems. To bridge the gap between past ISRU feasibility hardware and future hardware needed for space missions, and to increase confidence in mission and architecture planners that ISRU capabilities would meet exploration needs, the ISRU development project incorporated extensive ground and analog site testing to mature hardware, operations, and interconnectivity with other exploration systems linked to ISRU products. This report documents the series of analog test activities performed from 2008 to 2012, the stepwise progress achieved, and the end-to-end system and mission demonstrations accomplished in this test program.},
  keywords = {Analog testing,Field testing,In Situ Resource Utilization,Lunar human exploration}
}

@inproceedings{Santana2016,
  title = {{{RAO}} {${_\ast}$} : {{An Algorithm}} for {{Chance-Constrained POMDP}}'s},
  booktitle = {Association for the {{Advancement}} of {{Artificial Intelligence}}},
  author = {Santana, Pedro and Thi, Sylvie and Williams, Brian C.},
  year = {2016},
  pages = {3308--3314},
  keywords = {Technical Papers: Reasoning under Uncertainty},
  file = {/home/cameron/Zotero/storage/3AP8YHK6/12321-56288-1-PB.pdf}
}

@phdthesis{Santana2016a,
  title = {Dynamic {{Execution}} of {{Temporal Plans}} with {{Sensing Actions}} and {{Bounded Risk}}},
  author = {Santanta, Pedro},
  year = {2016},
  abstract = {A special report on the cover of the June 2016 issue of the IEEE Spectrum maga- zine reads: "can we trust robots?" In a world that has been experiencing a seem- ingly irreversible process by which autonomous systems have been given increasingly more space in strategic areas such as transportation, manufacturing, energy supply, planetary exploration, and even medical surgeries, it is natural that we start asking ourselves if these systems could be held at the same or even higher levels of safety than we expect from humans. In an effort to make a contribution towards a world of autonomy that we can trust, this thesis argues that one necessary step in this direc- tion is the endowment of autonomous agents with the ability to dynamically adapt to their environment while meeting strict safety guarantees. From a technical standpoint, we propose that autonomous agents in safety-critical applications be able to execute conditional plans (or policies) within risk bounds (also referred to as chance constraints). By being conditional, the plan allows the autonomous agent to adapt to its environment in real-time by conditioning the choice of activity to be executed on the agent's current level of knowledge, or belief, about the true state of world. This belief state is, in turn, a function of the history of potentially noisy sensor observations gathered by the agent from the environment. With respect to bounded risk, it refers to the fact that executing such conditional plans should guarantee to keep the agent "safe" - as defined by sets of state constraints - with high probability, while moving away from the conservatism of minimum risk approaches. In this thesis, we propose Chance-Constrained Partially Observable Markov De- cision Processes (CC-POMDP's) as a formalism for conditional risk-bounded plan- ning under uncertainty. Moreover, we present Risk-bounded AO* (RAO*), a heuris- tic forward search-based algorithm that searches for solutions to a CC-POMDP by leveraging admissible utility and risk heuristics to simultaneously guide the search and perform early pruning of overly-risky policy branches. In an effort to facilitate the specification of risk-bounded behavior by human modelers, we also present the Chance-constrained Reactive Model-based Programming Language (cRMPL), a novel variant of RMPL that incorporates chance constraints as part of its syntax. Finally, in support of the temporal planning applications with duration uncertainty that this thesis is concerned about, we present the Polynomial-time Algorithm for Risk-aware Scheduling (PARIS) and its extension to conditional scheduling of Probabilistic Tem- poral Plan Networks (PTPN's). The different tools and algorithms developed in the context of this thesis are com- bined to form the Conditional Planning for Autonomy with Risk (CLARK) system, a risk-aware conditional planning system that can generate chance-constrained, dy- namic temporal plans for autonomous agents that must operate under uncertainty. With respect to our empirical validation, each component of CLARK is benchmarked against the relevant state of the art throughout the chapters, followed by several demonstrations of the whole CLARK system working in tandem with other building blocks of an architecture for autonomy.},
  school = {Massachusetts Institute of Technology},
  file = {/home/cameron/Zotero/storage/NZT56NRR/Santanta - 2016 - Dynamic Execution of Temporal Plans with Sensing Actions and Bounded Risk.pdf}
}

@article{Santana2016b,
  title = {{{RAO}}: {{An}} Algorithm for Chance-Constrained {{POMDP}}'s},
  author = {Santana, Pedro and Th{\'i}ebaux, Sylvie and Williams, Brian},
  year = {2016},
  journal = {30th AAAI Conference on Artificial Intelligence, AAAI 2016},
  number = {Altman 1999},
  pages = {3308--3314},
  abstract = {Autonomous agents operating in partially observable stochastic environments often face the problem of optimizing expected performance while bounding the risk of violating safety constraints. Such problems can be modeled as chance-constrained POMDP's (CCPOMDP's). Our first contribution is a systematic derivation of execution risk in POMDP domains, which improves upon how chance constraints are handled in the constrained POMDP literature. Second, we present RAO, a heuristic forward search algorithm producing optimal, deterministic, finite-horizon policies for CCPOMDP's. In addition to the utility heuristic, RAO leverages an admissible execution risk heuristic to quickly detect and prune overly-risky policy branches. Third, we demonstrate the usefulness of RAO in two challenging domains of practical interest: power supply restoration and autonomous science agents.},
  isbn = {9781577357605},
  file = {/home/cameron/Zotero/storage/4W76PUXL/Santana, ThÃ­ebaux, Williams - 2016 - RAO An algorithm for chance-constrained POMDP's.pdf}
}

@article{Scalzo2013,
  title = {Reducing False Intracranial Pressure Alarms Using Morphological Waveform Features},
  author = {Scalzo, Fabien and Liebeskind, David and Hu, Xiao},
  year = {2013},
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {60},
  number = {1},
  pages = {235--239},
  issn = {00189294},
  doi = {10.1109/TBME.2012.2210042},
  abstract = {False alarms produced by patient monitoring sys- tems in intensive care units are a major issue that causes alarm fatigue, waste of human resources, and increased patient risks. While alarms are typically triggered by manually adjusted thresh- olds, the trend and patterns observed prior to threshold crossing are generally not used by current systems. This study introduces and evaluates, a smart alarm detection system for intracranial pressure signal (ICP) that is based on advanced pattern recogni- tion methods. Models are trained in a supervised fashion from a comprehensive dataset of 4791 manually labeled alarm episodes ex- tracted from 108 neurosurgical patients. The comparative analysis provided between spectral regression, kernel spectral regression, and support vector machines indicates the significant improvement of the proposed framework in detecting false ICP alarms in com- parison to a threshold-based technique that is conventionally used. Another contribution of this work is to exploit an adaptive dis- cretization to reduce the dimensionality of the input features. The resulting features lead to a decrease of 30\% of false ICP alarms without compromising sensitivity.},
  arxiv = {NIHMS150003},
  isbn = {1558-2531 (Electronic)\textbackslash r0018-9294 (Linking)},
  pmid = {22851230},
  keywords = {Brain injuries,false alarm,intensive care unit (ICU),intracranial pressure signal (ICP),patient monitoring,smart alarm,supervised learning},
  file = {/home/cameron/Zotero/storage/RVIVLEIR/alarm-fatigue-intracranial.pdf}
}

@article{scalzo2013,
  title = {Reducing False Intracranial Pressure Alarms Using Morphological Waveform Features},
  author = {Scalzo, Fabien and Liebeskind, David and Hu, Xiao},
  year = {2013},
  journal = {IEEE Transactions on Biomedical Engineering},
  volume = {60},
  number = {1},
  pages = {235--239},
  issn = {00189294},
  doi = {10.1109/TBME.2012.2210042},
  abstract = {False alarms produced by patient monitoring systems in intensive care units are a major issue that causes alarm fatigue, waste of human resources, and increased patient risks. While alarms are typically triggered by manually adjusted thresholds, the trend and patterns observed prior to threshold crossing are generally not used by current systems. This study introduces and evaluates, a smart alarm detection system for intracranial pressure signal (ICP) that is based on advanced pattern recognition methods. Models are trained in a supervised fashion from a comprehensive dataset of 4791 manually labeled alarm episodes extracted from 108 neurosurgical patients. The comparative analysis provided between spectral regression, kernel spectral regression, and support vector machines indicates the significant improvement of the proposed framework in detecting false ICP alarms in comparison to a threshold-based technique that is conventionally used. Another contribution of this work is to exploit an adaptive discretization to reduce the dimensionality of the input features. The resulting features lead to a decrease of 30\% of false ICP alarms without compromising sensitivity. \textcopyright{} 1964-2012 IEEE.},
  pmid = {22851230},
  keywords = {Brain injuries,false alarm,intensive care unit (ICU),intracranial pressure signal (ICP),patient monitoring,smart alarm,supervised learning},
  file = {/home/cameron/Zotero/storage/3HY2NI58/alarm-fatigue-intracranial.pdf}
}

@techreport{schaal,
  title = {Scalable {{Techniques}} from {{Nonparametric Statistics}} for {{Real Time Robot Learning Sethu Vijayakumar}} \textdaggerdbl{} {$\oplus$} {$\oplus$} {$\oplus$} {$\oplus$}},
  author = {Schaal, Stefan and Atkeson, Christopher G},
  abstract = {Locally weighted learning (LWL) is a class of techniques from nonparametric statistics that provides useful representations and training algorithms for learning about complex phenomena during autonomous adaptive control of robotic systems. This paper introduces several LWL algorithms that have been tested successfully in real-time learning of complex robot tasks. We discuss two major classes of LWL, memory-based LWL and purely incremental LWL that does not need to remember any data explicitly. In contrast to the traditional belief that LWL methods cannot work well in high-dimensional spaces, we provide new algorithms that have been tested on up to 90 dimensional learning problems. The applicability of our LWL algorithms is demonstrated in various robot learning examples, including the learning of devil-sticking, pole-balancing by a humanoid robot arm, and inverse-dynamics learning for a seven and a 30 degree-of-freedom robot. In all these examples, the application of our statistical neural networks techniques allowed either faster or more accurate acquisition of motor control than classical control engineering.},
  keywords = {Incremental Learning,Internal Models,Locally Weighted Learning,Motor Control,Nonparametric Regression},
  file = {/home/cameron/Zotero/storage/GICY3CHS/LWPRreference.pdf}
}

@article{Schaal2002,
  title = {Scalable Techniques from Nonparametric Statistics for Real Time Robot Learning},
  author = {Schaal, Stefan and Atkeson, Christopher G. and Vijayakumar, Sethu},
  year = {2002},
  journal = {Applied Intelligence},
  volume = {17},
  number = {1},
  pages = {49--60},
  issn = {0924669X},
  doi = {10.1023/A:1015727715131},
  abstract = {Locally weighted learning (LWL) is a class of techniques from nonparametric statis- tics that provides useful representations and training algorithms for learning about complex phenomena during autonomous adaptive control of robotic systems. This paper introduces sev- eral LWL algorithms that have been tested successfully in real-time learning of complex robot tasks. We discuss two major classes of LWL, memory-based LWL and purely incremental LWL that does not need to remember any data explicitly. In contrast to the traditional belief that LWL methods cannot work well in high-dimensional spaces, we provide new algorithms that have been tested on up to 90 dimensional learning problems. The applicability of our LWL algorithms is demonstrated in various robot learning examples, including the learning of devil-sticking, pole- balancing by a humanoid robot arm, and inverse-dynamics learning for a seven and a 30 degree- of-freedom robot. In all these examples, the application of our statistical neural networks tech- niques allowed either faster or more accurate acquisition of motor control than classical control engineering.},
  keywords = {Incremental learning,Internal models,Locally weighted learning,Motor control,Nonparametric regression},
  file = {/home/cameron/Zotero/storage/UGP8NG4F/LWPRreference.pdf}
}

@article{Schetter2000,
  title = {Multiple Agent-Based Autonomy for Satellite Constellations},
  author = {Schetter, Thomas and Campbell, Mark and Surka, Derek},
  year = {2000},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {1882},
  pages = {151--165},
  issn = {16113349},
  doi = {10.1016/S0004-3702(02)00382-X},
  abstract = {There is an increasing desire to use constellations of autonomous spacecraft working together to accomplish complex mission objectives. Multiple, highly autonomous, satellite systems are envisioned because they are capable of higher performance, lower cost, better fault tolerance, reconfigurability and upgradability. This paper presents an architecture and multi-agent design and simulation environment that will enable agent-based multi-satellite systems to fulfill their complex mission objectives, termed TeamAgent \texttrademark. Its application is shown for Tech- Sat21, a U.S. Air Force mission designed to explore the benefits of distributed satellite systems. Required spacecraft functions, software agents, and multi-agent organisations are described for the TechSat21 mission, as well as their implementation. Agent-based simulations of TechSat21 case studies show the autonomous operation and how TeamAgent can be used to evaluate and compare multi agent-based organisations.},
  isbn = {354041052X},
  keywords = {multi-agent systems,multiple satellite autonomy,software agents,spacecraft autonomy},
  file = {/home/cameron/Zotero/storage/G68W52RC/1-s2.0-S000437020200382X-main.pdf}
}

@techreport{schmid2007,
  title = {The {{Apollo Medical Operations Project}}: {{Recommendations}} to {{Improve Crew Health}} and {{Performance}} for {{Future Exploration Missions}} and {{Lunar Surface Operations}}},
  author = {Schmid, Josef and James Duncan, Nasa M and Jeffrey Davis, Nasa R and Novak, Joseph D},
  year = {2007},
  file = {/home/cameron/Zotero/storage/7E5B33LW/medical_distributed_planning_systems_lessons_learned.pdf}
}

@article{Schouwenaars2001c,
  title = {Mixed Integer Programming for Multi-Vehicle Path Planning},
  author = {Schouwenaars, Tom and De Moor, Bart and Feron, Eric and How, Jonathan},
  year = {2001},
  journal = {2001 European Control Conference, ECC 2001},
  number = {March 2014},
  pages = {2603--2608},
  publisher = {{IEEE}},
  doi = {10.23919/ecc.2001.7076321},
  abstract = {This paper presents a new approach to fuel-optimal path planning of multiple vehicles using a combination of linear and integer programming. The basic problem formulation is to have the vehicles move from an initial dynamic state to a final state without colliding with each other, while at the same time avoiding other stationary and moving obstacles. It is shown that this problem can be rewritten as a linear program with mixed integer/linear constraints that account for the collision avoidance. A key benefit of this approach is that the path optimization can be readily solved using the CPLEX optimization software with an AMPL/Matlab interface. An example is worked out to show that the framework of mixed integer/linear programming is well suited for path planning and collision avoidance problems. Implementation issues are also considered. In particular, we compare receding horizon strategies with fixed arrival time approaches.},
  isbn = {9783952417362},
  keywords = {autonomous vehicles,collision avoidance,path planning},
  file = {/home/cameron/Zotero/storage/53ECH8J2/Schouwenaars et al. - 2001 - Mixed integer programming for multi-vehicle path planning(2).pdf;/home/cameron/Zotero/storage/XNK3E6EC/Schouwenaars et al. - 2001 - Mixed integer programming for multi-vehicle path planning.pdf}
}

@article{Schulte2012,
  title = {A Multi-Language Computing Environment for Literate Programming and Reproducible Research},
  author = {Schulte, Eric and Davison, Dan and Dye, Thomas and Dominik, Carsten},
  year = {2012},
  journal = {Journal of Statistical Software},
  volume = {46},
  number = {3},
  issn = {15487660},
  doi = {10.18637/jss.v046.i03},
  abstract = {We present a new computing environment for authoring mixed natural and computer language documents. In this environment a single hierarchically-organized plain text source file may contain a variety of elements such as code in arbitrary programming languages, raw data, links to external resources, project management data, working notes, and text for publication. Code fragments may be executed in situ with graphical, numerical and textual output captured or linked in the file. Export to LATEX, HTML, LATEX beamer, DocBook and other formats permits working reports, presentations and manuscripts for publication to be generated from the file. In addition, functioning pure code files can be automatically extracted from the file. This environment is implemented as an extension to the Emacs text editor and provides a rich set of features for authoring both prose and code, as well as sophisticated project management capabilities.},
  keywords = {Compendium,Emacs,Literate programming,Reproducible research,WEB},
  file = {/home/cameron/Zotero/storage/AG5RCJ66/Schulte et al. - 2012 - A multi-language computing environment for literate programming and reproducible research.pdf}
}

@article{Scientia2018,
  title = {Working in {{Space}} : {{The Challenge}} for {{Mars}} and {{Beyond WORKING IN SPACE}} : {{THE CHALLENGE FOR}}},
  author = {Feigh, Karen M. and Miller, Matthew J. and Pittman, Cameron W.},
  year = {2018},
  journal = {Scientia},
  pages = {5},
  abstract = {Professor Karen Feigh and Dr Matthew Miller from the Georgia Institute of Technology examine what support will be required when astronauts need to work outside in deep space, where communication with Earth takes tens of minutes. Software engineer, Cameron Pittman, also joined the team to help develop functional prototypes so they can be tested in the lab and beyond.},
  file = {/home/cameron/Zotero/storage/WT5CZ29N/Feigh, Miller, Pittman - 2018 - Working in Space The Challenge for Mars and Beyond WORKING IN SPACE THE CHALLENGE FOR.pdf}
}

@article{sehlke2019,
  title = {Requirements for {{Portable Instrument Suites}} during {{Human Scientific Exploration}} of {{Mars}}},
  author = {Sehlke, Alexander and Mirmalek, Zara and Burtt, David and Haberle, Christopher W and {Santiago-materese}, Delia and Nawotniak, Shannon E Kobs and Hughes, Scott S. and Garry, W. Brent and Bramall, Nathan and Brown, Adrian J and Heldmann, Jennifer L and Lim, Darlene S.S.},
  year = {2019},
  volume = {19},
  number = {3},
  pages = {401--425},
  doi = {10.1089/ast.2018.1841},
  file = {/home/cameron/Zotero/storage/UHD67D5P/ast.2018.1841.pdf}
}

@article{Seibert2019b,
  title = {Developing {{Future Deep-Space Telecommunication Architectures}}: {{A Historical Look}} at the {{Benefits}} of {{Analog Research}} on the {{Development}} of {{Solar System Internetworking}} for {{Future Human Spaceflight}}},
  author = {Seibert, Marc A and Lim, Darlene S.S. and Miller, Matthew J. and {Santiago-Materese}, Delia and Downs, Michael T},
  year = {2019},
  month = mar,
  journal = {Astrobiology},
  volume = {19},
  number = {3},
  pages = {462--477},
  issn = {1531-1074},
  doi = {10.1089/ast.2018.1915},
  file = {/home/cameron/Zotero/storage/PZR68EVN/Seibert et al. - 2019 - Developing Future Deep-Space Telecommunication Architectures A Historical Look at the Benefits of Analog Resear.pdf}
}

@inproceedings{Selman1992,
  title = {A {{New Method}} for {{Solving Hard Satisfiability Problems}}},
  booktitle = {{{AAAI}}},
  author = {Selman, Bart and Levesque, Hector and Mitchell, David},
  year = {1992},
  pages = {440--446},
  abstract = {We introduce a greedy local search procedure called GSAT for solving propositional satisfiability problems. Our experiments show that this procedure can be used to solve hard, randomly generated problems that are an order of magnitude larger than those that can be handled by more traditional approaches such as the Davis-Putnam procedure or resolution. We also show that GSAT can solve structured satisfiability problems quickly. In particular, we solve encodings of graph coloring problems, N-queens, and Boolean induction. General application strategies and limitations of the ap- proach are also discussed. GSAT is best viewed as a model-finding procedure. Its good performance suggests that it may be advan- tageous to reformulate reasoning tasks that have tra- ditionally been viewed as theorem-proving problems as model-finding tasks.},
  file = {/home/cameron/Zotero/storage/VVPFYJXZ/Selman, Levesque, Mitchell - 1992 - A New Method for Solving Hard Satisfiability Problems.pdf}
}

@article{Selman1994,
  title = {Noise Strategies for Local Search},
  author = {Selman, B and Kautz, H and Cohen, B},
  year = {1994},
  journal = {AAAI/IAAI Proceedings},
  number = {1990},
  pages = {337--343},
  file = {/home/cameron/Zotero/storage/JAA9ULFG/Selman, Kautz, Cohen - 1994 - Noise strategies for local search.pdf}
}

@article{Shah2007,
  title = {A Fast Incremental Algorithm for Maintaining Dispatchability of Partially Controllable Plans},
  author = {Shah, Julie and Stedl, John and Williams, Brian and Robertson, Paul},
  year = {2007},
  journal = {ICAPS 2007, 17th International Conference on Automated Planning and Scheduling},
  pages = {296--303},
  abstract = {Autonomous systems operating in real-world environments must be able to plan, schedule, and execute missions while robustly adapting to uncertainty and disturbances. Previous work on dispatchable execution increases the efficiency of plan execution under uncertainty by introducing a temporal plan dispatcher that schedules events dynamically in response to disturbances, and a compiler that reduces a plan to a dispatchable form that enables real-time scheduling. However, this work does not address the situation where response requires modifying the plan in real-time. In these situations, after the autonomous system replans, compilation to dispatchable form must occur in near real-time. The key contribution of this paper is a fast Incremental Dynamic Control algorithm (IDC) for maintaining the dispatchability of a partially controllable plan, in response to incremental plan modifications by an online planner. IDC is developed as a set of incremental update rules that exploit the structure of the plan in order to efficiently propagate the effects of constraint loosening and tightening throughout the plan. IDC exhibits an order of magnitude improvement in compile time over the state of the art nonincremental algorithm applied to randomly generated problems. Its practicality is demonstrated on plans for coordinating rovers within the authors' hardware test-bed.},
  isbn = {9781577353447},
  file = {/home/cameron/Zotero/storage/PK9T5EH2/406bb6a7275de2485b1bf16453ea29c54eca.pdf}
}

@phdthesis{shah2011,
  title = {Fluid {{Coordination}} of {{Human-Robot Teams}}},
  author = {Shah, Julie A},
  year = {2011},
  school = {Massachusetts Institute of Technology},
  file = {/home/cameron/Zotero/storage/IIHNJJWE/4427590.pdf}
}

@article{Shah2018,
  title = {Bayesian Inference of Temporal Task Specifications from Demonstrations},
  author = {Shah, Ankit and Kamath, Pritish and Li, Shen and Shah, Julie},
  year = {2018},
  journal = {Advances in Neural Information Processing Systems},
  volume = {2018-Decem},
  number = {December 2018},
  pages = {3804--3813},
  issn = {10495258},
  abstract = {When observing task demonstrations, human apprentices are able to identify whether a given task is executed correctly long before they gain expertise in actually performing that task. Prior research into learning from demonstrations (LfD) has failed to capture this notion of the acceptability of an execution; meanwhile, temporal logics provide a flexible language for expressing task specifications. Inspired by this, we present Bayesian specification inference, a probabilistic model for inferring task specification as a temporal logic formula. We incorporate methods from probabilistic programming to define our priors, along with a domain-independent likelihood function to enable sampling-based inference. We demonstrate the efficacy of our model for inferring specifications with over 90\% similarity between the inferred specification and the ground truth, both within a synthetic domain and a real-world table setting task.},
  file = {/home/cameron/Zotero/storage/AYPP2P3L/neurips_2018_preprint.pdf}
}

@article{Shapiro2011,
  title = {Conflict-Free {{Replicated Data Types To}} Cite This Version :},
  author = {Shapiro, Marc and Pregui, Nuno and Baquero, Carlos and Zawirski, Marek and Baquero, Carlos and Zawirski, Marek and Data, Conflict-free Replicated},
  year = {2011},
  isbn = {9783642245503},
  file = {/home/cameron/Zotero/storage/WMVIL74L/Shapiro et al. - 2011 - Conflict-free Replicated Data Types To cite this version.pdf}
}

@article{Shu2005,
  title = {Enabling Fast Flexible Planning through Incremental Temporal Reasoning with Conflict Extraction},
  author = {Shu, I. Hsiang and Effinger, Robert and Williams, Brian},
  year = {2005},
  journal = {ICAPS 2005 - Proceedings of the 15th International Conference on Automated Planning and Scheduling},
  pages = {252--261},
  abstract = {In order for an autonomous agent to successfully complete its mission, the agent must be able to quickly re- plan on the fly, as unforeseen events arise in the environment. This is enabled through the use of temporally flexible plans, which allow the agent to adapt to execution uncertainties, by not over committing to timing constraints, and through continuous planners, which are able to replan at any point when the current plan fails. To achieve both of these requirements, planners must have the ability to reason quickly about timing constraints. We enable continuous, temporally flexible planning through a temporal consistency algorithm (ITC), which supports incremental consistency testing on a new type of disjunctive temporal constraint network, the Temporal Plan Network (TPN), and supports focused search through incremental conflict extraction. The ITC algorithm combines the speed of shortest-path algorithms known to network optimization with the spirit of incremental algorithms such as Incremental A* and those used within truth maintenance systems (TMS). Empirical studies of ITC applied to the Kirk temporal planner demonstrate an order of magnitude speed increase on cooperative air vehicle scenarios and on randomly generated plans. Copyright \textcopyright{} 2005,American Association for Artificial Intelligence.},
  keywords = {American Association for Artific,Copyright Â© 2005},
  file = {/home/cameron/Zotero/storage/FQPSJ9JR/document.pdf}
}

@article{Shute2015,
  title = {The Power of Play: {{The}} Effects of {{Portal}} 2 and {{Lumosity}} on Cognitive and Noncognitive Skills},
  author = {Shute, Valerie J. and Ventura, Matthew and Ke, Fengfeng},
  year = {2015},
  journal = {Computers and Education},
  volume = {80},
  pages = {58--67},
  publisher = {{Elsevier Ltd}},
  issn = {03601315},
  doi = {10.1016/j.compedu.2014.08.013},
  abstract = {In this study, we tested 77 undergraduates who were randomly assigned to play either a popular video game (Portal 2) or a popular brain training game (Lumosity) for 8 h. Before and after gameplay, participants completed a set of online tests related to problem solving, spatial skill, and persistence. Results revealed that participants who were assigned to play Portal 2 showed a statistically significant advantage over Lumosity on each of the three composite measures - problem solving, spatial skill, and persistence. Portal 2 players also showed significant increases from pretest to posttest on specific small- and large-scale spatial tests while those in the Lumosity condition did not show any pretest to posttest differences on any measure. Results are discussed in terms of the positive impact video games can have on cognitive and noncognitive skills.},
  keywords = {Assessment,Persistence,Problem solving,Spatial skills,Videogames},
  file = {/home/cameron/Zotero/storage/P4QQ3LUA/1-s2.0-S0360131514001869-main.pdf}
}

@misc{slaybaugh1969,
  title = {Tycho {{Rim Engineering Evaluation}}},
  author = {Slaybaugh, J.C.},
  year = {1969},
  pages = {15},
  address = {{Washington DC. USA}},
  file = {/home/cameron/Zotero/storage/G3QWW2KX/1969.Tycho_Rim.pdf}
}

@misc{Slaybaugh1971,
  title = {A {{Method}} of {{Depicting Lunar Surface EVA}}'s, {{Illustrated}} for the {{Apollo}} 14 {{Mission}} - {{Case}} 320},
  author = {Slaybaugh, J.C.},
  year = {1971},
  month = jan,
  pages = {16},
  publisher = {{Bellcomm, Inc}},
  address = {{Washington DC. USA}},
  abstract = {Recent reviews of the Apollo 14 lunar surface mission have led to the development of two tools which are useful for Sequence Chart - a quantitative, semi-detailed listing of EVA evaluating planned lunar surface EVA's: (1) an EVA Activities Surface Procedures - and (2) an EVA Time History - a graphic activity times and sequences, similar in content to the Lunar presentation of traverse distance from the LM versus elapsed EVA time. Discussion of these tools is illustrated with the Apollo 14 Mission. The Activities Sequence Chart has proved useful as the basis for an uncertainty analysis of Apollo 14 metabolic ex- penditures, as well as for a tabulation of cumulative time spent on various types of EVA activity. provides a-format useful for evaluating operational margins for The Time History both nominal and extended EVA'S. the first EVA on Apollo 14 have been examined and found to Two possible extensions to be operationally feasible. \{NBSA-CR-l16948)\vphantom\}},
  file = {/home/cameron/Zotero/storage/NJ86V2PD/Slaybaugh1971_A_Method_of_Depicting_Lunar_Surf.pdf}
}

@article{Smith2010,
  title = {The Lunar Orbiter Laser Altimeter Investigation on the Lunar Reconnaissance Orbiter Mission},
  author = {Smith, David E. and Zuber, Maria T. and Jackson, Glenn B. and Cavanaugh, John F. and Neumann, Gregory A. and Riris, Haris and Sun, Xiaoli and Zellar, Ronald S. and Coltharp, Craig and Connelly, Joseph and Katz, Richard B. and Kleyner, Igor and Liiva, Peter and Matuszeski, Adam and Mazarico, Erwan M. and McGarry, Jan F. and {Novo-Gradac}, Anne Marie and Ott, Melanie N. and Peters, Carlton and {Ramos-Izquierdo}, Luis A. and Ramsey, Lawrence and Rowlands, David D. and Schmidt, Stephen and Scott, V. Stanley and Shaw, George B. and Smith, James C. and Swinski, Joseph Paul and Torrence, Mark H. and Unger, Glenn and Yu, Anthony W. and Zagwodzki, Thomas W.},
  year = {2010},
  journal = {Space Science Reviews},
  volume = {150},
  number = {1-4},
  pages = {209--241},
  issn = {00386308},
  doi = {10.1007/s11214-009-9512-y},
  abstract = {The Lunar Orbiter Laser Altimeter (LOLA) is an instrument on the payload of NASA's Lunar Reconnaissance Orbiter spacecraft (LRO) (Chin et al., in Space Sci. Rev. 129:391-419, 2007). The instrument is designed to measure the shape of the Moon by measuring precisely the range from the spacecraft to the lunar surface, and incorporating precision orbit determination of LRO, referencing surface ranges to the Moon's center of mass. LOLA has 5 beams and operates at 28 Hz, with a nominal accuracy of 10 cm. Its primary objective is to produce a global geodetic grid for the Moon to which all other observations can be precisely referenced. \textcopyright{} 2009 Springer Science+Business Media B.V.},
  keywords = {Moon,Shape,Space instrumentation,Topography},
  file = {/home/cameron/Zotero/storage/Q743KAJE/smith_lola_ssr09.pdf}
}

@article{Society2018,
  title = {The {{Measurement}} of {{Productive Efficiency Author}} ( s ): {{M}} . {{J}} . {{Farrell Source}} : {{Journal}} of the {{Royal Statistical Society}} . {{Series A}} ( {{General}} ), {{Vol}} . 120 , {{No}} . 3 ( 1957 ), {{Published}} by : {{Wiley}} for the {{Royal Statistical Society Stable URL}} : {{http://www.js}}},
  author = {Society, Royal Statistical},
  year = {2018},
  volume = {120},
  number = {3},
  pages = {253--290},
  file = {/home/cameron/Zotero/storage/2YVLL996/new-methods-of-quality-control.pdf}
}

@article{Solvoll2017,
  title = {Alarm {{Fatigue}} vs {{User Expectations Regarding Context-Aware Alarm Handling}} in {{Hospital Environments Using CallMeSmart}}},
  author = {Solvoll, T and Arntsen, H and Hartvigsen, G},
  year = {2017},
  journal = {Studies in health technology and informatics},
  volume = {241},
  pages = {159--164},
  issn = {0926-9630},
  doi = {10.3233/978-1-61499-794-8-159},
  abstract = {Surveys and research show that mobile communication systems in hospital settings are old and cause frequent interruptions. In the quest to remedy this, an Android based communication system called CallMeSmart tries to encapsulate most of the frequent communication into one hand held device focusing on reducing interruptions and at the same time make the workday easier for healthcare workers. The objective of CallMeSmart is to use context-awareness techniques to automatically monitor the availability of physicians' and nurses', and use this information to prevent or route phone calls, text messages, pages and alarms that would otherwise compromise patient care. In this paper, we present the results from interviewing nurses on alarm fatigue and their expectations regarding context-aware alarm handling using CallMeSmart.},
  isbn = {978-1-61499-794-8; 978-1-61499-793-1},
  pmid = {28809200},
  keywords = {alarm fatigue,callmesmart,context awareness,hospital,mobile communication,systems},
  file = {/home/cameron/Zotero/storage/JJCQAXGH/alarm-fatigue-vs-user-expectations.pdf}
}

@article{Sondik1978,
  title = {The {{Optimal Control}} of {{Partially Observable Markov Processes Over}} the {{Infinite Horizon}}},
  author = {Sondik, Edward J.},
  year = {1978},
  journal = {Operations Research},
  volume = {26},
  number = {2},
  pages = {282--304},
  abstract = {This paper treats the discounted cost, optimal control problem for Markov processes with incomplete state information. The optimi- zation approach for these partially observable Markov processes is a generalization of the well-known policy iteration technique for finding optimal stationary policies for completely observable Markov processes. The state space for the problem is the space of state oc- cupancy probability distributions (the unit simplex). The development of the algorithm introduces several new ideas, including the class of finitely transient policies, which are shown to possess piecewise linear cost functions. The paper develops easily implemented ap- proximations to stationary policies based on these finitely transient policies and shows that the concave hull of an approximation can be included in the well-known Howard policy improvement algo- rithm with subsequent convergence. The paper closes with a detailed example illustrating the application of the algorithm to the two-state partially observable Markov process.},
  file = {/home/cameron/Zotero/storage/LD64ZK2Q/169635.pdf}
}

@techreport{Sonnett1963,
  title = {{{REPORT}} of the {{AD HOC WORKING GROUP ON APOLLO EXPERIMENTS AND TRAINING}} on the {{SCIENTIFIC ASPECTS OF THE APOLLO PROGRAM}}},
  author = {Sonnett, C. P.},
  year = {1963},
  institution = {{NASA}},
  abstract = {The Committee believes that the principle scientific achievements of the Apollo missions will lie in the area of the geology of the moon; however, it is believed that sigLificant obse rvatio ns can also be made in solid body geophysics, sur\textasciitilde ace physics, the investigation of the lunar atmosphere, and i n rudio astronomy. The report outlines the scientific activities that can be done in short and long missions. The report recommends: that the missions be supported by soft landed packages; that a ground vehicle be made available for the late missions; and that the space suit have maximum limb, arm, and digital mobility. A suggested first landing site is in the Copernicus Quadrangle at approximately 30N and 28\textdegree W (Lunar Carto- graphic Coordinates), and two lists of possible sites are appended. The commi t tee has recommended that a s c i en tist{$\cdot\cdot$} astro- naut be included or each mission and that the training of scientist-astronauts be done at a NASA facility physically near the Manned Spacec r a f t Center.},
  file = {/home/cameron/Zotero/storage/793VN94K/SonettReport.pdf}
}

@techreport{Stallman1981,
  title = {Emacs {{The Extensible}}, {{Customizable}}, {{Self-Documenting Display Editor}}},
  author = {Stallman, Richard M},
  year = {1981},
  pages = {29},
  address = {{Cambridge, MA}},
  institution = {{Massachusetts Institute of Technology Artificial Intelligence Laboratory}},
  abstract = {EMACS is a display editor which is implemented in an interpreted high level language. This allows users to extend the editor by replacing parts of it, to experiment with alternative command languages, and to share extensions which are generally useful. The ease of extension has contributed to the growth of a large set of useful features. This paper describes the organization of the EMACS system, emphasizing the way in which extensibility is achieved and used.},
  file = {/home/cameron/Zotero/storage/9PPFKTCY/Stallman - 1981 - Emacs The Extensible, Customizable, Self-Documenting Display Editor.pdf}
}

@phdthesis{Stedl2004,
  title = {Managing {{Temporal Uncertainty Under Limited Communication}}: {{A Formal Model}} of {{Tight}} and {{Loose Team Coordination}}},
  author = {Stedl, John L},
  year = {2004},
  abstract = {In the future, groups of autonomous robots will cooperate in large networks in order to achieve a common goal. These multi-agent systems will need to be able to execute cooperative temporal plans in the presence of temporal uncertainty and communication limitations. The duration of many planned activities will not be under direct control of the robots. In addition, robots will often not be able to communicate during plan execution. In order for the robots to robustly execute a cooperative plan, they will need to guarantee that a successful execution strategy exists, and provide a means to reactively compensate for the uncertainty in real-time. This thesis presents a multi-agent executive that enables groups of distributed autonomous robots to dynamically schedule temporally flexible plans that contain both temporal uncertainty under communication limitations. Previous work has presented controllability algorithms that compile the simple temporal networks with uncertainty, STNUs, into a form suitable for execution. This thesis extends the previous controllability algorithms to operate on two-layer plans that specify group level coordination at the highest level and agent level coordination at a lower level. We introduce a Hierarchical Reformulation (HR) algorithm that reformulates the two-layer plan in order to enable agents to dynamically adapt to uncertainty within each group plan and use a static execution strategy between groups in order to compensate for communication limitations. Formally, the HR algorithm ensures that the two-layer plan is strongly controllable at the highest level and dynamically controllable at the lower level. Furthermore, we introduce a new fast dynamic controllability algorithm that has been empirically shown to run in O(N 3) time.},
  school = {Massachusetts Institute of Technology},
  file = {/home/cameron/Zotero/storage/6KZDVA8H/JohnLStedlSMThesis.pdf}
}

@inproceedings{stefan2012,
  title = {A Load Balancing Algorithm for Multi-Agent Systems},
  booktitle = {Studies in {{Computational Intelligence}}},
  author = {{\c S}tefan, Iulia and Moi{\c s}, George and Enyedi, Szil{\'a}rd and Miclea, Liviu},
  year = {2012},
  volume = {402},
  pages = {103--114},
  issn = {1860949X},
  doi = {10.1007/978-3-642-27449-7_8},
  abstract = {Multi-agent societies are often used in manufacturing systems and other large-scale distributed systems. These systems often need an efficient task redistribution strategy in case of component faults or load variations. This chapter presents a simple, algorithmic approach for such a strategy, requiring low processing and communications resources. \textcopyright{} 2012 Springer-Verlag Berlin Heidelberg.},
  isbn = {978-3-642-27448-0},
  keywords = {distributed system,intelligent agent,Load balancing},
  file = {/home/cameron/Zotero/storage/Z24AXYQK/A_Load_Balancing_Algorithm_for_Multi-agent_Systems.pdf}
}

@article{stevens2017,
  title = {Are Neurodynamic Organizations a Fundamental Property of Teamwork?},
  author = {Stevens, Ronald H. and Galloway, Trysha L.},
  year = {2017},
  month = may,
  journal = {Frontiers in Psychology},
  volume = {8},
  number = {MAY},
  publisher = {{Frontiers Research Foundation}},
  issn = {16641078},
  doi = {10.3389/fpsyg.2017.00644},
  abstract = {When performing a task it is important for teams to optimize their strategies and actions to maximize value and avoid the cost of surprise. The decisions teams make sometimes have unintended consequences and they must then reorganize their thinking, roles and/or configuration into corrective structures more appropriate for the situation. In this study we ask: What are the neurodynamic properties of these reorganizations and how do they relate to the moment-by-moment, and longer, performance-outcomes of teams?. We describe an information-organization approach for detecting and quantitating the fluctuating neurodynamic organizations in teams. Neurodynamic organization is the propensity of team members to enter into prolonged (minutes) metastable neurodynamic relationships as they encounter and resolve disturbances to their normal rhythms. Team neurodynamic organizations were detected and modeled by transforming the physical units of each team member's EEG power levels into Shannon entropy-derived information units about the team's organization and synchronization. Entropy is a measure of the variability or uncertainty of information in a data stream. This physical unit to information unit transformation bridges micro level social coordination events with macro level expert observations of team behavior allowing multimodal comparisons across the neural, cognitive and behavioral time scales of teamwork. The measures included the entropy of each team member's data stream, the overall team entropy and the mutual information between dyad pairs of the team. Mutual information can be thought of as periods related to team member synchrony. Comparisons between individual entropy and mutual information levels for the dyad combinations of three-person teams provided quantitative estimates of the proportion of a person's neurodynamic organizations that represented periods of synchrony with other team members, which in aggregate provided measures of the overall degree of neurodynamic interactions of the team. We propose that increased neurodynamic organization occurs when a team's operating rhythm can no longer support the complexity of the task and the team needs to expend energy to re-organize into structures that better minimize the "surprise" in the environment. Consistent with this hypothesis, the frequency and magnitude of neurodynamic organizations were less in experienced military and healthcare teams than they were in more junior teams. Similar dynamical properties of neurodynamic organization were observed in models of the EEG data streams of military, healthcare and high school science teams suggesting that neurodynamic organization may be a common property of teamwork. The innovation of this study is the potential it raises for developing globally applicable quantitative models of team dynamics that will allow comparisons to be made across teams, tasks and training protocols.},
  keywords = {EEG,Entropy,Information theory,Social coordination,Team neurodynamics,Teamwork,Uncertainty},
  file = {/home/cameron/Zotero/storage/RLMV9FL6/Are Neurodynamic Organizations a Fundemental Property of Teamwork.pdf}
}

@article{Stevens2019,
  title = {Assessing the {{Acceptability}} of {{Science Operations Concepts}} and the {{Level}} of {{Mission Enhancement}} of {{Capabilities}} for {{Human Mars Exploration Extravehicular Activity}}},
  author = {Beaton, Kara H. and Chappell, Steven P. and Abercromby, Andrew F.J. and Miller, Matthew J. and Kobs Nawotniak, S.E. E. and Brady, Allyson L. and Stevens, Adam H. and Payler, Samuel J. and Hughes, Scott S. and Lim, Darlene S.S.},
  year = {2019},
  month = mar,
  journal = {Astrobiology},
  volume = {19},
  number = {3},
  pages = {321--346},
  publisher = {{Mary Ann Liebert Inc}},
  issn = {1531-1074},
  doi = {10.1089/ast.2018.1912},
  abstract = {The Biologic Analog Science Associated with Lava Terrains (BASALT) research project is investigating tools, techniques, and strategies for conducting Mars scientific exploration extravehicular activity (EVA). This has been accomplished through three science-driven terrestrial field tests (BASALT-1, BASALT-2, and BASALT-3) during which the iterative development, testing, assessment, and refinement of concepts of operations (ConOps) and capabilities were conducted. ConOps are the instantiation of operational design elements that guide the organization and flow of personnel, communication, hardware, software, and data products to enable a mission concept. Capabilities include the hardware, software, data products, and protocols that comprise and enable the ConOps. This paper describes the simulation quality and acceptability of the Mars-forward ConOps evaluated during BASALT-2. It also presents the level of mission enhancement and acceptability of the associated Mars-forward capabilities. Together, these results inform science operations for human planetary exploration.},
  keywords = {Communication latency and bandwidth.,Extravehicular activity,Human spaceflight,Operations concepts,Planetary analogs,Science operations},
  file = {/home/cameron/Zotero/storage/HV6R7KV9/Beaton et al. - 2019 - Assessing the Acceptability of Science Operations Concepts and the Level of Mission Enhancement of Capabilities f.pdf;/home/cameron/Zotero/storage/R6QJ7VQL/Beaton et al. - 2019 - Assessing the Acceptability of Science Operations Concepts and the Level of Mission Enhancement of Capabilitie(2).pdf}
}

@article{suits,
  title = {Biomedical {{Advisory System}} \ding{169} {{Technical Challenges}} : {{Biomedical Advisory System Technical Challenges}} :},
  author = {Suits, Configuration},
  pages = {1--9},
  file = {/home/cameron/Zotero/storage/W5A6DPGM/EVA Technology Development Content Review 2009_Biomed Advisor.pdf}
}

@article{Sussman1997,
  title = {Structure and Interpretation of Computer Programs, (Second Edition)},
  author = {Sussman, Gerald Jay and Sussman, Julie},
  year = {1997},
  journal = {Computers \& Mathematics with Applications},
  volume = {33},
  number = {4},
  pages = {133},
  issn = {08981221},
  doi = {10.1016/s0898-1221(97)90051-1},
  abstract = {Structure and Interpretation of Computer Programs has had a dramatic impact on computer science curricula over the past decade. This long-awaited revision contains changes throughout the text.There are new implementations of most of the major programming systems in the book, including the interpreters and compilers, and the authors have incorporated many small changes that reflect their experience teaching the course at MIT since the first edition was published.A new theme has been introduced that emphasizes the central role played by different approaches to dealing with time in computational models: objects with state, concurrent programming, functional programming and lazy evaluation, and nondeterministic programming. There are new example sections on higher-order procedures in graphics and on applications of stream processing in numerical programming, and many new exercises.In addition, all the programs have been reworked to run in any Scheme implementation that adheres to the IEEE standard.},
  file = {/home/cameron/Zotero/storage/Q8YQVHK4/sicp.pdf}
}

@article{Sverdlik2022,
  title = {Doctoral Students and {{COVID-19}}: Exploring Challenges, Academic Progress, and Well-Being},
  author = {Sverdlik, Anna and Hall, Nathan C and Vallerand, Robert J},
  year = {2022},
  month = jun,
  journal = {Educational Psychology},
  pages = {1--16},
  doi = {10.1080/01443410.2022.2091749},
  abstract = {Doctoral students often struggle with depression, anxiety, loneliness, and physical concerns, that are directly associated with their programs. Supporting doctoral students' well-being becomes critical during a global pandemic, when students become further isolated, uncertain, and struggle academically. The present study examined students' top challenges and coping strategies during the COVID-19 crisis, as well as gender differences in academic progress and well-being. Students' top challenges included: inability to see family/friends, being home-bound, blurring of work/ family time, isolation, and inability to access campus. Students' top coping strategies included: seeking social support, working, exercising, watching television, and creating a comfortable routine. The COVID-19 outbreak has disrupted female students' research progress more than males', and female doctoral students reported feeling anxious, upset, and irritable when engaging in academic work, while males felt enthusiastic. Finally, general anxiety and stress was higher in females than males during the COVID-19 outbreak. Theoretical and practical implications are discussed.},
  langid = {english},
  file = {/home/cameron/Zotero/storage/VSR83M4B/Sverdlik et al. - Doctoral students and COVID-19 exploring challeng.pdf}
}

@article{Tantardini2019,
  title = {Comparing Methods for Comparing Networks},
  author = {Tantardini, Mattia and Ieva, Francesca and Tajoli, Lucia and Piccardi, Carlo},
  year = {2019},
  journal = {Scientific Reports},
  volume = {9},
  number = {1},
  pages = {1--19},
  issn = {20452322},
  doi = {10.1038/s41598-019-53708-y},
  abstract = {With the impressive growth of available data and the flexibility of network modelling, the problem of devising effective quantitative methods for the comparison of networks arises. Plenty of such methods have been designed to accomplish this task: most of them deal with undirected and unweighted networks only, but a few are capable of handling directed and/or weighted networks too, thus properly exploiting richer information. In this work, we contribute to the effort of comparing the different methods for comparing networks and providing a guide for the selection of an appropriate one. First, we review and classify a collection of network comparison methods, highlighting the criteria they are based on and their advantages and drawbacks. The set includes methods requiring known node-correspondence, such as DeltaCon and Cut Distance, as well as methods not requiring a priori known node-correspondence, such as alignment-based, graphlet-based, and spectral methods, and the recently proposed Portrait Divergence and NetLSD. We test the above methods on synthetic networks and we assess their usability and the meaningfulness of the results they provide. Finally, we apply the methods to two real-world datasets, the European Air Transportation Network and the FAO Trade Network, in order to discuss the results that can be drawn from this type of analysis.},
  pmid = {31772246},
  file = {/home/cameron/Zotero/storage/8J495HGM/Tantardini et al. - 2019 - Comparing methods for comparing networks.pdf}
}

@article{Tapia2008,
  title = {Using Machine Learning for Real-Time Activity Recognition and Estimation of Energy Expenditure},
  author = {Tapia, E Munguia},
  year = {2008},
  journal = {Architecture},
  pages = {493},
  issn = {{$<$}null{$>$}},
  doi = {10.1093/jicru/ndi028},
  abstract = {Obesity is now considered a global epidemic and is predicted to become the number one preventive health threat in the industrialized world. Presently, over 60\% of the U.S. adult population is overweight and 30\% is obese. This is of concern because obesity is linked to leading causes of death, such as heart and pulmonary diseases, stroke, and type 2 diabetes. The dramatic rise in obesity rates is attributed to an environment that provides easy access to high caloric food and drink and promotes low levels of physical activity. Unfortunately, many people have a poor understanding of their own daily energy (im)balance: the number of calories they consume from food compared with what they expend through physical activity. Accelerometers offer promise as an objective measure of physical activity. In prior work they have been used to estimate energy expenditure and activity type. This work further demonstrates how wireless accelerometers can be used for real-time automatic recognition of physical activity type, intensity, and duration and estimation of energy expenditure. The parameters of the algorithms such as type of classifier/regressor, feature set, window length, signal preprocessing, sensor set utilized and their placement on the human body are selected by performing a set of incremental experiments designed to identify sets of parameters that may balance system usability with robust, real-time performance in low processing power devices such as mobile phones. The algorithms implemented are evaluated using a dataset of examples of 52 activities collected from 20 participants at a gymnasium and a residential home. The algorithms presented here may ultimately allow for the development of mobile phone-based just-in-time interventions to increase self-awareness of physical activity patterns and increases in physical activity levels in real-time during free-living that scale to large populations.},
  isbn = {9783110258899},
  pmid = {24170875},
  file = {/home/cameron/Zotero/storage/NY5M5PYU/300459396-MIT.pdf}
}

@techreport{tapia2008,
  title = {Using {{Machine Learning}} for {{Real-time Activity Recognition}} and {{Estimation}} of {{Energy Expenditure}}},
  author = {Tapia, Emmanuel Munguia},
  year = {2008},
  file = {/home/cameron/Zotero/storage/2XUYVABD/300459396-MIT.pdf}
}

@techreport{team2020,
  title = {Artemis {{III Science Definition Team Report The Beginning}} of a {{Bold New Era}} of {{Human Discovery DRAFT}}},
  author = {Team, Artemis III Science Definition},
  year = {2020},
  pages = {73},
  file = {/home/cameron/Zotero/storage/DZTZYRRN/Team - 2020 - Artemis III Science Definition Team Report The Beginning of a Bold New Era of Human Discovery DRAFT.pdf}
}

@book{Tedrake2021,
  title = {Robotic {{Manipulation}}},
  author = {Tedrake, Russ},
  year = {2021},
  address = {{Cambridge, MA}},
  file = {/home/cameron/Zotero/storage/G3EP7JHX/m-api-470442e7-0f99-a00f-5424-dfb7f979a045.pdf}
}

@article{Thalamy2019,
  title = {Distributed {{Self-Reconfiguration}} Using a {{Deterministic Autonomous Scaffolding Structure}}},
  author = {Thalamy, Pierre and Piranda, Benoit and Bourgeois, Julien},
  year = {2019},
  journal = {Proc. 18th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS)},
  pages = {140--148},
  isbn = {978-1-4503-6309-9},
  keywords = {2019,acm reference format,and julien bourgeois,autonomous robots,Autonomous Robots,benoÃ®t piranda,distributed algorithm,Distributed Algorithm,distributed self-,pierre thalamy,self-reconfiguration,Self-Reconfiguration},
  file = {/home/cameron/Zotero/storage/3M4BR4GI/p140.pdf}
}

@techreport{Theibaux2018,
  title = {Negotiating {{Mission Plans}} under {{Risk Bounds}}},
  author = {Thiebaux, Sylvie and Haslum, Patrik and Williams, Brian C. and Baumgartner, Peter and Trevizan, Felipe and Santana, Pedro and Vaquero, Tiago},
  year = {2018},
  address = {{Canberra, Australia}},
  institution = {{COMMONWEALTH SCIENTIFIC AND INUSTRIAL RESEARCH ORGANISATION}},
  abstract = {Autonomous systems operating in uncertain environments face the problem of optimizing their performance whilst satisfying complex mission constraints with high probability and bounding the risk of plan execution failure. Such problems can be modelled as constrained stochastic shortest path problems (C-SSPs), which are an active research topic in the operations research, artificial intelligence, robotics, and software verification communities. However, all existing algorithms for C-SSPs require generating and exploring the entire state space of the problem, making them im- practical for autonomous systems which have huge state spaces. This project has made significant advances towards more practical solutions to C-SSPs. We have devised the first heuristic search algorithms for C-SSPs. These algorithms typically explore a small fraction of the state space, and enable solving much larger problems than was previously possible. To be effective, they must be provided with an admissible heuristic function, that is a lower bound on the expected cost to reach the system goal under the constraints. To achieve this, we have devised the first domain-independent heuristics that take into account uncertainty, costs, and constraints. Our heuristics have become the state of the art also for regular (unconstrained) SSPs: even though heuristic search had been used to solve SSPs for over two decades, existing heuristics ignored uncertainty altogether. Moreover, we have extended these algorithms and heuristics in several directions, including to rich probabilistic temporal logic constraints, to partially observable environments, to hybrid dis- crete/continuous environments, and to efficiently handle dead-ends. In doing so, we have bridged the gap between several areas of research, across several scientific communities, specifically, heuris- tic search, classical planning heuristics and planning under uncertainty in Artificial Intelligence, Markov decision processes in Operations Research, synthesis of policies with probabilistic temporal logic objectives in the Formal Verification community, and motion planning in Robotics. Two of the ten top tier project publications have received best paper awards at the Interna- tional Conference on Automated Planning and Scheduling (ICAPS) in 2016, and 2017, respectively. Applications of these results are underway in collaboration with partners in the aerospace and au- tomobile industries.},
  file = {/home/cameron/Zotero/storage/K3Y2LUVE/Thiebaux et al. - 2018 - Negotiating Mission Plans under Risk Bounds.pdf}
}

@article{Timmermann2006,
  title = {Chapter 4 {{Forecast Combinations}}},
  author = {Timmermann, Allan},
  year = {2006},
  journal = {Handbook of Economic Forecasting},
  volume = {1},
  number = {05},
  eprint = {1011.1669v3},
  eprinttype = {arxiv},
  pages = {135--196},
  issn = {15740706},
  doi = {10.1016/S1574-0706(05)01004-9},
  abstract = {Forecast combinations have frequently been found in empirical studies to produce better forecasts on average than methods based on the ex ante best individual forecasting model. Moreover, simple combinations that ignore correlations between forecast errors often dominate more refined combination schemes aimed at estimating the theoretically optimal combination weights. In this chapter we analyze theoretically the factors that determine the advantages from combining forecasts (for example, the degree of correlation between forecast errors and the relative size of the individual models' forecast error variances). Although the reasons for the success of simple combination schemes are poorly understood, we discuss several possibilities related to model misspecification, instability (non-stationarities) and estimation error in situations where the number of models is large relative to the available sample size. We discuss the role of combinations under asymmetric loss and consider combinations of point, interval and probability forecasts. \textcopyright{} 2006 Elsevier B.V. All rights reserved.},
  archiveprefix = {arXiv},
  isbn = {9780444513953},
  pmid = {25246403},
  keywords = {diversification gains,forecast combinations,model misspecification,pooling and trimming,shrinkage methods},
  file = {/home/cameron/Zotero/storage/2ESY6V7H/forecast-combinations.pdf}
}

@article{timmermann2006,
  title = {Chapter 4 {{Forecast Combinations}}},
  author = {Timmermann, Allan},
  year = {2006},
  journal = {Handbook of Economic Forecasting},
  volume = {1},
  pages = {135--196},
  issn = {15740706},
  doi = {10.1016/S1574-0706(05)01004-9},
  abstract = {Forecast combinations have frequently been found in empirical studies to produce better forecasts on average than methods based on the ex ante best individual forecasting model. Moreover, simple combinations that ignore correlations between forecast errors often dominate more refined combination schemes aimed at estimating the theoretically optimal combination weights. In this chapter we analyze theoretically the factors that determine the advantages from combining forecasts (for example, the degree of correlation between forecast errors and the relative size of the individual models' forecast error variances). Although the reasons for the success of simple combination schemes are poorly understood, we discuss several possibilities related to model misspecification, instability (non-stationarities) and estimation error in situations where the number of models is large relative to the available sample size. We discuss the role of combinations under asymmetric loss and consider combinations of point, interval and probability forecasts. \textcopyright{} 2006 Elsevier B.V. All rights reserved.},
  isbn = {9780444513953},
  keywords = {diversification gains,forecast combinations,model misspecification,pooling and trimming,shrinkage methods},
  file = {/home/cameron/Zotero/storage/S5CLU9UF/forecast-combinations.pdf}
}

@misc{Timmons,
  title = {A {{Language}} and {{Executive}} for {{Risk-Aware}} , {{Timed}} , {{Multiagent}} , {{State-Based Programming}} - {{Proposal DRAFT}}},
  author = {Timmons, Eric},
  pages = {1--10},
  file = {/home/cameron/Zotero/storage/G5554RW9/Timmons Thesis Proposal Draft 2021-02-03.pdf}
}

@article{Timmons2016,
  title = {Preliminary {{Deployment}} of a {{Risk-aware Goal-directed Executive}} on {{Autonomous Underwater Glider}}},
  author = {Timmons, Eric and Vaquero, Tiago and Williams, Brian C and Camilli, Richard},
  year = {2016},
  journal = {PlanRob Workshop, ICAPS 2016},
  abstract = {In this paper we describe a three-layered architecture for resilient control of autonomous systems and present the risk-aware goal-directed executive residing in the top layer of the architecture. The executive combines (1) a hierarchical activity planner that performs goal selection; (2) a generative planner for activities with probabilistic durations; (3) a kino-dynamic path planner that allows a vehicle to traverse an environment with bounded risk of obstacle collision; and (4) an execution monitor. A preliminary, simplified version of the executive has been used to plan for an autonomous underwater glider in the Timor Sea. In the process, the executive managed temporal constraints, the AUV's dynamics, and the lagoon's topology. We conclude with lessons learned that will be of interest to the PlanRob community and a plan forward for the deployment of the full exeuctive.},
  keywords = {application,autonomous underwater vehicle,goal-directed,goal-directed architecture,risk-aware,risk-aware architecture},
  file = {/home/cameron/Zotero/storage/YDAAQNSS/Timmons et al. - 2016 - Preliminary Deployment of a Risk-aware Goal-directed Executive on Autonomous Underwater Glider.pdf}
}

@article{Tompkins2010,
  title = {Flight Operations for the {{LCROSS}} Lunar Impactor Mission},
  author = {Tompkins, Paul D. and Hunt, Rusty and D'Ortenzio, Matt and Strong, James and Galal, Ken and Bresina, John L. and Foreman, Darin and Barber, Robert and Shirley, Mark and Munger, James and Drucker, Eric},
  year = {2010},
  journal = {SpaceOps 2010 Conference},
  number = {April},
  pages = {1--13},
  doi = {10.2514/6.2010-1986},
  abstract = {The LCROSS (Lunar CRater Observation and Sensing Satellite) mission was conceived as a low-cost means of determining the nature of hydrogen concentrated at the polar regions of the moon. Co-manifested for launch with LRO (Lunar Reconnaissance Orbiter), LCROSS guided its spent Centaur upper stage into the Cabeus crater as a kinetic impactor, and observed the impact flash and resulting debris plume for signs of water and other compounds from a Shepherding Spacecraft. Led by NASA Ames Research Center, LCROSS flight operations spanned 112 days, from June 18 through October 9, 2009. This paper summarizes the experiences from the LCROSS flight, highlights the challenges faced during the mission, and examines the reasons for its ultimate success. \textcopyright{} 2010 by the American Institute of Aeronautics and Astronautics, Inc.},
  isbn = {9781624101649},
  file = {/home/cameron/Zotero/storage/LM4Q6752/6.2010-1986.pdf}
}

@article{Torvalds1997,
  title = {Linux: A Portable Operating System},
  author = {Torvalds, L},
  year = {1997},
  journal = {University of Helsinki},
  pages = {52},
  abstract = {We explore the hardware portability issues in Linux that were uncovered when porting the operating system to multiple CPU and bus architectures. We also discuss software interface portability issues, especially with regard to binary compatibility with other operating systems that can share the same hardware platform. The approach taken in Linux is described, with a few example architectures covered in some more detail.},
  keywords = {Alpha,Linux,Operating System,Portability,Sparc},
  file = {/home/cameron/Zotero/storage/GAC27GFR/m-api-32c2b8d5-77b2-3141-29d5-95f8201c5785.pdf}
}

@article{Trimble2016,
  title = {Agile: {{From}} Software to Mission System},
  author = {Trimble, Jay and Shirley, Mark H. and Hobart, Sarah Groves},
  year = {2016},
  journal = {SpaceOps 2016 Conference},
  number = {May},
  pages = {1--8},
  doi = {10.2514/6.2016-2477},
  abstract = {The Resource Prospector (RP) is an in-situ resource utilization (ISRU) technology demonstration mission, designed to search for volatiles at the Lunar South Pole. This is NASA's first near real time tele-operated rover on the Moon. The primary objective is to search for volatiles at one of the Lunar Poles. The combination of short mission duration, a solar powered rover, and the requirement to explore shadowed regions makes for an operationally challenging mission. To maximize efficiency and flexibility in Mission System design and thus to improve the performance and reliability of the resulting Mission System, we are tailoring Agile principles that we have used effectively in ground data system software development and applying those principles to the design of elements of the mission operations system.},
  isbn = {9781624104268},
  file = {/home/cameron/Zotero/storage/Y23X7264/6.2016-2477.pdf}
}

@phdthesis{Tsamardinos1998,
  title = {Reformulating {{Temporal Plans}} for {{Efficient Execution Ioannis Tsamardinos}}},
  author = {Tsamardinos, Ioannis and Morris, Paul},
  year = {1998},
  journal = {Principles of Knowledge Representation and Reasoning},
  number = {June},
  abstract = {The Simple Temporal Network formalism permits signiicant exibility in specifying the occurrence time of events in temporal plans. However, to retain this exibility during execution , there is a need to propagate the actual execution times of past events so that the occurrence windows of future events are adjusted appropriately. Unfortunately, this may run afoul of tight real-time control requirements that dictate extreme eeciency. The performance may be improved by restricting the propagation. However, a fast, locally propagating, execution controller may incorrectly execute a consistent plan. To resolve this dilemma, we identify a class of dis-patchable networks that are guaranteed to execute correctly under local propagation. We show that every consistent temporal plan can be reformulated as an equivalent dispatch-able network, and we present an algorithm that constructs such a network. Moreover, the constructed network is shown to have a minimum number of edges among all such networks. This algorithm will be own on an autonomous spacecraft as part of the Deep Space 1 Remote Agent experiment.},
  school = {University of Pittsburgh},
  file = {/home/cameron/Zotero/storage/HLH5YC7L/download.pdf}
}

@article{Tsamardinos2002,
  title = {A Probabilistic Approach to Robust Execution of Temporal Plans with Uncertainty},
  author = {Tsamardinos, Ioannis},
  year = {2002},
  journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
  volume = {2308},
  pages = {97--108},
  issn = {16113349},
  doi = {10.1007/3-540-46014-4_10},
  abstract = {In Temporal Planning a typical assumption is that the agent controls the execution time of all events such as starting and ending actions. In real domains however, this assumption is commonly violated and certain events are beyond the direct control of the plan's executive. Previous work on reasoning with uncontrollable events (Simple Temporal Problem with Uncertainty) assumes that we can bound the occurrence of each uncontrollable within a time interval. In principle however, there is no such bounding interval since there is always a non-zero probability the event will occur outside the bounds. Here we develop a new more general formalism called the Probabilistic Simple Temporal Problem (PSTP) following a probabilistic approach. We present a method for scheduling a PSTP maximizing the probability of correct execution. Subsequently, we use this method to solve the problem of finding an optimal execution strategy, i.e. a dynamic schedule where scheduling decisions can be made on-line.},
  isbn = {3540434720},
  file = {/home/cameron/Zotero/storage/UPKYK6BV/10.1.1.75.1325.pdf}
}

@article{Tsamardinos2003,
  title = {Efficient Solution Techniques for Disjunctive Temporal Reasoning Problems},
  author = {Tsamardinos, Ioannis and Pollack, Martha E.},
  year = {2003},
  journal = {Artificial Intelligence},
  volume = {151},
  number = {1-2},
  pages = {43--89},
  issn = {00043702},
  doi = {10.1016/S0004-3702(03)00113-9},
  abstract = {Over the past few years, a new constraint-based formalism for temporal reasoning has been developed to represent and reason about Disjunctive Temporal Problems (DTPs). The class of DTPs is significantly more expressive than other problems previously studied in constraint-based temporal reasoning. In this paper we present a new algorithm for DTP solving, called Epilitis, which integrates strategies for efficient DTP solving from the previous literature, including conflict-directed backjumping, removal of subsumed variables, and semantic branching, and further adds no-good recording as a central technique. We discuss the theoretical and technical issues that arise in successfully integrating this range of strategies with one another and with no-good recording in the context of DTP solving. Using an implementation of Epilitis, we explore the effectiveness of various combinations of strategies for solving DTPs, and based on this analysis we demonstrate that Epilitis can achieve a nearly two order-of-magnitude speed-up over the previously published algorithms on benchmark problems in the DTP literature. \textcopyright{} 2003 Elsevier B.V. All rights reserved.},
  keywords = {Constraint satisfaction,Constraint-based temporal reasoning,Planning,Scheduling},
  file = {/home/cameron/Zotero/storage/QELN93IL/Tsamardinos, Pollack - 2003 - Efficient solution techniques for disjunctive temporal reasoning problems.pdf}
}

@phdthesis{TsamardinosThesis,
  title = {Reformulating {{Temporal Plans}} for {{Efficient Execution}}},
  author = {Tsamardinos, Ioannis},
  year = {1998},
  number = {June},
  school = {University of Pittsburgh},
  file = {/home/cameron/Zotero/storage/NMYNLPA7/Tsamardinos - 1998 - Reformulating Temporal Plans for Efficient Execution.pdf}
}

@article{turki2019,
  title = {A {{Complete Multi-Robot Path-Planning Algorithm}} {${_\ast}$}},
  author = {Turki, Ebtehal and Alotaibi, Saho},
  year = {2019},
  pages = {158--160},
  keywords = {2019,a complete multi-robot path-planning,acm reference format,complete algorithms,ebtehal turki saho alotaibi,multi-robot,path planning algorithms},
  file = {/home/cameron/Zotero/storage/8VFMQ5LN/p158.pdf}
}

@misc{Vaz2018,
  title = {A Localization Approach for Autonomous Underwater Vehicles: {{A ROS-Gazebo}} Framework},
  shorttitle = {A Localization Approach for Autonomous Underwater Vehicles},
  author = {Vaz, Frederico C. and Portugal, David and Ara{\'u}jo, Andr{\'e} and Couceiro, Micael S. and Rocha, Rui P.},
  year = {2018},
  month = nov,
  number = {arXiv:1811.05836},
  eprint = {1811.05836},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  abstract = {Autonomous Underwater Vehicles (AUVs) have the ability to operate in harsh underwater environments without endangering human lives in the process. Nevertheless, just like their ground and aerial counterparts, AUVs need to be able to estimate their own position. Yet, unlike ground and aerial robots, estimating the pose of AUVs is very challenging, with only a few high-cost technological solutions available in the market. In this paper, we present the development of a realistic underwater acoustic model, implemented within the Robot Operating System (ROS) and the Gazebo simulator framework, for localization of AUVs using a set of water surface robots, time of flight of underwater propagated acoustic waves, and a multilateration genetic algorithm approach.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {/home/cameron/Zotero/storage/WGQ7R63M/Vaz et al. - 2018 - A localization approach for autonomous underwater .pdf}
}

@article{Veletsianos2012,
  title = {Assumptions and {{Challenges}} of {{Open Scholarship}}},
  author = {Veletsianos, George and Kimmons, Royce},
  year = {2012},
  journal = {International Review of Research in Open and Distributed Learning},
  volume = {13},
  pages = {166--181},
  issn = {0207-401X},
  doi = {10.19173/irrodl.v13i4.1313},
  abstract = {Researchers, educators, policymakers, and other education stakeholders hope and anticipate that openness and open scholarship will generate positive outcomes for education and scholarship. Given the emerging nature of open practices, educators and scholars are finding themselves in a position in which they can shape and/or be shaped by openness. The intention of this paper is (a) to identify the assumptions of the open scholarship movement and (b) to highlight challenges associated with the movement's aspirations of broadening access to education and knowledge. Through a critique of technology use in education, an understanding of educational technology narratives and their unfulfilled potential, and an appreciation of the negotiated implementation of technology use, we hope that this paper helps spark a conversation for a more critical, equitable, and effective future for education and open scholarship.},
  file = {/home/cameron/Zotero/storage/VG98KXZZ/R01- Veletsainos, Kimmons Open Scholarship.pdf}
}

@article{Vidal1999,
  title = {Handling {{Contingency}} in {{Temporal Constraint Networks}}: {{From Consistency}} to {{Controllabilities}}},
  author = {Vidal, Thierry and Fargier, H{\'e}l{\'e}ne},
  year = {1999},
  journal = {Journal of Experimental and Theoretical Artificial Intelligence},
  volume = {11},
  number = {1},
  pages = {23--45},
  issn = {13623079},
  doi = {10.1080/095281399146607},
  abstract = {Temporal Constraint Networks (TCN) allow one to express minimal and maximal durations between time-points. Although being used in many research areas, this model disregards the contingent nature of some constraints, whose effective duration cannot be decided by the system but is provided by the external world. We propose an extension of TCN based on the definition of the Simple Temporal Problem under Uncertainty (STPU) in which the classical network consistency property must be redefined in terms of controllability: intuitively, we would like to say that a network is controllable iff it is consistent in any situation (i.e. any assignment of the whole set of contingent intervals) that may arise in the external world. Three levels of controllability must be distinguished, namely the Strong, the Weak and the Dynamic ones. This paper provides a full characterization of those properties and their usefulness in practice, and proposes algorithms for checking them. Complexity issues and tractable equivalence classes are only partially tackled, since it is still the topic of on-going work. All the same, hardness is discussed and argued, giving evidence for the general intractability of Dynamic controllability, which is the most commonly required property in domains such as planning or scheduling. \textcopyright{} 1999 Taylor \& Francis Group, LLC.},
  keywords = {Temporal constraints uncertainty planning and sche},
  file = {/home/cameron/Zotero/storage/HTEIUCEN/10.1.1.23.3619.pdf}
}

@article{Vidal2000,
  title = {Controllability Characterization and Checking in {{Contingent Temporal Constraint Networks}}},
  author = {Vidal, Thierry},
  year = {2000},
  journal = {Proceedings of the Seventh International Conference on Principles of Knowledge Representation and Reasoning},
  pages = {559--570},
  abstract = {Temporal Constraint Networks allow to express possible durations or delays between time-points, in the shape of intervals of values. A solution of such a network is a precise ...},
  file = {/home/cameron/Zotero/storage/WDADQ7ZB/10.1.1.24.3824.pdf}
}

@article{Vijayan2017,
  title = {Alignment of Dynamic Networks},
  author = {Vijayan, V. and Critchlow, D. and Milenkovi{\'c}, T.},
  year = {2017},
  journal = {Bioinformatics},
  volume = {33},
  number = {14},
  pages = {i180-i189},
  issn = {14602059},
  doi = {10.1093/bioinformatics/btx246},
  abstract = {Motivation: Network alignment (NA) aims to find a node mapping that conserves similar regions between compared networks. NA is applicable to many fields, including computational biology, where NA can guide the transfer of biological knowledge from well-to poorly-studied species across aligned network regions. Existing NA methods can only align static networks. However, most complex real-world systems evolve over time and should thus be modeled as dynamic networks. We hypothesize that aligning dynamic network representations of evolving systems will produce superior alignments compared to aligning the systems' static network representations, as is currently done. Results: For this purpose, we introduce the first ever dynamic NA method, DynaMAGNA ++. This proof-of-concept dynamic NA method is an extension of a state-of-the-art static NA method, MAGNA++. Even though both MAGNA++ and DynaMAGNA++ optimize edge as well as node conservation across the aligned networks, MAGNA++ conserves static edges and similarity between static node neighborhoods, while DynaMAGNA++ conserves dynamic edges (events) and similarity between evolving node neighborhoods. For this purpose, we introduce the first ever measure of dynamic edge conservation and rely on our recent measure of dynamic node conservation. Importantly, the two dynamic conservation measures can be optimized with any state-of-the-art NA method and not just MAGNA++. We confirm our hypothesis that dynamic NA is superior to static NA, on synthetic and real-world networks, in computational biology and social domains. DynaMAGNA++ is parallelized and has a user-friendly graphical interface.},
  pmid = {28881980},
  file = {/home/cameron/Zotero/storage/5JVTQPIP/Vijayan, Critchlow, MilenkoviÄ‡ - 2017 - Alignment of dynamic networks.pdf}
}

@article{Vijayan2018,
  title = {Aligning Dynamic Networks with {{DynaWAVE}}},
  author = {Vijayan, Vipin and Milenkovi{\'c}, Tijana},
  year = {2018},
  journal = {Bioinformatics},
  volume = {34},
  number = {10},
  pages = {1795--1798},
  issn = {14602059},
  doi = {10.1093/bioinformatics/btx841},
  abstract = {Motivation Network alignment (NA) aims to find similar (conserved) regions between networks, such as cellular networks of different species. Until recently, existing methods were limited to aligning static networks. However, real-world systems, including cellular functioning, are dynamic. Hence, in our previous work, we introduced the first ever dynamic NA method, DynaMAGNA++, which improved upon the traditional static NA. However, DynaMAGNA++ does not necessarily scale well to larger networks in terms of alignment quality or runtime. Results To address this, we introduce a new dynamic NA approach, DynaWAVE. We show that DynaWAVE complements DynaMAGNA++: while DynaMAGNA++ is more accurate yet slower than DynaWAVE for smaller networks, DynaWAVE is both more accurate and faster than DynaMAGNA++ for larger networks. We provide a friendly user interface and source code for DynaWAVE. Availability and implementation https://www.nd.edu/-cone/DynaWAVE/.},
  pmid = {29300873},
  file = {/home/cameron/Zotero/storage/FCHX4C9S/Vijayan, MilenkoviÄ‡ - 2018 - Aligning dynamic networks with DynaWAVE.pdf}
}

@article{Voshell2016,
  title = {Multi-Level Human-Autonomy Teams for Distributed Mission Management},
  author = {Voshell, Martin and Tittle, James and Roth, Emilie},
  year = {2016},
  journal = {AAAI Spring Symposium - Technical Report},
  volume = {SS-16-01 -},
  pages = {40--44},
  abstract = {Control of the air in envisioned large-scale battles against near-peer adversaries will require revolutionary new approaches to airborne mission management, where decision authority and platform autonomy are dynamically delegated and functional roles and combat capabilities are assigned across multiple distributed tiers of platforms and human operators. System capabilities range from traditional airborne battle managers, to manned tactical aviators, to autonomous unmanned aerial systems. Due to the overwhelming complexity, human operators will require the assistance of advanced autonomy decision aids with new mechanisms for operator supervision and management of teams of manned and unmanned systems. In this paper we describe a conceptual distributed mission management approach that employs novel human-automation teaming constructs to address the complexity of envisioned operations in highly contested environments. We then discuss a cognitive engineering approach to designing role- and task-tailored human machine interfaces between humans and the autonomous systems. We conclude with a discussion of multi-level evaluation approaches for experimentation.},
  isbn = {9781577357544},
  file = {/home/cameron/Zotero/storage/TH2EAV82/12758-56110-1-PB.pdf}
}

@article{Wageman2009,
  title = {Leading {{Teams When}} the {{Time}} Is {{Right}}:. {{Finding}} the {{Best Moments}} to {{Act}}},
  author = {Wageman, Ruth and Fisher, Colin M. and Hackman, J. Richard},
  year = {2009},
  journal = {Organizational Dynamics},
  volume = {38},
  number = {3},
  pages = {192--203},
  issn = {00902616},
  doi = {10.1016/j.orgdyn.2009.04.004},
  file = {/home/cameron/Zotero/storage/AAMRZ7YP/Wageman, Fisher, Hackman - 2009 - Leading Teams When the Time is Right. Finding the Best Moments to Act.pdf}
}

@inproceedings{Wang,
  title = {{{TBurton}}: {{A}} Divide and Conquer Temporal Planner},
  booktitle = {Proceedings of the {{National Conference}} on {{Artificial Intelligence}}},
  author = {Wang, David and Williams, Brian C.},
  year = {2015},
  volume = {5},
  pages = {3409--3417},
  abstract = {Planning for and controlling a network of interacting devices requires a planner that accounts for the automatic timed transitions of devices, while meeting deadlines and achieving durative goals. Consider a planner for an imaging satellite with a camera that cannot tolerate exhaust. The planner would need to determine that opening a valve causes a chain reaction that ignites the engine, and thus needs to shield the camera. While planners exist that support deadlines and durative goals, currently, no planners can handle automatic timed transitions. We present tBurton, a temporal planner that supports these features, while additionally producing a temporally least-commitment plan. tBurton uses a divide and conquer approach: dividing the problem using causal-graph decomposition and conquering each factor with heuristic forward search. The 'sub-plans' from each factor are then unified in a conflict directed search, guided by the causal graph structure. We describe why this approach is fast and efficient, and demonstrate its ability to improve the performance of existing planners on factorable problems through benchmarks from the International Planning Competition.},
  isbn = {978-1-57735-703-2},
  keywords = {Planning and Scheduling Track},
  file = {/home/cameron/Zotero/storage/P384B4QJ/d7dc7c15734e60d325e924ab9363ba86bbb0.pdf}
}

@article{Wang2015,
  title = {Chance-Constrained Scheduling via Conflict-Directed Risk Allocation},
  author = {Wang, Andrew J. and Williams, Brian C.},
  year = {2015},
  journal = {Proceedings of the National Conference on Artificial Intelligence},
  volume = {5},
  pages = {3620--3627},
  abstract = {Temporal uncertainty in large-scale logistics forces one to trade off between lost efficiency through built-in slack and costly replanning when deadlines are missed. Due to the difficulty of reasoning about such likelihoods and consequences, a computational framework is needed to quantify and bound the risk of violating scheduling requirements. This work addresses the chance-constrained scheduling problem, where actions' durations are modeled probabilistically. Our solution method uses conflict-directed risk allocation to efficiently compute a scheduling policy. The key insight, compared to previous work in probabilistic scheduling, is to decouple the reasoning about temporal and risk constraints. This decomposes the problem into a separate master and subproblem, which can be iteratively solved much quicker. Through a set of simulated car-sharing scenarios, it is empirically shown that conflict-directed risk allocation computes solutions nearly an order of magnitude faster than prior art does, which considers all constraints in a single lump-sum optimization.},
  isbn = {9781577357032},
  file = {/home/cameron/Zotero/storage/PWTWS2GT/Williams_Chance-constrained scheduling via.pdf}
}

@article{washington2019,
  title = {{{IAC-19-B3}}.5.7x50867 {{Immersive Mixed Reality Capabilities}} for {{Planning}} and {{Executing Exploration Extravehicular Activity Kara H}}. {{Beaton}}},
  author = {Washington, D C and States, United},
  year = {2019},
  number = {October},
  pages = {21--25},
  keywords = {abbreviations,acronyms,capability,extravehicular activity,mixed reality,science operations,spaceflight analog},
  file = {/home/cameron/Zotero/storage/UDXTXNRR/2019 Beaton et al_IAC BASALT-3_UPLOAD2.pdf}
}

@techreport{Wehowsky2005,
  title = {{{ROBUST DISTRIBUTED COORDINATION OF HETEROGENEOUS ROBOTS THROUGH TEMPORAL PLAN NETWORKS}}},
  author = {Wehowsky, Andreas and Block, Stephen A. and Williams, Brian C.},
  year = {2005},
  pages = {1--8},
  address = {{Cambridge, MA}},
  institution = {{Massachusetts Institute of Technology}},
  abstract = {Real-world applications of autonomous agents require coordinated groups to work in collaboration. Dependable systems must plan and carry out activities in a way that is robust to failure to and uncertainty. Previous work has produced algorithms that provide robustness at the plan- ning phase, by choosing between functionally redundant methods, and at the execution phase, by dispatching tem- porally flexible plans. However, these algorithms use a centralized architecture in which all computation is per- formed by a single processor. As a result, these imple- mentations suffer from communication bottlenecks at the master processor, require significant computational capa- bilities, and do not scale well. This paper introduces the plan extraction component of a robust, distributed executive for contingent plans. Con- tingent plans are encoded as Temporal Plan Networks (TPNs), which compose temporally flexible plans hier- archically and provide a choose operator. First, the TPN is distributed over multiple agents, by creating a hierar- chical ad-hoc network and mapping the TPN onto this hierarchy. Second, candidate plans are extracted from the TPN with a distributed, parallel algorithm that exploits the structure of the TPN. Third, temporal consistency of the candidate plans is tested using a distributed Bellman- Ford algorithm. This algorithm is empirically validated on randomized contingent plans.},
  keywords = {distributed ai,plan execution and,planning},
  file = {/home/cameron/Zotero/storage/NTPLBS75/m-api-594cd29b-9442-3a4d-740e-beb8f841d139.pdf}
}

@article{Weld1994,
  title = {An {{Introduction}} to {{Least Commitment Planning Confrontation}}},
  author = {Weld, Daniel S and Amador, I Franz and Barrett, Tony and Cronquist, Darren and Draper, Denise and Davis, Ernie and Etzioni, Oren and Fowler, Nort and Kambhampati, Rao and Knoblock, Craig and Kushmerick, Nick and Lochbaum, Karen and Mcdermott, Drew and Patil, Ramesh and Pulli, Kari and Sun, Ying},
  year = {1994},
  journal = {Analysis},
  file = {/home/cameron/Zotero/storage/IELBBIL4/lec-07-reading-1.pdf}
}

@phdthesis{Wen2010,
  title = {Rich {{Vehicle Routing Problems}} and {{Applications}}},
  author = {Wen, Min},
  year = {2010},
  abstract = {The Vehicle Routing Problem (VRP) is one of the most important and challenging optimization problems in the field of Operations Research. It was introduced by Dantzig and Ramser (1959) and defined as the problem of designing the optimal set of routes for a fleet of vehicles in order to serve a given set of customers. The VRP is a computationally hard combinatorial problem and has been intensively studied by numerous researchers in the last fifty years. Due to the significant economic benefit that can be achieved by optimizing the routing problems in practice, more and more attention has been given to various extensions of the VRP that arise in real life. These extensions are often called Rich Vehicle Routing Problems (RVRPs). In contrast to the research of classical VRP that focuses on the idealized models with unrealistic assumptions, the research of RVRPs considers those complicated constraints encountered in the real-life planning and provides solutions that are executable in practice. In this thesis, we investigated the models and algorithms of three practical vehicle routing problems. Each of them involves special practical issues that are only considered in very few papers. Our study of these problems was motivated by our cooperation with industrial companies, particularly Transvision A/S and its client distributors, and Danish Crown. The models and methods proposed in the thesis are general and can be applied to practical routing problems arising in many other distribution companies as well. We first consider a vehicle routing problem with cross-docking options, in which products are picked up from suppliers by vehicles, consolidated at the depot and immediately delivered to customers by the same set of vehicles. It is more complex than the traditional vehicle routing problems in the sense that consolidation decisions have to be made at the depot and these decisions interact with the planning of pickup and delivery routes. We presented a mathematical model and proposed a Tabu Search based heuristic to solve it. It is shown that the approach can produce near-optimal solutions within very short computational time on real-life data involving up to 200 pairs of suppliers and customers. The second problem we consider is a dynamic vehicle routing problem with multiple objectives over a planning horizon that consists of multiple periods. In this problem, customer orders are revealed incrementally over the planning horizon. The delivery plan must be made and executed in every period without knowing the future orders. We modeled the problem as a mixed integer linear program and solved it by means of a three-phase heuristic that works over a rolling planning horizon. The method improves the company's solution in terms of all the objectives, including the travel time, customer waiting and daily workload balances, under the given constraints considered in the work. Finally, we address an integrated vehicle routing and driver scheduling problem, in which a large number of practical constraints are considered, such as the multi-period horizon, the time windows for the delivery, the heterogeneous vehicles, the drivers' predefined working regulations, the driving rule etc. The problem is formulated as a mixed integer linear program and treated by a multilevel variable neighborhood search algorithm. The method is implemented and tested on real-life data involving up to 2000 orders. It is shown that the method is able to provide solutions of good quality within reasonable running time.},
  isbn = {978-87-90855-89-5},
  school = {Denmarks Tekniske University},
  file = {/home/cameron/Zotero/storage/WMA8UQNV/Wen - 2010 - Rich Vehicle Routing Problems and Applications.pdf}
}

@article{whelley2014,
  title = {{{LiDAR-derived}} Surface Roughness Texture Mapping: {{Application}} to Mount {{St}}. {{Helens}} Pumice Plain Deposit Analysis},
  author = {Whelley, Patrick L. and Glaze, Lori S. and Calder, Eliza S. and Harding, David J.},
  year = {2014},
  month = jan,
  journal = {IEEE Transactions on Geoscience and Remote Sensing},
  volume = {52},
  number = {1},
  pages = {426--438},
  issn = {01962892},
  doi = {10.1109/TGRS.2013.2241443},
  abstract = {Statistical measures of patterns (textures) in surface roughness are used to quantitatively differentiate volcanic deposit facies on the Pumice Plain, on the northern flank of Mount St. Helens (MSH). Surface roughness values are derived from a Light Detection and Ranging (LiDAR) point cloud collected in 2004 from a fixed-wing airborne platform. Patterns in surface roughness are characterized using co-occurrence texture statistics. Pristine-pyroclastic, reworked-pyroclastic, mudflow, boulder beds, eroded lava flows, braided streams, and other units within the Pumice Plain are all found to have significantly distinct roughness textures. The MSH deposits are reasonably accessible, and the textural variations have been verified in the field. Results of this work indicate that by affecting the distribution of large clasts and tens-of-meter scale landforms, modification of pyroclastic deposits by lahars alters the morphology of the surface in detectable quantifiable ways. When a lahar erodes a pyroclastic deposit, surface roughness increases, as does the randomness in the deposit surface. Conversely, when a lahar deposits material, the resulting landforms are less rough but more random than pristine pumice-rich pyroclastic deposits. By mapping these relationships and others, volcanic deposit facies can be differentiated. This new method of mapping, based on roughness texture, has the potential to aid mapping efforts in more remote regions, both on this planet and elsewhere in the solar system. \textcopyright{} 1980-2012 IEEE.},
  keywords = {Light Detection and Ranging (LiDAR),Mount St. Helens (MSH),Surface roughness,Texture,Volcanology},
  file = {/home/cameron/Zotero/storage/GKR8VPWN/Whelley_2013_LiDAR-Derived_Surface_Roughness_T.pdf}
}

@article{whelley2017,
  title = {{{LiDAR-derived}} Surface Roughness Signatures of Basaltic Lava Types at the {{Muliwai}} a {{Pele Lava Channel}}, {{Mauna Ulu}}, {{Hawai}}`i},
  author = {Whelley, Patrick L. and Garry, W. Brent and Hamilton, Christopher W. and Bleacher, Jacob E.},
  year = {2017},
  month = nov,
  journal = {Bulletin of Volcanology},
  volume = {79},
  number = {11},
  publisher = {{Springer Verlag}},
  issn = {14320819},
  doi = {10.1007/s00445-017-1161-5},
  abstract = {\textcopyright{} 2017, Springer-Verlag GmbH Germany. We used light detection and ranging (LiDAR) data to calculate roughness patterns (homogeneity, mean-roughness, and entropy) for five lava types at two different resolutions (1.5 and 0.1~m/pixel). We found that end-member types ('a'\=a and p\=ahoehoe) are separable (with 95\% confidence) at both scales, indicating that roughness patterns are well suited for analyzing types of lava. Intermediate lavas were also explored, and we found that slabby-p\=ahoehoe is separable from the other end-members using 1.5~m/pixel data, but not in the 0.1~m/pixel analysis. This suggests that the conversion from p\=ahoehoe to slabby-p\=ahoehoe is a meter-scale process, and the finer roughness characteristics of p\=ahoehoe, such as ropes and toes, are not significantly affected. Furthermore, we introduce the ratio ENTHOM (derived from lava roughness) as a proxy for assessing local lava flow rate from topographic data. High entropy and low homogeneity regions correlate with high flow rate while low entropy and high homogeneity regions correlate with low flow rate. We suggest that this relationship is not directional, rather it is apparent through roughness differences of the associated lava type emplaced at the high and low rates, respectively.},
  keywords = {Lava,LiDAR,Roughness,Statistics},
  file = {/home/cameron/Zotero/storage/M3VZKS82/Whelley_2017_LiDAR-derived_surface_roughness_s.pdf}
}

@book{WhettenCameron,
  title = {Developing {{Management Skills}}},
  author = {Whetten, David A and Cameron, Kim S},
  editor = {Wall, Stephanie},
  year = {2016},
  journal = {Fortune},
  publisher = {{Pearson}},
  address = {{Boston, MA}},
  isbn = {978-0-13-312747-8},
  keywords = {Management},
  file = {/home/cameron/Zotero/storage/JSPA5PXB/Whetten, Cameron - 2016 - Developing Management Skills.pdf}
}

@inproceedings{whoimers2019,
  title = {Risk-Bounded, {{Goal-directed Mission Planning}} and {{Execution}} for {{Autonomous Ocean Exploration}}},
  booktitle = {2019 {{Astrobiology Science Conference}}},
  author = {Timmons, Eric and Ayton, Benjamin and Bhargava, Nikhil and Duguid, Zachary and Fang, Cheng and Pascucci, Nick and Reeves, Marlyse and Strawser, Daniel and Wang, Andrew and Zhang, Yuening and Camilli, Richard and Williams, Brian C.},
  year = {2019},
  pages = {1},
  address = {{Seattle, WA}},
  abstract = {Extraterrestrial ocean worlds represent some of the best opportunities within our solar system to find life beyond Earth. However, robotic explorers sent to seek out life in these unknown, likely hazardous environments will find themselves hampered by today's overly risk-averse mission operation protocols. Important transient or motile phenomena will be effectively unobservable if Earth telemetry is a required component of the decision process. Potential data will be lost if an explorer is mortally damaged and spends its last minutes waiting for instructions from Earth instead of collecting, interpreting, and transmitting information. In a dynamic environment an explorer may never be able to fully safe itself to await further instructions if something unexpected happens. The list goes on. In order to maximize the likelihood of finding life, we must enable our explorers to boldly go where no one has gone before by endowing them with the ability to make critical decisions onboard, autonomously.},
  file = {/home/cameron/Zotero/storage/MLH2FN3D/main.pdf}
}

@article{Wilken2017,
  title = {Alarm Fatigue: {{Causes}} and Effects},
  author = {Wilken, Marc and {H{\"u}ske-Kraus}, Dirk and Klausen, Andreas and Koch, Christian and Schlauch, Wolfgang and R{\"o}hrig, Rainer},
  year = {2017},
  journal = {Studies in Health Technology and Informatics},
  volume = {243},
  pages = {107--111},
  issn = {18798365},
  doi = {10.3233/978-1-61499-808-2-107},
  abstract = {The term "Alarm fatigue" is commonly used to describe the effect which a high number of alarms can have on caregivers: Frequent alarms, many of which are avoidable, can lead to inadequate responses, severely impacting patient safety. In the first step of a long-term effort to address this problem, both the direct and indirect impact of alarms, as well as possible causes of unnecessary alarms were focused. Models of these causes and impacts were developed using a scoping review which included guided interviews with experts from medical informatics, clinicians and medical device manufacturers. These models can provide the methodical grounds for the definition of targeted interventions and the assessment of their effects.},
  isbn = {9781614998075},
  pmid = {28883181},
  keywords = {Alarm fatigue,Clinical alarms,Clinical alarms: organization and administration,Critical Care,Patient Safety,Sociotechnical System},
  file = {/home/cameron/Zotero/storage/ZJK3II6T/alarm-fatigue.pdf}
}

@inproceedings{Williams1996b,
  title = {A {{Model-Based Approach}} to {{Reactive Self-Configuring Systems}}},
  booktitle = {{{AAAI}}},
  author = {Williams, Brian C. and Pandurang Nayak, P},
  year = {1996},
  pages = {971--978},
  abstract = {This paper describes Livingstone, an implemented kernel for a model-based reactive self-configuring au- tonomous system. It presents a formal characteriza- tion of Livingstone's representation formalism, and re- ports on our experience with the implementation in a variety of domains. Livingstone provides a reac- tive system that performs significant deduction in the sense/response loop by drawing on our past experi- ence at building fast propositional conflict-based al- gorithms for model-based diagnosis, and by framing a model-based configuration manager as a proposi- tional feedback controller that generates focused, opti- mal responses. Livingstone's representation formalism achieves broad coverage of hybrid hardware/software systems by coupling the transition system models un- derlying concurrent reactive languages with the qual- itative representations developed in model-based rea- soning. Livingstone automates a wide variety of tasks using a single model and a single core algorithm, thus making significant progress towards achieving a cen- tral goal of model-based reasoning. Livingstone, to- gether with the HSTS planning and scheduling engine and the RAPS executive, has been selected as part of the core autonomy architecture for NASA's first New Millennium spacecraft.},
  file = {/home/cameron/Zotero/storage/3B2ULZS3/Williams, Pandurang Nayak - 1996 - A Model-Based Approach to Reactive Self-Configuring Systems.pdf;/home/cameron/Zotero/storage/XAMTTMKX/Williams, Pandurang Nayak - Unknown - A Model-based Approach to Reactive Self-Configuring Systems.pdf}
}

@article{Williams1997,
  title = {A Reactive Planner for Model-Based Executive},
  author = {Williams, Brian C. and Nayak, P. Pandurang},
  year = {1997},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  volume = {2},
  pages = {1178--1185},
  issn = {10450823},
  abstract = {A new generation of reactive, model-based executives are emerging that make extensive use of component-based declarative models to analyze anomalous situations and generate novel sequences for the internal control of complex autonomous systems. Burton, a generative, model-based planner offers a core element that bridges the gap between current and target states within the reactive loop. Burton is a sound, complete, reactive planner that generates a single control action of a valid plan in average case constant time, and compensates for anomalies at every step. Burton will not generate irreversible, potentially damaging sequences, except to effect repairs. We present model compilation, causal analysis, and online policy construction methods that are key to Burton's performance.},
  file = {/home/cameron/Zotero/storage/WVAC8XJL/ijcai97.pdf}
}

@inproceedings{Williams2001,
  title = {Mode {{Estimation}} of {{Model-based Programs}}: {{Monitoring Systems}} with {{Complex Behavior}}},
  booktitle = {Proceedings of the {{Seventeenth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Williams, Brian C. and Chung, Seung H. and Gupta, Vineet},
  year = {2001},
  pages = {7},
  file = {/home/cameron/Zotero/storage/A57ZDQZV/Williams, Chung, Gupta - 2001 - Mode Estimation of Model-based Programs Monitoring Systems with Complex Behavior.pdf}
}

@article{Williams2003,
  title = {Model-Based Programming of Intelligent Embedded Systems and Robotic Space Explorers},
  author = {Williams, Brian C. and Ingham, Michel D. and Chung, Seung H. and Elliott, Paul H.},
  year = {2003},
  journal = {Proceedings of the IEEE},
  volume = {91},
  number = {1},
  pages = {212--236},
  issn = {00189219},
  doi = {10.1109/JPROC.2002.805828},
  abstract = {Programming complex embedded systems involves reasoning through intricate system interactions along lengthy paths between sensors, actuators, and control processors. This is a challenging, time-consuming, and error-prone process requiring significant interaction between engineers and software programmers. Furthermore, the resulting code generally lacks modularity and robustness in the presence of failure. Model-based programming addresses these limitations, allowing engineers to program reactive systems by specifying high-level control strategies and by assembling commonsense models of the system hardware and software. In executing a control strategy, model-based executives reason about the models "on the fly," to track system state, diagnose faults, and perform reconfigurations. This paper develops the reactive model-based programming language (RMPL) and its executive, called Titan. RMPL provides the features of synchronous, reactive languages, with the added ability of reading and writing to state variables that are hidden within the physical plant being controlled. Titan executes an RMPL program using extensive component-based declarative models of the plant to track states, analyze anomalous situations, and generate novel control sequences. Within its reactive control loop, Titan employs propositional inference to deduce the system's current and desired states, and it employs model-based reactive planning to move the plant from the current to the desired state.},
  keywords = {Constraint programming,Model-based autonomy,Model-based execution,Model-based programming,Model-based reasoning,Robotic execution,Synchronous programming},
  file = {/home/cameron/Zotero/storage/BDIC392D/Williams et al. - 2003 - Model-based programming of intelligent embedded systems and robotic space explorers.pdf}
}

@article{Williams2007,
  title = {Conflict-Directed {{A}}* and Its Role in Model-Based Embedded Systems},
  author = {Williams, Brian C. and Ragno, Robert J.},
  year = {2007},
  journal = {Discrete Applied Mathematics},
  volume = {155},
  number = {12},
  pages = {1562--1595},
  issn = {0166218X},
  doi = {10.1016/j.dam.2005.10.022},
  abstract = {Artificial Intelligence has traditionally used constraint satisfaction and logic to frame a wide range of problems, including planning, diagnosis, cognitive robotics and embedded systems control. However, many decision making problems are now being re-framed as optimization problems, involving a search over a discrete space for the best solution that satisfies a set of constraints. The best methods for finding optimal solutions, such as A*, explore the space of solutions one state at a time. This paper introduces conflict-directed A*, a method for solving optimal constraint satisfaction problems. Conflict-directed A* searches the state space in best first order, but accelerates the search process by eliminating subspaces around each state that are inconsistent. This elimination process builds upon the concepts of conflict and kernel diagnosis used in model-based diagnosis [J. de Kleer, B.C. Williams, Diagnosing multiple faults, Artif. Intell. 32(1) (1987) 97-130; J. de Kleer, A. Mackworth, R. Reiter, Characterizing diagnoses and systems, Artif. Intell. 56 (1992) 197-222] and in dependency-directed search [R. Stallman, G.J. Sussman, Forward reasoning and dependency-directed backtracking in a system for computer-aided circuit analysis, Artif. Intell. 9 (1977) 135-196; J. Gaschnig, Performance measurement and analysis of certain search algorithms, Technical Report CMU-CS-79-124, Carnegie-Mellon University, Pittsburgh, PA, 1979; J. de Kleer, B.C. Williams, Back to backtracking: controlling the ATMS, in: Proceedings of AAAI-86, 1986, pp. 910-917; M. Ginsberg, Dynamic backtracking, J. Artif. Intell. Res. 1 (1993) 25-46]. Conflict-directed A* is a fundamental tool for building model-based embedded systems, and has been used to solve a range of problems, including fault isolation [J. de Kleer, B.C. Williams, Diagnosing multiple faults, Artif. Intell. 32(1) (1987) 97-130], diagnosis [J. de Kleer, B.C. Williams, Diagnosis with behavioral modes, in: Proceedings of IJCAI-89, 1989, pp. 1324-1330], mode estimation and repair [B.C. Williams, P. Nayak, A model-based approach to reactive self-configuring systems, in: Proceedings of AAAI-96, 1996, pp. 971-978], model-compilation [B.C. Williams, P. Nayak, A reactive planner for a model-based executive, in: Proceedings of IJCAI-97, 1997] and model-based programming [M. Ingham, R. Ragno, B.C. Williams, A reactive model-based programming language for robotic space explorers, in: Proceedings of ISAIRAS-01, 2001]. \textcopyright{} 2006 Elsevier B.V. All rights reserved.},
  keywords = {Conflict and clause learning,Constraint optimization with logical constraints,Model-based autonomous and embedded systems,Propositional satisfiability},
  file = {/home/cameron/Zotero/storage/CSH5C9GV/Conflict-directed+A_+and+its+Role+in+Model-based+Embedded%250ASystems.pdf}
}

@article{Wu2020,
  title = {On the {{Feasibility}} of {{Stealthily Introducing Vulnerabilities}} in {{Open-Source Software}} via {{Hypocrite Commits}}},
  author = {Wu, Qiushi and Lu, Kangjie},
  year = {2020},
  abstract = {On the Feasibility of Stealthily IntroducingVulnerabilities in Open-Source Software viaHypocrite CommitsQiushi Wu and Kangjie LuUniversity of Minnesota\{wu000273, kjlu\}@umn.eduAbstract\textemdash Open source software (OSS) has thrived since theforming of Open Source Initiative in 1998. A prominent exampleis the Linux kernel, which has been used by numerous majorsoftware vendors and empowering billions of devices. The higheravailability and lower costs of OSS boost its adoption, while itsopenness and flexibility enable quicker innovation. More impor-tantly, the OSS development approach is believed to producemore reliable and higher-quality software since it typically hasthousands of independent programmers testing and fixing bugsof the software collaboratively.In this paper, we instead investigate the insecurity of OSS froma critical perspective\textemdash the feasibility of stealthily introducingvulnerabilities in OSS via hypocrite commits (i.e., seeminglybeneficial commits that in fact introduce other critical issues).The introduced vulnerabilities are critical because they may bestealthily exploited to impact massive devices. We first identifythree fundamental reasons that allow hypocrite commits. (1)OSS is open by nature, so anyone from anywhere, includingmalicious ones, can submit patches. (2) Due to the overwhelmingpatches and performance issues, it is impractical for maintainersto accept preventive patches for ``immature vulnerabilities''. (3)OSS like the Linux kernel is extremely complex, so the patch-review process often misses introduced vulnerabilities that involvecomplicated semantics and contexts. We then systematically studyhypocrite commits, including identifying immature vulnerabilitiesand potential vulnerability-introducing minor patches. We alsoidentify multiple factors that can increase the stealthiness ofhypocrite commits and render the patch-review process lesseffective. As proof of concept, we take the Linux kernel as targetOSS and safely demonstrate that it is practical for a maliciouscommitter to introduce use-after-free bugs. Furthermore, wesystematically measure and characterize the capabilities andopportunities of a malicious committer. At last, to improvethe security of OSS, we propose mitigations against hypocritecommits, such as updating the code of conduct for OSS anddeveloping tools for patch testing and verification.},
  file = {/home/cameron/Zotero/storage/RW7SSTJP/OpenSourceInsecurity.pdf}
}

@article{Yang2021,
  title = {Teaser: {{Fast}} and Certifiable Point Cloud Registration},
  author = {Yang, Heng and Shi, Jingnan and Carlone, Luca},
  year = {2021},
  journal = {IEEE Transactions on Robotics},
  volume = {37},
  number = {2},
  eprint = {2001.07715},
  eprinttype = {arxiv},
  pages = {314--333},
  issn = {19410468},
  doi = {10.1109/TRO.2020.3033695},
  abstract = {We propose the first fast and certifiable algorithm for the registration of two sets of three-dimensional (3-D) points in the presence of large amounts of outlier correspondences. A certifiable algorithm is one that attempts to solve an intractable optimization problem (e.g., robust estimation with outliers) and provides readily checkable conditions to verify if the returned solution is optimal (e.g., if the algorithm produced the most accurate estimate in the face of outliers) or bound its suboptimality or accuracy. Toward this goal, we first reformulate the registration problem using a truncated least squares (TLS) cost that makes the estimation insensitive to a large fraction of spurious correspondences. Then, we provide a general graph-theoretic framework to decouple scale, rotation, and translation estimation, which allows solving in cascade for the three transformations. Despite the fact that each subproblem (scale, rotation, and translation estimation) is still nonconvex and combinatorial in nature, we show that 1) TLS scale and (component-wise) translation estimation can be solved in polynomial time via an adaptive voting scheme, 2) TLS rotation estimation can be relaxed to a semidefinite program (SDP) and the relaxation is tight, even in the presence of extreme outlier rates, and 3) the graph-theoretic framework allows drastic pruning of outliers by finding the maximum clique. We name the resulting algorithm TEASER (Truncated least squares Estimation And SEmidefinite Relaxation). While solving large SDP relaxations is typically slow, we develop a second fast and certifiable algorithm, named TEASER++, that uses graduated nonconvexity to solve the rotation subproblem and leverages Douglas-Rachford Splitting to efficiently certify global optimality. For both algorithms, we provide theoretical bounds on the estimation errors, which are the first of their kind for robust registration problems. Moreover, we test their performance on standard benchmarks, object detection datasets, and the 3DMatch scan matching dataset, and show that 1) both algorithms dominate the state-of-the-art (e.g., RANSAC, branch-\&-bound, heuristics) and are robust to more than 99\% outliers when the scale is known, 2) TEASER++ can run in milliseconds and it is currently the fastest robust registration algorithm, and 3) TEASER++ is so robust it can also solve problems without correspondences (e.g., hypothesizing all-to-all correspondences), where it largely outperforms ICP and it is more accurate than Go-ICP while being orders of magnitude faster. We release a fast open-source C++ implementation of TEASER++.},
  archiveprefix = {arXiv},
  keywords = {3-D robot vision,Certifiable algorithms,Object pose estimation,Outliers-robust estimation,Point cloud alignment,Robust estimation,Scan matching,Three-dimensional (3-D) registration},
  file = {/home/cameron/Zotero/storage/IVD9KE9J/TEASER_Fast_and_Certifiable_Point_Cloud_Registration.pdf}
}

@article{Yilmaz2008,
  title = {Path Planning of Autonomous Underwater Vehicles for Adaptive Sampling Using Mixed Integer Linear Programming},
  author = {Yilmaz, Namik Kemal and Evangelinos, Constantinos and Lermusiaux, Pierre F.J. and Patrikalakis, Nicholas M.},
  year = {2008},
  journal = {IEEE Journal of Oceanic Engineering},
  volume = {33},
  number = {4},
  pages = {522--537},
  issn = {03649059},
  doi = {10.1109/JOE.2008.2002105},
  abstract = {The goal of adaptive sampling in the ocean is to predict the types and locations of additional ocean measurements that would be most useful to collect. Quantitatively, what is most useful is defined by an objective function and the goal is then to optimize this objective under the constraints of the available observing network. Examples of objectives are better oceanic understanding, to improve forecast quality, or to sample regions of high interest. This work provides a new path-planning scheme for the adaptive sampling problem. We define the path-planning problem in terms of an optimization framework and propose a method based on mixed integer linear programming (MILP). The mathematical goal is to find the vehicle path that maximizes the line integral of the uncertainty of field estimates along this path. Sampling this path can improve the accuracy of the field estimates the most. While achieving this objective, several constraints must be satisfied and are implemented. They relate to vehicle motion, intervehicle coordination, communication, collision avoidance, etc. The MILP formulation is quite powerful to handle different problem constraints and flexible enough to allow easy extensions of the problem. The formulation covers single- and multiple-vehicle cases as well as singleand multiple-day formulations. The need for a multiple-day formulation arises when the ocean sampling mission is optimized for several days ahead. We first introduce the details of the formulation, then elaborate on the objective function and constraints, and finally, present a varied set of examples to illustrate the applicability of the proposed method. \textcopyright{} 2008 IEEE.},
  isbn = {0001404105},
  keywords = {Adaptive sampling,Autonomous ocean sampling Network (AOSN),Autonomous underwater vehicle (AUV),Data assimilation,Error subspace,Mixed integer linear programming (MILP),Monterey Bay,Ocean modeling,Ocean observing system,Path planning,Routing,Trajectory planning},
  file = {/home/cameron/Zotero/storage/TTMH4M5N/Yilmaz et al. - 2008 - Path planning of autonomous underwater vehicles for adaptive sampling using mixed integer linear programming.pdf}
}

@article{Young2016,
  title = {A Review of the Handheld {{X-ray}} Fluorescence Spectrometer as a Tool for Field Geologic Investigations on {{Earth}} and in Planetary Surface Exploration},
  author = {Young, Kelsey E. and Evans, Cynthia A. and Hodges, Kip V. and Bleacher, Jacob E. and Graff, Trevor G.},
  year = {2016},
  journal = {Applied Geochemistry},
  volume = {72},
  pages = {77--87},
  publisher = {{Elsevier Ltd}},
  issn = {18729134},
  doi = {10.1016/j.apgeochem.2016.07.003},
  abstract = {X-ray fluorescence (XRF) spectroscopy is a well-established and commonly used technique in obtaining diagnostic compositional data on geological samples. Recently, developments in X-ray tube and detector technologies have resulted in miniaturized, field-portable instruments that enable new applications both in and out of standard laboratory settings. These applications, however, have not been extensively applied to geologic field campaigns. This study investigates the feasibility of using developing handheld XRF (hXRF) technology to enhance terrestrial field geology, with potential applications in planetary surface exploration missions. We demonstrate that the hXRF is quite stable, providing reliable and accurate data continuously over a several year period. Additionally, sample preparation is proved to have a marked effect on the strategy for collecting and assimilating hXRF data. While the hXRF is capable of obtaining data that are comparable to laboratory XRF analysis for several geologically-important elements (such as Si, Ca, Ti, and K), the instrument is unable to detect other elements (such as Mg and Na) reliably. While this limits the use of the hXRF, especially when compared to laboratory XRF techniques, the hXRF is still capable of providing the field user with significantly improved contextual awareness of a field site, and more work is needed to fully evaluate the potential of this instrument in more complex geologic environments.},
  keywords = {Field portable technology,Field spectroscopy,Handheld X-ray fluorescence spectrometer (hXRF),In situ field geologic instrument,In situ geochemistry,Planetary field geology},
  file = {/home/cameron/Zotero/storage/9VVLDBZT/Young et al. - 2016 - A review of the handheld X-ray fluorescence spectrometer as a tool for field geologic investigations on Earth and.pdf}
}

@article{Young2018,
  title = {The {{Incorporation}} of {{Field Portable Instrumentation Into Human Planetary Surface Exploration}}},
  author = {Young, K. E. and Bleacher, J. E. and Rogers, A. D. and Schmitt, H. H. and McAdam, A. C. and Garry, W. B. and Whelley, P. L. and Scheidt, S. P. and Ito, G. and Knudson, C. A. and Graff, T. G. and Bleacher, L. V. and Whelley, N. and Evans, C. A. and Hurtado, J. M. and Glotch, T. D.},
  year = {2018},
  journal = {Earth and Space Science},
  volume = {5},
  number = {11},
  pages = {697--720},
  issn = {23335084},
  doi = {10.1029/2018EA000378},
  abstract = {Field portable instrumentation, such as in situ geochemical analyzers or broader field of view instruments like multispectral imagers or other imaging capabilities, has the potential to dramatically increase the science return of a planetary surface exploration mission. However, more work is needed to determine how emerging portable technologies should be designed and implemented into evolving mission architectures. This work summarizes the efforts of the RIS4E (Remote, In Situ and Synchrotron Studies for Science and Exploration) SSERVI (Solar System Exploration Research Virtual Institute) team in investigating how field portable instruments should be including into planning for future exploration EVAs (extravehicular activities). EVA crews of geologists and astronauts tested a variety of portable and handheld technologies at both the December 1974 lava flow, Kilauea Volcano, Hawai'i, and Kilbourne Hole, New Mexico, both of which are planetary analog sites. The timeline data gathered during instrument deployment were then mapped onto EVA timelines used in large-scale NASA planetary surface exploration analog missions. Results and recommendations for future instrument hardware and software development are discussed, as is the operational framework necessary for incorporating in situ analytical capabilities into future planetary surface exploration.},
  keywords = {extravehicular activity,field instruments,field portable instruments,planetary field geology,planetary surface exploration,science operations},
  file = {/home/cameron/Zotero/storage/NTNCE4T2/Young et al. - 2018 - The Incorporation of Field Portable Instrumentation Into Human Planetary Surface Exploration.pdf}
}

@inproceedings{Young2020,
  title = {Planetary {{Science Context}} for {{EVA}}},
  booktitle = {{{NASA EVA Exploration Workshop}}},
  author = {Young, Kelsey and Graff, Trevor},
  year = {2020},
  number = {February},
  pages = {40},
  address = {{Houston, TX}},
  file = {/home/cameron/Zotero/storage/SLTFFD4L/m-api-29e034a6-c3df-8ab1-3e47-5a923992ef0d.pdf}
}

@article{Yu2013,
  title = {Continuously Relaxing Over-Constrained Conditional Temporal Problems through Generalized Conflict Learning and Resolution},
  author = {Yu, Peng and Williams, Brian},
  year = {2013},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  pages = {2429--2436},
  issn = {10450823},
  abstract = {Over-constrained temporal problems are commonly encountered while operating autonomous and decision support systems. An intelligent system must learn a human's preference over a problem in order to generate preferred resolutions that minimize perturbation. We present the Best-first Conflict-Directed Relaxation (BCDR) algorithm for enumerating the best continuous relaxation for an over-constrained conditional temporal problem with controllable choices. BCDR reformulates such a problem by making its temporal constraints relaxable and solves the problem using a conflictdirected approach. It extends the Conflict-Directed A* (CD-A*) algorithm to conditional temporal problems, by first generalizing the conflict learning process to include all discrete variable assignments and continuous temporal constraints, and then by guiding the forward search away from known infeasible regions using conflict resolution. When evaluated empirically on a range of coordinated car sharing network problems, BCDR demonstrates a substantial improvement in performance and solution quality compared to previous conflict-directed approaches.},
  isbn = {9781577356332},
  file = {/home/cameron/Zotero/storage/HNPT26YR/Yu, Williams - 2013 - Continuously relaxing over-constrained conditional temporal problems through generalized conflict learning and res.pdf}
}

@article{Yu2014,
  title = {Resolving Uncontrollable Conditional Temporal Problems Using Continuous Relaxations},
  author = {Yu, Peng and Fang, Cheng and Williams, Brian},
  year = {2014},
  journal = {Proceedings International Conference on Automated Planning and Scheduling, ICAPS},
  volume = {2014-Janua},
  number = {January},
  pages = {341--349},
  issn = {23340843},
  abstract = {Uncertainty is commonly encountered in temporal scheduling and planning problems, and can often lead to over-constrained situations. Previous relaxation algorithms for over-constrained temporal problems only work with requirement constraints, whose outcomes can be controlled by the agents. When applied to uncontrollable durations, these algorithms may only satisfy a subset of the random outcomes and hence their relaxations may fail during execution. In this paper, we present a new relaxation algorithm, Conflict-Directed Relaxation with Uncertainty (CDRU), which generates relaxations that restore the controllability of conditional temporal problems with uncontrollable durations. CDRU extends the Best-first Conflict-Directed Relaxation (BCDR) algorithm to uncontrollable temporal problems. It generalizes the conflict-learning process to extract conflicts from strong and dynamic controllability checking algorithms, and resolves the conflicts by both relaxing constraints and tightening uncontrollable du-rations. Empirical test results on a range of trip scheduling problems show that CDRU is efficient in resolving large scale uncontrollable problems: computing strongly controllable relaxations takes the same order of magnitude in time compared to consistent relaxations that do not account for uncontrollable durations. While computing dynamically controllable relaxations takes two orders of magnitude more time, it provides significant improvements in solution quality when compared to strongly controllable relaxations.},
  file = {/home/cameron/Zotero/storage/CXA9DM9J/Yu, Fang, Williams - 2014 - Resolving uncontrollable conditional temporal problems using continuous relaxations.pdf}
}

@article{Yu2015,
  title = {Resolving Over-Constrained Probabilistic Temporal Problems through {{Chance}} Constraint Relaxation},
  author = {Yu, Peng and Fang, Cheng and Williams, Brian},
  year = {2015},
  journal = {Proceedings of the National Conference on Artificial Intelligence},
  volume = {5},
  pages = {3425--3431},
  abstract = {When scheduling tasks for field-deployable systems, our solutions must be robust to the uncertainty inherent in the real world. Although human intuition is trusted to balance reward and risk, humans perform poorly in risk assessment at the scale and complexity of real world problems. In this paper, we present a decision aid system that helps human operators diagnose the source of risk and manage uncertainty in temporal problems. The core of the system is a conflict-directed relaxation algorithm, called Conflict-Directed Chance-constraint Relaxation (CDCR), which specializes in resolving overconstrained temporal problems with probabilistic durations and a chance constraint bounding the risk of failure. Given a temporal problem with uncertain duration, CDCR proposes execution strategies that operate at acceptable risk levels and pinpoints the source of risk. If no such strategy can be found that meets the chance constraint, it can help humans to repair the overconstrained problem by trading off between desirability of solution and acceptable risk levels. The decision aid has been incorporated in a mission advisory system for assisting oceanographers to schedule activities in deepsea expeditions, and demonstrated its effectiveness in scenarios with realistic uncertainty.},
  isbn = {9781577357032},
  file = {/home/cameron/Zotero/storage/74E4CBGX/Yu, Fang, Williams - 2015 - Resolving over-constrained probabilistic temporal problems through Chance constraint relaxation.pdf}
}

@article{Yu2016,
  title = {Resolving Over-Constrained Conditional Temporal Problems Using Semantically Similar Alternatives},
  author = {Yu, Peng and Shen, Jiaying and Yeh, Peter Z. and Williams, Brian},
  year = {2016},
  journal = {IJCAI International Joint Conference on Artificial Intelligence},
  volume = {2016-Janua},
  pages = {3300--3307},
  issn = {10450823},
  abstract = {In recent literature, several approaches have been developed to solve over-constrained travel planning problems, which are often framed as conditional temporal problems with discrete choices. These approaches are able to explain the causes of failure and recommend alternative solutions by suspending or weakening temporal constraints. While helpful, they may not be practical in many situations, as we often cannot compromise on time. In this paper, we present an approach for solving such over-constrained problems, by also relaxing non-temporal variable domains through the consideration of additional options that are semantically similar. Our solution, called Conflict-Directed Semantic Relaxation (CDSR), integrates a knowledge base and a semantic similarity calculator, and is able to simultaneously enumerate both temporal and domain relaxations in best-first order. When evaluated empirically on a range of urban trip planning scenarios, CDSR demonstrates a substantial improvement in flexibility compared to temporal relaxation only approaches.},
  file = {/home/cameron/Zotero/storage/RNKR832S/Yu et al. - 2016 - Resolving over-constrained conditional temporal problems using semantically similar alternatives.pdf}
}

@phdthesis{Yu2017,
  title = {Collaborative {{Diagnosis}} of {{Over-Subscribed Temporal Plans Collaborative Diagnosis}} of {{Over-Subscribed Temporal Plans}}},
  author = {Yu, Peng},
  year = {2016},
  abstract = {Over-subscription, that is, being assigned too many tasks or requirements that are too demanding, is commonly encountered in temporal planning problems. As human beings, we often want to do more than we can, ask for things that may not be available, while underestimating how long it takes to perform each task. It is often difficult for us to detect the causes of failure in such situations and then find resolutions that are effective. We can greatly benefit from tools that assist us by looking out for these plan failures, by identifying their root causes, and by proposing preferred resolutions to these failures that lead to feasible plans. In recent literature, several approaches have been developed to resolve such over- subscribed problems, which are often framed as over-constrained scheduling, config- uration design or optimal planning problems. Most of them take an all-or-nothing approach, in which over-subscription is resolved through suspending constraints or dropping goals. While helpful, in real-world scenarios, we often want to preserve our plan goals as much possible. As human beings, we know that slightly weakening the requirements of a travel plan, or replacing one of its destinations with an alternative one is often sufficient to resolve an over-subscription problem, no matter if the re- quirement being weakened is the duration of a deep-sea survey being planned for, or the restaurant cuisine for a dinner date. The goal of this thesis is to develop domain independent relaxation algorithms that perform this type of slight weakening of constraints, which we will formalize as contin- uous relaxation, and to embody them in a computational aid, Uhura, that performs tasks akin to an experienced travel agent or ocean scientists. In over-subscribed situ- ations, Uhura helps us diagnose the causes of failure, suggests alternative plans, and collaborates with us in order to resolve conflicting requirements in the most preferred way. Most importantly, the algorithms underlying Uhura supports the weakening, instead of suspending, of constraints and variable domains in a temporally flexible plan. The contribution of this thesis is two-fold. First, we developed an algorithmic framework, called Best-first Conflict-Directed Relaxation (BCDR), for performing plan relaxation. Second, we use the BCDR framework to perform relaxation for several different families of plan representations involving different types of constraints. These include temporal constraints, chance constraints and variable domain con- straints, and we incorporate several specialized conflict detection and resolution algo- rithms in support of the continuous weakening of them. The key idea behind BCDR's approach to continuous relaxation is to generalize the concepts of discrete conflicts and relaxations, first introduced by the model-based diagnosis community, to hybrid conflicts and relaxations, which denote minimal inconsistencies and minimal relax- ations to both discrete and continuous relaxable constraints. In addition, we present the design and implementation of Uhura, the integrated plan advisory system that incorporates BCDR for resolving over-subscribed temporal plans. Uhura can efficiently produce a relaxed plan for the user to support multiple, interrelated constraints and activities. We have applied Uhura to different types of plans to illustrate the practical generality of our approach, which includes deep- sea exploration, job-shop scheduling and transit system management. Results from the computational experiments we performed also show that BCDR is one to two orders of magnitude faster than existing algorithms that build on state-of-the-art numerical solvers, making it an effective approach for many large-scale plans in the aforementioned domains.},
  school = {Massachusetts Institute of Technology},
  file = {/home/cameron/Zotero/storage/GBB3UNQM/834a34fe2f68670dedbfcfd802d4a81e4332.pdf}
}

@phdthesis{Yu2017a,
  title = {Collaborative {{Diagnosis}} of {{Over-Subscribed Temporal Plans}}},
  author = {Yu, Peng},
  year = {2017},
  number = {2015},
  file = {/home/cameron/Zotero/storage/QKRZ8WDJ/986241659-MIT.pdf}
}

@article{zavatteri2019,
  title = {Conditional Simple Temporal Networks with Uncertainty and Decisions},
  author = {Zavatteri, Matteo and Vigan{\`o}, Luca},
  year = {2019},
  journal = {Theoretical Computer Science},
  volume = {797},
  issn = {03043975},
  doi = {10.1016/j.tcs.2018.09.023},
  abstract = {A Conditional Simple Temporal Network with Uncertainty (CSTNU) is a formalism able to model temporal plans subject to both conditional constraints and uncertain durations. The combination of these two characteristics represents the uncontrollable part of the network. That is, before the network starts executing, we do not know completely which time points and constraints will be taken into consideration nor how long the uncertain durations will last. Dynamic Controllability (DC) implies the existence of a strategy scheduling the time points of the network in real time depending on how the uncontrollable part behaves. Despite all this, CSTNUs fail to model temporal plans in which a few conditional constraints are under control and may therefore influence (or be influenced by) the uncontrollable part. To bridge this gap, this paper proposes Conditional Simple Temporal Networks with Uncertainty and Decisions (CSTNUDs) which introduce decision time points into the specification in order to operate on this conditional part under control. We model the dynamic controllability checking (DC-checking) of a CSTNUD as a two-player game in which each player makes his moves in his turn at a specific time instant. We give an encoding into timed game automata for a sound and complete DC-checking. We also synthesize memoryless execution strategies for CSTNUDs proved to be DC and carry out an experimental evaluation with (Figure presented.), a tool that we have designed for CSTNUDs to make the approach fully automated.}
}

@inproceedings{Zhang2021,
  title = {State-{{Temporal Decoupling}} of {{Multi-Agent Plans}} under {{Limited Communication}}},
  booktitle = {Association for the {{Advancement}} of {{Artificial Intelligence}}},
  author = {Zhang, Yuening and Chen, Jingkai and Timmons, Eric and Reeves, Marlyse and Williams, Brian},
  year = {2021},
  pages = {9},
  abstract = {When a team of agents execute a mission in a distributed fash- ion, they often communicate with each other to synchronize their progress. However, in situations where communication may be delayed, unavailable or costly, such as when a suite of underwater vehicles is scouting an underwater area in the ocean, pre-coordination is needed beforehand to compensate for limited communication. Previous work proposed decou- pling algorithms for Multi-Agent Simple Temporal Network with Uncertainty (MaSTNU) in order to find decoupled ex- ecution strategies for the agents, including communication strategy, that satisfy all the inter-agent temporal constraints. However, there is often the coupling between temporal and state constraints, such as the constraint that the vehicles may only communicate with each other when they are within a certain distance. In this paper, we propose using Multi-Agent Qualitative State Plan (MaQSP) that extends MaSTNU to in- cluding continuous state constraints in order to model multi- agent plans with coupled state and temporal constraints. We describe a decoupling algorithm for MaQSP using a mixed- integer linear programming (MILP) encoding, which includes a novel path planning algorithm under temporal uncertainty.},
  file = {/home/cameron/Zotero/storage/Q6WXLGGZ/ICAPS_2022__Spatial_Temporal_Decoupling.pdf}
}

@article{Zhang2021a,
  title = {Privacy-{{Preserving Algorithm}} for {{Decoupling}} of {{Multi-Agent Plans}} with {{Uncertainty}}},
  author = {Zhang, Yuening and Williams, Brian},
  year = {2021},
  journal = {Proceedings International Conference on Automated Planning and Scheduling, ICAPS},
  volume = {2021-Augus},
  number = {Vidal 1999},
  pages = {426--434},
  issn = {23340843},
  abstract = {The execution of multi-agent plans often requires communication between agents in order to synchronize their tasks. In cases where communication is unreliable or undesirable, temporal decoupling algorithms allow agents to find a distributed execution strategy beforehand without requiring perfect communication on the fly. The state-of-the-art Multi-Agent Simple Temporal Network with Uncertainty (MaSTNU) framework extends the decoupling problem for Multi-Agent Simple Temporal Network (MaSTN) to allow the modeling of uncertain durations and allow agents to communicate when certain events occur and communication is available. However, the existing approach assumes centralized knowledge of the MaSTNU, whereas in the multi-agent context, privacy is an important concern. In this paper, we propose a distributed, privacy-preserving algorithm for finding distributed execution strategies for MaSTNU. Experiments also showed significant speed-up of the proposed algorithm when the multiagent plan is loosely coupled and mostly private.},
  isbn = {9781713832317},
  keywords = {ICAPS: Multi-agent And Distributed Planning},
  file = {/home/cameron/Zotero/storage/QEB4Y2P4/Zhang, Williams - 2021 - Privacy-Preserving Algorithm for Decoupling of Multi-Agent Plans with Uncertainty.pdf}
}

@article{Zhou2021,
  title = {Implementing the {{Global University}} Publications Licence: {{A}} New Open Scholarship Model for Advocating Change},
  author = {Zhou, Jiafeng and Wu, Ke and Smyth, Neil},
  year = {2021},
  journal = {Insights: the UKSG Journal},
  volume = {34},
  pages = {1--12},
  issn = {20487754},
  doi = {10.1629/UKSG.531},
  abstract = {Universities want a voluntary, non-exclusive licence from authors to disseminate publications. This practitioner case study explores an innovative model to communicate and advance open and equitable scholarship through the implementation of the Global University Publications Licence at the University of Nottingham Ningbo China. This article explains the licensing policy and key influences, including, the copyright law of the People's Republic of China and the Declaration on Research Assessment (DORA). The University approved the Global University Publications Licence, with implementation from 1 August 2019. It is available in Chinese and English. Since implementation, the University has retained rights for 74\% of research publications submitted. 100\% of those publications are available through the University with a CC-BY licence and zero embargo. The open scholarship model provides an equitable approach to versions and citation. The article concludes by suggesting university libraries can exploit copyright law in China to progress open scholarship strategies, including recognition of employers as authors of works, a priority right to the exploitation of works and an embargo protection of two years after the completion of the work. The author's final version of publications can be open, discoverable, cited and preserved through trusted universities with global reputations for high-quality research.},
  keywords = {Open access,Open scholarship,Research publications citations,University licensing},
  file = {/home/cameron/Zotero/storage/IUQLBXCL/R05 - Zhou, Wu, Smyth - Global Pub.pdf}
}

@article{Zivan2006,
  title = {Message Delay and {{DisCSP}} Search Algorithms},
  author = {Zivan, Roie and Meisels, Amnon},
  year = {2006},
  journal = {Annals of Mathematics and Artificial Intelligence},
  volume = {46},
  number = {4},
  pages = {415--439},
  issn = {10122443},
  doi = {10.1007/s10472-006-9033-2},
  abstract = {Distributed constraint satisfaction problems (DisCSPs) are composed of agents, each holding its own variables, that are connected by constraints to variables of other agents. Due to the distributed nature of the problem, message delay can have unexpected effects on the behavior of distributed search algorithms on DisCSPs. This has been recently shown in experimental studies of asynchronous backtracking algorithms (Bejar et al., Artif. Intell., 161:117-148, 2005; Silaghi and Faltings, Artif. Intell., 161:25-54, 2005). To evaluate the impact of message delay on the run of DisCSP search algorithms, a model for distributed performance measures is presented. The model counts the number of non concurrent constraints checks, to arrive at a solution, as a non concurrent measure of distributed computation. A simpler version measures distributed computation cost by the non-concurrent number of steps of computation. An algorithm for computing these distributed measures of computational effort is described. The realization of the model for measuring performance of distributed search algorithms is a simulator which includes the cost of message delays. Two families of distributed search algorithms on DisCSPs are investigated. Algorithms that run a single search process, and multiple search processes algorithms. The two families of algorithms are described and associated with existing algorithms. The performance of three representative algorithms of these two families is measured on randomly generated instances of DisCSPs with delayed messages. The delay of messages is found to have a strong negative effect on single search process algorithms, whether synchronous or asynchronous. Multi search process algorithms, on the other hand, are affected very lightly by message delay. \textcopyright{} Springer Science+Business Media, Inc. 2006.},
  keywords = {Distributed AI,Distributed constraint satisfaction,Search},
  file = {/home/cameron/Zotero/storage/R8BT5EF8/Zivan-Meisels2006_Article_MessageDelayAndDisCSPSearchAlg.pdf}
}

@misc{zotero-1828,
  title = {Introduction-to-{{ISS-Mission-Control}}},
  file = {/home/cameron/Zotero/storage/XM5NPEGC/Introduction-to-ISS-Mission-Control.docx}
}

@misc{zotero-1832,
  title = {A {{Concept}} of {{Operations}} 20170227},
  file = {/home/cameron/Zotero/storage/BZTI89AL/A Concept of Operations 20170227.docx}
}

@misc{zotero-1840,
  title = {{{EVA Technology Workshop Bioinformatics}}\_pubpressreview\_clean},
  file = {/home/cameron/Zotero/storage/7W4AHP5N/EVA Technology Workshop Bioinformatics_pubpressreview_clean.pptx}
}

@misc{zotero-1842,
  title = {Brockwell\_{{Peter}}\_{{J}}\_{{Introduction}}.{{PDF}}},
  file = {/home/cameron/Zotero/storage/V7YGIZBN/Brockwell_Peter_J_Introduction.PDF}
}

@misc{zotero-1852,
  title = {Bioadvisory\_{{Alg}}\_{{Functional}}\_{{Spec}}\_{{26April2010}}},
  file = {/home/cameron/Zotero/storage/PFA6P37I/Bioadvisory_Alg_Functional_Spec_26April2010.doc}
}

@article{zotero-1858,
  title = {Additional Documentation and the Code for the {{BioAdvisory Algorithm}} Are Stored in the {{EPSP Project Archives}} in {{JSC Building}} 37, {{Room}} 1005.},
  pages = {1005},
  file = {/home/cameron/Zotero/storage/LZKKAFQU/READ-ME-BioadvisoryAlgorithmLocation.pdf}
}

@misc{zotero-1887,
  title = {Significant\_{{Incidents}}.Pdf},
  file = {/home/cameron/Zotero/storage/JXAYFAWV/Significant_Incidents.pdf}
}

@article{zotero-1951,
  title = {583320main\_2011\_{{Present}}\_{{NASA}}\_{{IT}}\_{{Summit}}\_{{Hall}}\_{{Tim}}\_{{OCAMS}}},
  file = {/home/cameron/Zotero/storage/FQ8YDA7Y/583320main_2011_Present_NASA_IT_Summit_Hall_Tim_OCAMS.pptx}
}

@article{zotero-2483,
  title = {Technical Approach\_9.18.18\_final},
  file = {/home/cameron/Zotero/storage/UBFS2G4R/Technical approach_9.18.18_final.pdf}
}


